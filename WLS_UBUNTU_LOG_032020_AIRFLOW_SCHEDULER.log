jdearce@LAPTOP-I43LI54D:~$ airflow scheduler

WARNING:root:/c/Users/jdearce/airflow/logs/scheduler/2020-03-18 already exists
WARNING:root:/c/Users/jdearce/airflow/logs/scheduler/latest already exists as a dir/file. Skip creating symlink.
[2020-03-18 08:06:58,082] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=114
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2020-03-18 08:08:51,460] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:08:52,689] {scheduler_job.py:1320} INFO - Starting the scheduler
[2020-03-18 08:08:52,689] {scheduler_job.py:1328} INFO - Running execute loop for -1 seconds
[2020-03-18 08:08:52,691] {scheduler_job.py:1329} INFO - Processing each file at most -1 times
[2020-03-18 08:08:52,691] {scheduler_job.py:1332} INFO - Searching for files in /c/Users/jdearce/airflow/dags
[2020-03-18 08:08:54,886] {scheduler_job.py:1334} INFO - There are 20 files in /c/Users/jdearce/airflow/dags
[2020-03-18 08:09:00,360] {scheduler_job.py:1382} INFO - Resetting orphaned tasks for active dag runs
[2020-03-18 08:09:05,903] {dag_processing.py:556} INFO - Launched DagFileProcessorManager with pid: 279
[2020-03-18 08:09:05,985] {settings.py:54} INFO - Configured default timezone <Timezone [UTC]>
[2020-03-18 08:09:06,175] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=279
[2020-03-18 08:09:17,930] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: example_branch_operator.run_this_first 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:09:18,010] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued
[2020-03-18 08:09:18,011] {scheduler_job.py:986} INFO - DAG example_branch_operator has 0/16 running and queued tasks
[2020-03-18 08:09:18,024] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_branch_operator.run_this_first 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:09:18,173] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: example_branch_operator.run_this_first 2020-03-17 00:00:00+00:00 [queued]>
[2020-03-18 08:09:18,175] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'run_this_first', datetime.datetime(2020, 3, 17, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 11 and queue default
[2020-03-18 08:09:18,176] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'run_this_first', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-18 08:09:18,184] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'run_this_first', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-18 08:09:21,381] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=328
[2020-03-18 08:09:22,035] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-17 00:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:09:22,045] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 127 open slots and 2 task instances ready to be queued
[2020-03-18 08:09:22,047] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-18 08:09:22,048] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-18 08:09:22,060] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-17 00:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:09:22,470] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-17 00:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-17 00:00:00+00:00 [queued]>
[2020-03-18 08:09:22,473] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 17, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-18 08:09:22,474] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:09:22,476] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 17, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-18 08:09:22,477] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:09:22,480] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:09:22,482] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:09:26,019] {scheduler_job.py:927} INFO - 4 tasks up for execution:
        <TaskInstance: example_skip_dag.skip_operator_2 2020-03-17 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.skip_operator_1 2020-03-17 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.always_true_2 2020-03-17 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.always_true_1 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:09:26,034] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 125 open slots and 4 task instances ready to be queued
[2020-03-18 08:09:26,034] {scheduler_job.py:986} INFO - DAG example_skip_dag has 0/16 running and queued tasks
[2020-03-18 08:09:26,035] {scheduler_job.py:986} INFO - DAG example_skip_dag has 1/16 running and queued tasks
[2020-03-18 08:09:26,037] {scheduler_job.py:986} INFO - DAG example_skip_dag has 2/16 running and queued tasks
[2020-03-18 08:09:26,038] {scheduler_job.py:986} INFO - DAG example_skip_dag has 3/16 running and queued tasks
[2020-03-18 08:09:26,051] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_skip_dag.skip_operator_2 2020-03-17 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.skip_operator_1 2020-03-17 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.always_true_2 2020-03-17 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.always_true_1 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:09:26,486] {scheduler_job.py:1112} INFO - Setting the following 4 tasks to queued state:
        <TaskInstance: example_skip_dag.skip_operator_2 2020-03-17 00:00:00+00:00 [queued]>
        <TaskInstance: example_skip_dag.skip_operator_1 2020-03-17 00:00:00+00:00 [queued]>
        <TaskInstance: example_skip_dag.always_true_2 2020-03-17 00:00:00+00:00 [queued]>
        <TaskInstance: example_skip_dag.always_true_1 2020-03-17 00:00:00+00:00 [queued]>
[2020-03-18 08:09:26,491] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'skip_operator_2', datetime.datetime(2020, 3, 17, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-18 08:09:26,493] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'skip_operator_2', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-18 08:09:26,495] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'skip_operator_1', datetime.datetime(2020, 3, 17, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-18 08:09:26,497] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'skip_operator_1', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-18 08:09:26,498] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'always_true_2', datetime.datetime(2020, 3, 17, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-18 08:09:26,500] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'always_true_2', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-18 08:09:26,501] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'always_true_1', datetime.datetime(2020, 3, 17, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-18 08:09:26,503] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'always_true_1', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-18 08:09:26,508] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'skip_operator_2', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-18 08:09:26,513] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'skip_operator_1', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-18 08:09:26,519] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'always_true_2', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-18 08:09:26,529] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'always_true_1', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-18 08:09:27,042] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:09:27,044] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-18 08:09:28,255] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: tutorial.print_date 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:09:28,374] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=347
[2020-03-18 08:09:28,413] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=348
[2020-03-18 08:09:28,544] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 121 open slots and 1 task instances ready to be queued
[2020-03-18 08:09:28,725] {scheduler_job.py:986} INFO - DAG tutorial has 0/16 running and queued tasks
[2020-03-18 08:09:29,176] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: tutorial.print_date 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:09:29,360] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: tutorial.print_date 2020-03-17 00:00:00+00:00 [queued]>
[2020-03-18 08:09:29,362] {scheduler_job.py:1148} INFO - Sending ('tutorial', 'print_date', datetime.datetime(2020, 3, 17, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-18 08:09:29,364] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'tutorial', 'print_date', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-18 08:09:29,367] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'tutorial', 'print_date', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-18 08:09:31,320] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.run_this_first 2020-03-17T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:09:33,692] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=388
[2020-03-18 08:09:33,698] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=391
[2020-03-18 08:09:33,740] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=389
[2020-03-18 08:09:34,735] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:09:34,756] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=390
[2020-03-18 08:09:34,762] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-18 08:09:34,841] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: example_http_operator.http_sensor_check 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:09:34,854] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 120 open slots and 1 task instances ready to be queued
[2020-03-18 08:09:34,855] {scheduler_job.py:986} INFO - DAG example_http_operator has 0/16 running and queued tasks
[2020-03-18 08:09:34,887] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_http_operator.http_sensor_check 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:09:35,062] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: example_http_operator.http_sensor_check 2020-03-17 00:00:00+00:00 [queued]>[2020-03-18 08:09:35,070] {__init__.py:51} INFO - Using executor LocalExecutor

[2020-03-18 08:09:35,074] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-18 08:09:35,074] {scheduler_job.py:1148} INFO - Sending ('example_http_operator', 'http_sensor_check', datetime.datetime(2020, 3, 17, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 6 and queue default
[2020-03-18 08:09:35,082] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_http_operator', 'http_sensor_check', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-18 08:09:35,089] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_http_operator', 'http_sensor_check', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-18 08:09:36,083] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-17T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:09:38,361] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=411
[2020-03-18 08:09:38,960] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-17T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:09:41,149] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-17 00:00:00+00:00 [scheduled]>[2020-03-18 08:09:41,233] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=448

[2020-03-18 08:09:41,492] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 119 open slots and 1 task instances ready to be queued
[2020-03-18 08:09:41,629] {scheduler_job.py:986} INFO - DAG latest_only has 0/16 running and queued tasks
[2020-03-18 08:09:41,694] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:09:41,834] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-18 08:09:41,887] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:09:42,055] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:09:42,057] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-18 08:09:42,231] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-17 00:00:00+00:00 [queued]>[2020-03-18 08:09:42,231] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:09:42,237] {__init__.py:51} INFO - Using executor LocalExecutor

[2020-03-18 08:09:42,246] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-18 08:09:42,249] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-18 08:09:42,252] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 17, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-18 08:09:42,264] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-18 08:09:42,274] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-18 08:09:42,283] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:09:42,285] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py
[2020-03-18 08:09:44,428] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.always_true_2 2020-03-17T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:09:44,558] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.always_true_1 2020-03-17T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:09:45,290] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:09:45,292] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py
[2020-03-18 08:09:45,582] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=522
[2020-03-18 08:09:45,643] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.skip_operator_1 2020-03-17T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:09:46,013] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.skip_operator_2 2020-03-17T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:09:46,096] {cli.py:545} INFO - Running <TaskInstance: tutorial.print_date 2020-03-17T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:09:47,543] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: example_short_circuit_operator.condition_is_True 2020-03-17 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.condition_is_False 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:09:47,568] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 118 open slots and 2 task instances ready to be queued
[2020-03-18 08:09:47,575] {scheduler_job.py:986} INFO - DAG example_short_circuit_operator has 0/16 running and queued tasks
[2020-03-18 08:09:47,576] {scheduler_job.py:986} INFO - DAG example_short_circuit_operator has 1/16 running and queued tasks
[2020-03-18 08:09:47,597] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_short_circuit_operator.condition_is_True 2020-03-17 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.condition_is_False 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:09:47,841] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: example_short_circuit_operator.condition_is_True 2020-03-17 00:00:00+00:00 [queued]>
        <TaskInstance: example_short_circuit_operator.condition_is_False 2020-03-17 00:00:00+00:00 [queued]>
[2020-03-18 08:09:47,845] {scheduler_job.py:1148} INFO - Sending ('example_short_circuit_operator', 'condition_is_True', datetime.datetime(2020, 3, 17, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-18 08:09:47,847] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_short_circuit_operator', 'condition_is_True', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-18 08:09:47,848] {scheduler_job.py:1148} INFO - Sending ('example_short_circuit_operator', 'condition_is_False', datetime.datetime(2020, 3, 17, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-18 08:09:47,850] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_short_circuit_operator', 'condition_is_False', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-18 08:09:47,854] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_short_circuit_operator', 'condition_is_True', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-18 08:09:47,856] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_short_circuit_operator', 'condition_is_False', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-18 08:09:49,323] {cli.py:545} INFO - Running <TaskInstance: example_http_operator.http_sensor_check 2020-03-17T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:09:50,010] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:09:50,012] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-18 08:09:52,192] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=596
[2020-03-18 08:09:52,382] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=597
[2020-03-18 08:09:54,934] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-17T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:09:55,912] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:09:55,914] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py
[2020-03-18 08:09:56,523] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:09:56,526] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py
[2020-03-18 08:10:04,078] {cli.py:545} INFO - Running <TaskInstance: example_short_circuit_operator.condition_is_True 2020-03-17T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:10:04,942] {cli.py:545} INFO - Running <TaskInstance: example_short_circuit_operator.condition_is_False 2020-03-17T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:10:15,528] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-17 04:00:00+00:00 [scheduled]>
[2020-03-18 08:10:15,538] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 118 open slots and 1 task instances ready to be queued
[2020-03-18 08:10:15,540] {scheduler_job.py:986} INFO - DAG latest_only has 1/16 running and queued tasks
[2020-03-18 08:10:15,553] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-17 04:00:00+00:00 [scheduled]>
[2020-03-18 08:10:15,656] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-17 04:00:00+00:00 [queued]>
[2020-03-18 08:10:15,657] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 17, 4, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-18 08:10:15,658] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-17T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-18 08:10:15,664] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-17T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-18 08:10:19,115] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=921
[2020-03-18 08:10:24,682] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:10:24,684] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-18 08:10:25,550] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-17 04:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-17 04:00:00+00:00 [scheduled]>
[2020-03-18 08:10:25,560] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 123 open slots and 2 task instances ready to be queued
[2020-03-18 08:10:25,561] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-18 08:10:25,562] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-18 08:10:25,573] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-17 04:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-17 04:00:00+00:00 [scheduled]>
[2020-03-18 08:10:25,817] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-17 04:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-17 04:00:00+00:00 [queued]>
[2020-03-18 08:10:25,819] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 17, 4, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-18 08:10:25,821] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-17T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:10:25,822] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 17, 4, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-18 08:10:25,824] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-17T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:10:25,828] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-17 00:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:10:25,833] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-17 00:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:10:25,831] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-17T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:10:25,831] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-17T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:10:29,554] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: example_branch_operator.branching 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:10:29,571] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 121 open slots and 1 task instances ready to be queued
[2020-03-18 08:10:29,572] {scheduler_job.py:986} INFO - DAG example_branch_operator has 0/16 running and queued tasks
[2020-03-18 08:10:29,589] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_branch_operator.branching 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:10:29,773] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: example_branch_operator.branching 2020-03-17 00:00:00+00:00 [queued]>
[2020-03-18 08:10:29,775] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'branching', datetime.datetime(2020, 3, 17, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 10 and queue default
[2020-03-18 08:10:29,777] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'branching', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-18 08:10:29,779] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.run_this_first execution_date=2020-03-17 00:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:10:29,780] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'branching', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-18 08:10:30,252] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1035
[2020-03-18 08:10:30,349] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-17T04:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:10:31,196] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1034
[2020-03-18 08:10:33,826] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1067
[2020-03-18 08:10:34,683] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:10:34,684] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-18 08:10:35,067] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:10:35,069] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-18 08:10:36,131] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:10:36,133] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-18 08:10:36,154] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-17T04:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:10:36,244] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-17T04:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:10:38,241] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.branching 2020-03-17T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:10:40,685] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: example_skip_dag.one_success 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:10:40,720] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 124 open slots and 1 task instances ready to be queued
[2020-03-18 08:10:40,729] {scheduler_job.py:986} INFO - DAG example_skip_dag has 0/16 running and queued tasks
[2020-03-18 08:10:40,754] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_skip_dag.one_success 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:10:40,933] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: example_skip_dag.one_success 2020-03-17 00:00:00+00:00 [queued]>
[2020-03-18 08:10:40,935] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'one_success', datetime.datetime(2020, 3, 17, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-18 08:10:40,936] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'one_success', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-18 08:10:40,938] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.always_true_2 execution_date=2020-03-17 00:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:10:40,940] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'one_success', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-18 08:10:40,946] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.always_true_1 execution_date=2020-03-17 00:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:10:40,953] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.skip_operator_1 execution_date=2020-03-17 00:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:10:40,956] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.skip_operator_2 execution_date=2020-03-17 00:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:10:42,654] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: tutorial.templated 2020-03-17 00:00:00+00:00 [scheduled]>
        <TaskInstance: tutorial.sleep 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:10:42,689] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 123 open slots and 2 task instances ready to be queued
[2020-03-18 08:10:42,694] {scheduler_job.py:986} INFO - DAG tutorial has 0/16 running and queued tasks
[2020-03-18 08:10:42,697] {scheduler_job.py:986} INFO - DAG tutorial has 1/16 running and queued tasks
[2020-03-18 08:10:42,728] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: tutorial.templated 2020-03-17 00:00:00+00:00 [scheduled]>
        <TaskInstance: tutorial.sleep 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:10:43,055] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: tutorial.templated 2020-03-17 00:00:00+00:00 [queued]>
        <TaskInstance: tutorial.sleep 2020-03-17 00:00:00+00:00 [queued]>
[2020-03-18 08:10:43,057] {scheduler_job.py:1148} INFO - Sending ('tutorial', 'templated', datetime.datetime(2020, 3, 17, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-18 08:10:43,058] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'tutorial', 'templated', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-18 08:10:43,059] {scheduler_job.py:1148} INFO - Sending ('tutorial', 'sleep', datetime.datetime(2020, 3, 17, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-18 08:10:43,060] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'tutorial', 'sleep', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-18 08:10:43,066] {scheduler_job.py:1288} INFO - Executor reports execution of tutorial.print_date execution_date=2020-03-17 00:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:10:43,065] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'tutorial', 'templated', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-18 08:10:43,066] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'tutorial', 'sleep', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-18 08:10:46,648] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: example_http_operator.post_op 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:10:46,657] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 121 open slots and 1 task instances ready to be queued
[2020-03-18 08:10:46,658] {scheduler_job.py:986} INFO - DAG example_http_operator has 0/16 running and queued tasks
[2020-03-18 08:10:46,670] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_http_operator.post_op 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:10:46,859] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: example_http_operator.post_op 2020-03-17 00:00:00+00:00 [queued]>
[2020-03-18 08:10:46,860] {scheduler_job.py:1148} INFO - Sending ('example_http_operator', 'post_op', datetime.datetime(2020, 3, 17, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 5 and queue default
[2020-03-18 08:10:46,861] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_http_operator', 'post_op', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-18 08:10:46,865] {scheduler_job.py:1288} INFO - Executor reports execution of example_http_operator.http_sensor_check execution_date=2020-03-17 00:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:10:46,865] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_http_operator', 'post_op', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-18 08:10:47,488] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1186
[2020-03-18 08:10:49,498] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1203
[2020-03-18 08:10:49,606] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1204
[2020-03-18 08:10:51,018] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-17 08:00:00+00:00 [scheduled]>
[2020-03-18 08:10:51,081] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 120 open slots and 1 task instances ready to be queued
[2020-03-18 08:10:51,087] {scheduler_job.py:986} INFO - DAG latest_only has 1/16 running and queued tasks
[2020-03-18 08:10:51,137] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-17 08:00:00+00:00 [scheduled]>
[2020-03-18 08:10:51,388] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-17 08:00:00+00:00 [queued]>
[2020-03-18 08:10:51,392] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 17, 8, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-18 08:10:51,394] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-17T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-18 08:10:51,401] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-17 00:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:10:51,402] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-17T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-18 08:10:52,663] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1233
[2020-03-18 08:10:53,642] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:10:53,643] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-18 08:10:54,830] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:10:54,834] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py
[2020-03-18 08:10:54,972] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:10:54,975] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py
[2020-03-18 08:10:55,647] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1282
[2020-03-18 08:10:56,793] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: example_short_circuit_operator.true_1 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:10:56,803] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 119 open slots and 1 task instances ready to be queued
[2020-03-18 08:10:56,804] {scheduler_job.py:986} INFO - DAG example_short_circuit_operator has 0/16 running and queued tasks
[2020-03-18 08:10:56,826] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_short_circuit_operator.true_1 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:10:56,958] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: example_short_circuit_operator.true_1 2020-03-17 00:00:00+00:00 [queued]>
[2020-03-18 08:10:56,973] {scheduler_job.py:1148} INFO - Sending ('example_short_circuit_operator', 'true_1', datetime.datetime(2020, 3, 17, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-18 08:10:56,975] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_short_circuit_operator', 'true_1', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-18 08:10:56,984] {scheduler_job.py:1288} INFO - Executor reports execution of example_short_circuit_operator.condition_is_True execution_date=2020-03-17 00:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:10:56,984] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_short_circuit_operator', 'true_1', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-18 08:10:57,006] {scheduler_job.py:1288} INFO - Executor reports execution of example_short_circuit_operator.condition_is_False execution_date=2020-03-17 00:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:10:57,271] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:10:57,276] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py
[2020-03-18 08:10:57,568] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.one_success 2020-03-17T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:10:57,726] {cli.py:545} INFO - Running <TaskInstance: tutorial.sleep 2020-03-17T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:10:57,953] {cli.py:545} INFO - Running <TaskInstance: tutorial.templated 2020-03-17T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:10:58,864] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:10:58,872] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-18 08:10:59,747] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1362
[2020-03-18 08:11:01,076] {cli.py:545} INFO - Running <TaskInstance: example_http_operator.post_op 2020-03-17T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:11:02,667] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-17T08:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:11:02,937] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:11:02,940] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py
[2020-03-18 08:11:05,139] {cli.py:545} INFO - Running <TaskInstance: example_short_circuit_operator.true_1 2020-03-17T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:11:16,808] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-17 12:00:00+00:00 [scheduled]>
[2020-03-18 08:11:16,818] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 122 open slots and 1 task instances ready to be queued
[2020-03-18 08:11:16,819] {scheduler_job.py:986} INFO - DAG latest_only has 1/16 running and queued tasks
[2020-03-18 08:11:16,841] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-17 12:00:00+00:00 [scheduled]>
[2020-03-18 08:11:16,983] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-17 12:00:00+00:00 [queued]>
[2020-03-18 08:11:16,985] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 17, 12, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-18 08:11:16,986] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-17T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-18 08:11:16,990] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-17 04:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:11:16,991] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-17T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-18 08:11:20,010] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1628
[2020-03-18 08:11:20,815] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-17 08:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-17 08:00:00+00:00 [scheduled]>
[2020-03-18 08:11:20,827] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 121 open slots and 2 task instances ready to be queued
[2020-03-18 08:11:20,828] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-18 08:11:20,829] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-18 08:11:20,844] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-17 08:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-17 08:00:00+00:00 [scheduled]>
[2020-03-18 08:11:20,931] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-17 08:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-17 08:00:00+00:00 [queued]>
[2020-03-18 08:11:20,934] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 17, 8, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-18 08:11:20,936] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-17T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:11:20,936] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 17, 8, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-18 08:11:20,938] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-17T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:11:20,943] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-17 04:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:11:20,945] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-17T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:11:20,946] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-17T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:11:20,952] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-17 04:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:11:24,536] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:11:24,538] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-18 08:11:24,850] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: example_branch_operator.branch_c 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:11:24,863] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 121 open slots and 1 task instances ready to be queued
[2020-03-18 08:11:24,865] {scheduler_job.py:986} INFO - DAG example_branch_operator has 0/16 running and queued tasks
[2020-03-18 08:11:24,879] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_branch_operator.branch_c 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:11:24,948] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: example_branch_operator.branch_c 2020-03-17 00:00:00+00:00 [queued]>
[2020-03-18 08:11:24,953] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'branch_c', datetime.datetime(2020, 3, 17, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-18 08:11:24,956] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'branch_c', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-18 08:11:24,963] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1679
[2020-03-18 08:11:24,965] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.branching execution_date=2020-03-17 00:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:11:24,965] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'branch_c', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-18 08:11:25,066] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1678
[2020-03-18 08:11:27,483] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-17T12:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:11:29,298] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1723
[2020-03-18 08:11:29,898] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:11:29,901] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-18 08:11:30,122] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:11:30,124] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-18 08:11:33,203] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:11:33,205] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-18 08:11:33,288] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-17T08:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:11:33,765] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-17T08:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:11:34,851] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: example_skip_dag.final_2 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:11:34,861] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 124 open slots and 1 task instances ready to be queued
[2020-03-18 08:11:34,862] {scheduler_job.py:986} INFO - DAG example_skip_dag has 0/16 running and queued tasks
[2020-03-18 08:11:34,875] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_skip_dag.final_2 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:11:34,999] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: example_skip_dag.final_2 2020-03-17 00:00:00+00:00 [queued]>
[2020-03-18 08:11:35,001] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'final_2', datetime.datetime(2020, 3, 17, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-18 08:11:35,003] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'final_2', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-18 08:11:35,006] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.one_success execution_date=2020-03-17 00:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:11:35,007] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'final_2', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-18 08:11:35,849] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.branch_c 2020-03-17T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:11:36,859] {scheduler_job.py:1288} INFO - Executor reports execution of tutorial.templated execution_date=2020-03-17 00:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:11:37,970] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1830
[2020-03-18 08:11:40,736] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:11:40,739] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-18 08:11:40,904] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-17 16:00:00+00:00 [scheduled]>
[2020-03-18 08:11:40,926] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 123 open slots and 1 task instances ready to be queued
[2020-03-18 08:11:40,929] {scheduler_job.py:986} INFO - DAG latest_only has 1/16 running and queued tasks
[2020-03-18 08:11:40,947] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-17 16:00:00+00:00 [scheduled]>
[2020-03-18 08:11:41,178] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-17 16:00:00+00:00 [queued]>
[2020-03-18 08:11:41,221] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 17, 16, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-18 08:11:41,253] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-17T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-18 08:11:41,368] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-17T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-18 08:11:41,449] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-17 08:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:11:43,409] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.final_2 2020-03-17T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:11:43,544] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: example_short_circuit_operator.true_2 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:11:43,553] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 122 open slots and 1 task instances ready to be queued
[2020-03-18 08:11:43,554] {scheduler_job.py:986} INFO - DAG example_short_circuit_operator has 0/16 running and queued tasks
[2020-03-18 08:11:43,567] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_short_circuit_operator.true_2 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:11:43,699] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: example_short_circuit_operator.true_2 2020-03-17 00:00:00+00:00 [queued]>
[2020-03-18 08:11:43,701] {scheduler_job.py:1148} INFO - Sending ('example_short_circuit_operator', 'true_2', datetime.datetime(2020, 3, 17, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-18 08:11:43,703] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_short_circuit_operator', 'true_2', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-18 08:11:43,707] {scheduler_job.py:1288} INFO - Executor reports execution of example_short_circuit_operator.true_1 execution_date=2020-03-17 00:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:11:43,708] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_short_circuit_operator', 'true_2', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-18 08:11:46,319] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1924
[2020-03-18 08:11:48,864] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1952
[2020-03-18 08:11:50,278] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:11:50,282] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-18 08:11:51,918] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:11:51,920] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py
[2020-03-18 08:11:52,743] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-17T16:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:11:54,349] {cli.py:545} INFO - Running <TaskInstance: example_short_circuit_operator.true_2 2020-03-17T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:11:57,070] {scheduler_job.py:1288} INFO - Executor reports execution of tutorial.sleep execution_date=2020-03-17 00:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:11:59,070] {scheduler_job.py:1288} INFO - Executor reports execution of example_http_operator.post_op execution_date=2020-03-17 00:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:12:01,096] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-17 20:00:00+00:00 [scheduled]>
[2020-03-18 08:12:01,128] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued
[2020-03-18 08:12:01,129] {scheduler_job.py:986} INFO - DAG latest_only has 1/16 running and queued tasks
[2020-03-18 08:12:01,157] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-17 20:00:00+00:00 [scheduled]>
[2020-03-18 08:12:01,258] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-17 20:00:00+00:00 [queued]>
[2020-03-18 08:12:01,260] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 17, 20, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-18 08:12:01,262] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-17T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-18 08:12:01,264] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-17 12:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:12:01,265] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-17T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-18 08:12:03,092] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-17 12:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-17 12:00:00+00:00 [scheduled]>
[2020-03-18 08:12:03,109] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 125 open slots and 2 task instances ready to be queued
[2020-03-18 08:12:03,110] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-18 08:12:03,114] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-18 08:12:03,127] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-17 12:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-17 12:00:00+00:00 [scheduled]>
[2020-03-18 08:12:03,189] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-17 12:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-17 12:00:00+00:00 [queued]>
[2020-03-18 08:12:03,193] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 17, 12, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-18 08:12:03,194] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-17T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:12:03,195] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 17, 12, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-18 08:12:03,197] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-17T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:12:03,204] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-17 08:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:12:03,204] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-17T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:12:03,206] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-17T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:12:03,211] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-17 08:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:12:04,484] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2195
[2020-03-18 08:12:05,088] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: example_branch_operator.follow_branch_c 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:12:05,098] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 123 open slots and 1 task instances ready to be queued
[2020-03-18 08:12:05,099] {scheduler_job.py:986} INFO - DAG example_branch_operator has 0/16 running and queued tasks
[2020-03-18 08:12:05,127] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_branch_operator.follow_branch_c 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:12:05,355] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: example_branch_operator.follow_branch_c 2020-03-17 00:00:00+00:00 [queued]>
[2020-03-18 08:12:05,357] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'follow_branch_c', datetime.datetime(2020, 3, 17, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-18 08:12:05,358] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'follow_branch_c', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-18 08:12:05,361] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.branch_c execution_date=2020-03-17 00:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:12:05,361] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'follow_branch_c', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-18 08:12:07,658] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2221
[2020-03-18 08:12:07,712] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2220
[2020-03-18 08:12:08,173] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:12:08,175] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-18 08:12:09,294] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2256
[2020-03-18 08:12:09,846] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-17T20:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:12:10,468] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:12:10,472] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-18 08:12:10,645] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:12:10,646] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-18 08:12:11,114] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.final_2 execution_date=2020-03-17 00:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:12:11,302] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-17T12:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:12:11,497] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-17T12:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:12:11,877] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:12:11,879] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-18 08:12:13,043] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.follow_branch_c 2020-03-17T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:12:17,108] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-18 00:00:00+00:00 [scheduled]>
[2020-03-18 08:12:17,119] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 124 open slots and 1 task instances ready to be queued
[2020-03-18 08:12:17,120] {scheduler_job.py:986} INFO - DAG latest_only has 1/16 running and queued tasks
[2020-03-18 08:12:17,137] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-18 00:00:00+00:00 [scheduled]>
[2020-03-18 08:12:17,282] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-18 00:00:00+00:00 [queued]>
[2020-03-18 08:12:17,284] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 18, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-18 08:12:17,285] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-18T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-18 08:12:17,288] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-17 16:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:12:17,289] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-18T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-18 08:12:21,119] {scheduler_job.py:1288} INFO - Executor reports execution of example_short_circuit_operator.true_2 execution_date=2020-03-17 00:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:12:22,263] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2411
[2020-03-18 08:12:25,624] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:12:25,626] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-18 08:12:27,411] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-18T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:12:35,124] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-18 04:00:00+00:00 [scheduled]>
[2020-03-18 08:12:35,134] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 127 open slots and 1 task instances ready to be queued
[2020-03-18 08:12:35,135] {scheduler_job.py:986} INFO - DAG latest_only has 1/16 running and queued tasks
[2020-03-18 08:12:35,146] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-18 04:00:00+00:00 [scheduled]>
[2020-03-18 08:12:35,345] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-18 04:00:00+00:00 [queued]>
[2020-03-18 08:12:35,347] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 18, 4, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-18 08:12:35,347] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-18T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-18 08:12:35,351] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-17 20:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:12:35,351] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-18T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-18 08:12:37,136] {scheduler_job.py:927} INFO - 3 tasks up for execution:
        <TaskInstance: example_branch_operator.join 2020-03-17 00:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-17 16:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-17 16:00:00+00:00 [scheduled]>
[2020-03-18 08:12:37,150] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 127 open slots and 3 task instances ready to be queued
[2020-03-18 08:12:37,151] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-18 08:12:37,152] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-18 08:12:37,152] {scheduler_job.py:986} INFO - DAG example_branch_operator has 0/16 running and queued tasks
[2020-03-18 08:12:37,168] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-17 16:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-17 16:00:00+00:00 [scheduled]>
        <TaskInstance: example_branch_operator.join 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:12:37,225] {scheduler_job.py:1112} INFO - Setting the following 3 tasks to queued state:
        <TaskInstance: example_branch_operator.join 2020-03-17 00:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-17 16:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-17 16:00:00+00:00 [queued]>
[2020-03-18 08:12:37,228] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'join', datetime.datetime(2020, 3, 17, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-18 08:12:37,230] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'join', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-18 08:12:37,231] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 17, 16, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-18 08:12:37,232] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-17T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:12:37,233] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 17, 16, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-18 08:12:37,234] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-17T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:12:37,238] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-17 12:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:12:37,237] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-17T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:12:37,238] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-17T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:12:37,239] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'join', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-18 08:12:37,254] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-17 12:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:12:37,265] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.follow_branch_c execution_date=2020-03-17 00:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:12:37,380] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2651
[2020-03-18 08:12:39,634] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:12:39,636] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-18 08:12:40,258] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-18T04:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:12:40,661] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2677
[2020-03-18 08:12:41,074] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2678
[2020-03-18 08:12:41,163] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2679
[2020-03-18 08:12:42,890] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:12:42,897] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-18 08:12:43,280] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-17T16:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:12:43,702] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:12:43,703] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-18 08:12:43,851] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:12:43,852] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-18 08:12:44,861] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-17T16:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:12:45,045] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.join 2020-03-17T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:12:47,171] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-18 08:00:00+00:00 [scheduled]>
[2020-03-18 08:12:47,185] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 124 open slots and 1 task instances ready to be queued
[2020-03-18 08:12:47,187] {scheduler_job.py:986} INFO - DAG latest_only has 1/16 running and queued tasks
[2020-03-18 08:12:47,202] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-18 08:00:00+00:00 [scheduled]>
[2020-03-18 08:12:47,465] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-18 08:00:00+00:00 [queued]>
[2020-03-18 08:12:47,467] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 18, 8, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-18 08:12:47,468] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-18T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-18 08:12:47,471] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-18 00:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:12:47,471] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-18T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-18 08:12:51,222] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2832
[2020-03-18 08:12:54,791] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:12:54,794] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-18 08:12:55,939] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-18T08:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:13:03,203] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-18 04:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:13:07,185] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-17 20:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-17 20:00:00+00:00 [scheduled]>
[2020-03-18 08:13:07,195] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 127 open slots and 2 task instances ready to be queued
[2020-03-18 08:13:07,196] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-18 08:13:07,197] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-18 08:13:07,212] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-17 20:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-17 20:00:00+00:00 [scheduled]>
[2020-03-18 08:13:07,437] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-17 20:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-17 20:00:00+00:00 [queued]>
[2020-03-18 08:13:07,440] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 17, 20, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-18 08:13:07,440] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-17T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:13:07,442] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 17, 20, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-18 08:13:07,444] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-17T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:13:07,446] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-17 16:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:13:07,447] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-17T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:13:07,447] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-17T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:13:07,452] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-17 16:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:13:09,207] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.join execution_date=2020-03-17 00:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:13:10,847] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3085
[2020-03-18 08:13:10,979] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3086
[2020-03-18 08:13:14,007] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:13:14,008] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-18 08:13:14,251] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:13:14,252] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-18 08:13:14,594] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-17T20:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:13:14,667] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-17T20:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:13:19,193] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.task1 2020-03-18 08:00:00+00:00 [scheduled]>
[2020-03-18 08:13:19,201] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued
[2020-03-18 08:13:19,202] {scheduler_job.py:986} INFO - DAG latest_only has 0/16 running and queued tasks
[2020-03-18 08:13:19,215] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.task1 2020-03-18 08:00:00+00:00 [scheduled]>
[2020-03-18 08:13:19,437] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.task1 2020-03-18 08:00:00+00:00 [queued]>
[2020-03-18 08:13:19,439] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'task1', datetime.datetime(2020, 3, 18, 8, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-18 08:13:19,441] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'task1', '2020-03-18T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-18 08:13:19,443] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-18 08:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:13:19,443] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'task1', '2020-03-18T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-18 08:13:23,901] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3204
[2020-03-18 08:13:26,292] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:13:26,294] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-18 08:13:28,714] {cli.py:545} INFO - Running <TaskInstance: latest_only.task1 2020-03-18T08:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:13:37,207] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-18 00:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-18 00:00:00+00:00 [scheduled]>
[2020-03-18 08:13:37,220] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 2 task instances ready to be queued
[2020-03-18 08:13:37,223] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-18 08:13:37,224] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-18 08:13:37,240] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-18 00:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-18 00:00:00+00:00 [scheduled]>
[2020-03-18 08:13:37,395] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-18 00:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-18 00:00:00+00:00 [queued]>
[2020-03-18 08:13:37,398] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 18, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-18 08:13:37,400] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-18T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:13:37,401] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 18, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-18 08:13:37,403] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-18T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:13:37,408] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-17 20:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:13:37,408] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-18T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:13:37,409] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-18T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:13:37,427] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-17 20:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:13:40,309] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3417
[2020-03-18 08:13:40,355] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3416
[2020-03-18 08:13:42,149] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:13:42,153] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-18 08:13:42,526] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:13:42,528] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-18 08:13:42,556] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-18T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:13:42,854] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-18T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:13:47,215] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.task1 execution_date=2020-03-18 08:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:14:01,220] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-18 04:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-18 04:00:00+00:00 [scheduled]>
[2020-03-18 08:14:01,232] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 2 task instances ready to be queued
[2020-03-18 08:14:01,235] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-18 08:14:01,236] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-18 08:14:01,254] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-18 04:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-18 04:00:00+00:00 [scheduled]>
[2020-03-18 08:14:01,396] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-18 04:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-18 04:00:00+00:00 [queued]>
[2020-03-18 08:14:01,399] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 18, 4, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-18 08:14:01,402] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-18T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:14:01,404] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 18, 4, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-18 08:14:01,406] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-18T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:14:01,409] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-18 00:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:14:01,409] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-18T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:14:01,409] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-18T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:14:01,415] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-18 00:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:14:04,265] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3715
[2020-03-18 08:14:04,354] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3714
[2020-03-18 08:14:06,527] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:14:06,529] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-18 08:14:06,756] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:14:06,758] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-18 08:14:06,974] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-18T04:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:14:07,383] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-18T04:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:14:27,230] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-18 08:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-18 08:00:00+00:00 [scheduled]>
[2020-03-18 08:14:27,239] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 2 task instances ready to be queued
[2020-03-18 08:14:27,240] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-18 08:14:27,241] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-18 08:14:27,255] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-18 08:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-18 08:00:00+00:00 [scheduled]>
[2020-03-18 08:14:27,370] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-18 08:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-18 08:00:00+00:00 [queued]>
[2020-03-18 08:14:27,374] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 18, 8, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-18 08:14:27,376] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-18T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:14:27,377] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 18, 8, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-18 08:14:27,379] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-18T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:14:27,383] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-18T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:14:27,383] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-18 04:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:14:27,383] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-18T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:14:27,390] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-18 04:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:14:30,403] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=4017
[2020-03-18 08:14:30,487] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=4016
[2020-03-18 08:14:32,827] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:14:32,829] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-18 08:14:32,896] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:14:32,897] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-18 08:14:33,155] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-18T08:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:14:33,255] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-18T08:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:14:51,247] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task1 2020-03-18 08:00:00+00:00 [scheduled]>
[2020-03-18 08:14:51,257] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued
[2020-03-18 08:14:51,260] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-18 08:14:51,279] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task1 2020-03-18 08:00:00+00:00 [scheduled]>
[2020-03-18 08:14:51,446] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task1 2020-03-18 08:00:00+00:00 [queued]>
[2020-03-18 08:14:51,448] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task1', datetime.datetime(2020, 3, 18, 8, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-18 08:14:51,450] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task1', '2020-03-18T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:14:51,453] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-18 08:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:14:51,453] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task1', '2020-03-18T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:14:51,459] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-18 08:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:14:53,597] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=4316
[2020-03-18 08:14:55,476] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:14:55,478] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-18 08:14:55,960] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task1 2020-03-18T08:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:15:13,272] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task4 2020-03-18 08:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task3 2020-03-18 08:00:00+00:00 [scheduled]>
[2020-03-18 08:15:13,286] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 2 task instances ready to be queued
[2020-03-18 08:15:13,287] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-18 08:15:13,290] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-18 08:15:13,303] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task4 2020-03-18 08:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task3 2020-03-18 08:00:00+00:00 [scheduled]>
[2020-03-18 08:15:13,372] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task4 2020-03-18 08:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.task3 2020-03-18 08:00:00+00:00 [queued]>
[2020-03-18 08:15:13,374] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task4', datetime.datetime(2020, 3, 18, 8, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-18 08:15:13,375] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task4', '2020-03-18T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:15:13,376] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task3', datetime.datetime(2020, 3, 18, 8, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-18 08:15:13,377] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task3', '2020-03-18T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:15:13,380] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task1 execution_date=2020-03-18 08:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:15:13,380] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task4', '2020-03-18T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:15:13,380] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task3', '2020-03-18T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 08:15:16,636] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=4561
[2020-03-18 08:15:16,894] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=4562
[2020-03-18 08:15:18,814] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:15:18,816] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-18 08:15:19,009] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:15:19,011] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-18 08:15:19,536] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task4 2020-03-18T08:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:15:19,689] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task3 2020-03-18T08:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:15:41,284] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task4 execution_date=2020-03-18 08:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:15:41,290] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task3 execution_date=2020-03-18 08:00:00+00:00 exited with status success for try_number 1
[2020-03-18 08:16:29,300] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: example_http_operator.post_op 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:16:29,310] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued
[2020-03-18 08:16:29,313] {scheduler_job.py:986} INFO - DAG example_http_operator has 0/16 running and queued tasks
[2020-03-18 08:16:29,327] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_http_operator.post_op 2020-03-17 00:00:00+00:00 [scheduled]>
[2020-03-18 08:16:29,544] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: example_http_operator.post_op 2020-03-17 00:00:00+00:00 [queued]>
[2020-03-18 08:16:29,545] {scheduler_job.py:1148} INFO - Sending ('example_http_operator', 'post_op', datetime.datetime(2020, 3, 17, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 2) to executor with priority 5 and queue default
[2020-03-18 08:16:29,547] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_http_operator', 'post_op', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-18 08:16:29,549] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_http_operator', 'post_op', '2020-03-17T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-18 08:16:31,448] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=5278
[2020-03-18 08:16:33,106] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 08:16:33,108] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py
[2020-03-18 08:16:33,347] {cli.py:545} INFO - Running <TaskInstance: example_http_operator.post_op 2020-03-17T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 08:17:08,142] {scheduler_job.py:1288} INFO - Executor reports execution of example_http_operator.post_op execution_date=2020-03-17 00:00:00+00:00 exited with status success for try_number 2
[2020-03-18 12:53:31,043] {scheduler_job.py:224} WARNING - Killing PID 15050
[2020-03-18 12:53:31,132] {scheduler_job.py:224} WARNING - Killing PID 15050
[2020-03-18 12:53:39,383] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-18 12:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-18 12:00:00+00:00 [scheduled]>
[2020-03-18 12:53:39,401] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 2 task instances ready to be queued
[2020-03-18 12:53:39,402] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-18 12:53:39,406] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-18 12:53:39,432] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-18 12:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-18 12:00:00+00:00 [scheduled]>
[2020-03-18 12:53:39,743] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-18 12:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-18 12:00:00+00:00 [queued]>
[2020-03-18 12:53:39,781] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 18, 12, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-18 12:53:39,783] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-18T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 12:53:39,785] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 18, 12, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-18 12:53:39,786] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-18T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 12:53:39,789] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-18T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 12:53:39,789] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-18T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 12:53:45,396] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=15118
[2020-03-18 12:53:45,397] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=15119
[2020-03-18 12:53:48,374] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 12:53:48,375] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-18 12:53:48,619] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 12:53:48,621] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-18 12:53:49,222] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-18T12:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 12:53:49,649] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-18T12:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 12:53:51,394] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-18 12:00:00+00:00 [scheduled]>
[2020-03-18 12:53:51,405] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued
[2020-03-18 12:53:51,407] {scheduler_job.py:986} INFO - DAG latest_only has 0/16 running and queued tasks
[2020-03-18 12:53:51,420] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-18 12:00:00+00:00 [scheduled]>
[2020-03-18 12:53:51,995] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-18 12:00:00+00:00 [queued]>
[2020-03-18 12:53:51,997] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 18, 12, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-18 12:53:51,998] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-18T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-18 12:53:52,001] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-18T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-18 12:53:55,815] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=15235
[2020-03-18 12:53:57,805] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 12:53:57,807] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-18 12:53:58,102] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-18T12:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 12:54:11,398] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task1 2020-03-18 12:00:00+00:00 [scheduled]>
[2020-03-18 12:54:11,414] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 127 open slots and 1 task instances ready to be queued
[2020-03-18 12:54:11,415] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-18 12:54:11,431] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task1 2020-03-18 12:00:00+00:00 [scheduled]>
[2020-03-18 12:54:11,766] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task1 2020-03-18 12:00:00+00:00 [queued]>
[2020-03-18 12:54:11,779] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task1', datetime.datetime(2020, 3, 18, 12, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-18 12:54:11,788] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task1', '2020-03-18T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 12:54:11,796] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-18 12:00:00+00:00 exited with status success for try_number 1
[2020-03-18 12:54:11,796] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task1', '2020-03-18T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 12:54:11,823] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-18 12:00:00+00:00 exited with status success for try_number 1
[2020-03-18 12:54:15,344] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=15472
[2020-03-18 12:54:17,591] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 12:54:17,593] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-18 12:54:18,322] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task1 2020-03-18T12:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 12:54:23,413] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.task1 2020-03-18 12:00:00+00:00 [scheduled]>
[2020-03-18 12:54:23,436] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 127 open slots and 1 task instances ready to be queued
[2020-03-18 12:54:23,437] {scheduler_job.py:986} INFO - DAG latest_only has 0/16 running and queued tasks
[2020-03-18 12:54:23,455] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.task1 2020-03-18 12:00:00+00:00 [scheduled]>
[2020-03-18 12:54:23,818] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.task1 2020-03-18 12:00:00+00:00 [queued]>
[2020-03-18 12:54:23,820] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'task1', datetime.datetime(2020, 3, 18, 12, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-18 12:54:23,821] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'task1', '2020-03-18T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-18 12:54:23,824] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-18 12:00:00+00:00 exited with status success for try_number 1
[2020-03-18 12:54:23,824] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'task1', '2020-03-18T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-18 12:54:27,953] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=15573
[2020-03-18 12:54:30,388] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 12:54:30,390] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-18 12:54:31,066] {cli.py:545} INFO - Running <TaskInstance: latest_only.task1 2020-03-18T12:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 12:54:37,422] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task4 2020-03-18 12:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task3 2020-03-18 12:00:00+00:00 [scheduled]>
[2020-03-18 12:54:37,446] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 127 open slots and 2 task instances ready to be queued
[2020-03-18 12:54:37,447] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-18 12:54:37,448] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-18 12:54:37,470] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task4 2020-03-18 12:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task3 2020-03-18 12:00:00+00:00 [scheduled]>
[2020-03-18 12:54:37,565] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task4 2020-03-18 12:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.task3 2020-03-18 12:00:00+00:00 [queued]>
[2020-03-18 12:54:37,568] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task4', datetime.datetime(2020, 3, 18, 12, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-18 12:54:37,570] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task4', '2020-03-18T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 12:54:37,572] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task3', datetime.datetime(2020, 3, 18, 12, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-18 12:54:37,574] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task3', '2020-03-18T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 12:54:37,577] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task4', '2020-03-18T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 12:54:37,579] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task1 execution_date=2020-03-18 12:00:00+00:00 exited with status success for try_number 1
[2020-03-18 12:54:37,578] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task3', '2020-03-18T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-18 12:54:40,688] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=15742
[2020-03-18 12:54:40,779] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=15744
[2020-03-18 12:54:43,003] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 12:54:43,005] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-18 12:54:43,119] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-18 12:54:43,121] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-18 12:54:43,806] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task3 2020-03-18T12:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 12:54:44,521] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task4 2020-03-18T12:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-18 12:54:49,848] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.task1 execution_date=2020-03-18 12:00:00+00:00 exited with status success for try_number 1
[2020-03-18 12:55:19,515] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task3 execution_date=2020-03-18 12:00:00+00:00 exited with status success for try_number 1
[2020-03-18 12:55:19,525] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task4 execution_date=2020-03-18 12:00:00+00:00 exited with status success for try_number 1
[2020-03-18 14:59:25,362] {scheduler_job.py:224} WARNING - Killing PID 4807
[2020-03-18 22:59:17,785] {base_job.py:206} ERROR - SchedulerJob heartbeat got an exception
Traceback (most recent call last):
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 799, in _checkout
    raise exc.InvalidatePoolError()
sqlalchemy.exc.InvalidatePoolError: ()

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2276, in _wrap_pool_connect
    return fn()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 363, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 829, in _checkout
    fairy._connection_record._checkin_failed(err)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 153, in reraise
    raise value
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 825, in _checkout
    fairy._connection_record.get_connection()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 593, in get_connection
    self.__connect()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 639, in __connect
    connection = pool._invoke_creator(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/usr/lib/python3/dist-packages/psycopg2/__init__.py", line 126, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5432?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/jobs/base_job.py", line 173, in heartbeat
    session.merge(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 2104, in merge
    _resolve_conflict_map=_resolve_conflict_map,
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 2177, in _merge
    merged = self.query(mapper.class_).get(key[1])
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 982, in get
    return self._get_impl(ident, loading.load_on_pk_identity)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 1094, in _get_impl
    return db_load_fn(self, primary_key_identity)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/loading.py", line 284, in load_on_pk_identity
    return q.one()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3325, in one
    ret = self.one_or_none()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3294, in one_or_none
    ret = list(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3367, in __iter__
    return self._execute_and_instances(context)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3389, in _execute_and_instances
    querycontext, self._connection_from_session, close_with_result=True
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3404, in _get_bind_args
    mapper=self._bind_mapper(), clause=querycontext.statement, **kw
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3382, in _connection_from_session
    conn = self.session.connection(**kw)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1124, in connection
    execution_options=execution_options,
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1130, in _connection_for_bind
    engine, execution_options
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 431, in _connection_for_bind
    conn = bind._contextual_connect()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2242, in _contextual_connect
    self._wrap_pool_connect(self.pool.connect, None),
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2280, in _wrap_pool_connect
    e, dialect, self
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1547, in _handle_dbapi_exception_noconnection
    util.raise_from_cause(sqlalchemy_exception, exc_info)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 398, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 152, in reraise
    raise value.with_traceback(tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2276, in _wrap_pool_connect
    return fn()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 363, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 829, in _checkout
    fairy._connection_record._checkin_failed(err)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 153, in reraise
    raise value
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 825, in _checkout
    fairy._connection_record.get_connection()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 593, in get_connection
    self.__connect()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 639, in __connect
    connection = pool._invoke_creator(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/usr/lib/python3/dist-packages/psycopg2/__init__.py", line 126, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5432?

(Background on this error at: http://sqlalche.me/e/e3q8)Process DagFileProcessor11048-Process:
Process DagFileProcessor11047-Process:

Traceback (most recent call last):
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2276, in _wrap_pool_connect
    return fn()
Traceback (most recent call last):
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 363, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2276, in _wrap_pool_connect
    return fn()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 363, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 492, in checkout
    rec = pool._do_get()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/impl.py", line 139, in _do_get
    self._dec_overflow()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 492, in checkout
    rec = pool._do_get()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/impl.py", line 139, in _do_get
    self._dec_overflow()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 153, in reraise
    raise value
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/impl.py", line 136, in _do_get
    return self._create_connection()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 153, in reraise
    raise value
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 308, in _create_connection
    return _ConnectionRecord(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/impl.py", line 136, in _do_get
    return self._create_connection()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 437, in __init__
    self.__connect(first_connect_check=True)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 308, in _create_connection
    return _ConnectionRecord(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 639, in __connect
    connection = pool._invoke_creator(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 437, in __init__
    self.__connect(first_connect_check=True)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 639, in __connect
    connection = pool._invoke_creator(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File "/usr/lib/python3/dist-packages/psycopg2/__init__.py", line 126, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
psycopg2.OperationalError: could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5432?

  File "/usr/lib/python3/dist-packages/psycopg2/__init__.py", line 126, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)

The above exception was the direct cause of the following exception:

psycopg2.OperationalError: could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5432?

Traceback (most recent call last):

The above exception was the direct cause of the following exception:

  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/jobs/scheduler_job.py", line 157, in _run_file_processor
    pickle_dags)
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/utils/db.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/jobs/scheduler_job.py", line 157, in _run_file_processor
    pickle_dags)
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/jobs/scheduler_job.py", line 1548, in process_file
    dag.sync_to_db()
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/utils/db.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/utils/db.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/jobs/scheduler_job.py", line 1548, in process_file
    dag.sync_to_db()
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/models/dag.py", line 1364, in sync_to_db
    DagModel).filter(DagModel.dag_id == self.dag_id).first()
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/utils/db.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3265, in first
    ret = list(self[0:1])
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/models/dag.py", line 1364, in sync_to_db
    DagModel).filter(DagModel.dag_id == self.dag_id).first()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3043, in __getitem__
    return list(res)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3265, in first
    ret = list(self[0:1])
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3367, in __iter__
    return self._execute_and_instances(context)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3043, in __getitem__
    return list(res)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3389, in _execute_and_instances
    querycontext, self._connection_from_session, close_with_result=True
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3367, in __iter__
    return self._execute_and_instances(context)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3404, in _get_bind_args
    mapper=self._bind_mapper(), clause=querycontext.statement, **kw
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3389, in _execute_and_instances
    querycontext, self._connection_from_session, close_with_result=True
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3382, in _connection_from_session
    conn = self.session.connection(**kw)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3404, in _get_bind_args
    mapper=self._bind_mapper(), clause=querycontext.statement, **kw
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1124, in connection
    execution_options=execution_options,
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3382, in _connection_from_session
    conn = self.session.connection(**kw)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1130, in _connection_for_bind
    engine, execution_options
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1124, in connection
    execution_options=execution_options,
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 431, in _connection_for_bind
    conn = bind._contextual_connect()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1130, in _connection_for_bind
    engine, execution_options
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2242, in _contextual_connect
    self._wrap_pool_connect(self.pool.connect, None),
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 431, in _connection_for_bind
    conn = bind._contextual_connect()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2280, in _wrap_pool_connect
    e, dialect, self
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2242, in _contextual_connect
    self._wrap_pool_connect(self.pool.connect, None),
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1547, in _handle_dbapi_exception_noconnection
    util.raise_from_cause(sqlalchemy_exception, exc_info)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2280, in _wrap_pool_connect
    e, dialect, self
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 398, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1547, in _handle_dbapi_exception_noconnection
    util.raise_from_cause(sqlalchemy_exception, exc_info)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 152, in reraise
    raise value.with_traceback(tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 398, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2276, in _wrap_pool_connect
    return fn()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 152, in reraise
    raise value.with_traceback(tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 363, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2276, in _wrap_pool_connect
    return fn()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 363, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 492, in checkout
    rec = pool._do_get()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/impl.py", line 139, in _do_get
    self._dec_overflow()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 492, in checkout
    rec = pool._do_get()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/impl.py", line 139, in _do_get
    self._dec_overflow()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 153, in reraise
    raise value
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/impl.py", line 136, in _do_get
    return self._create_connection()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 153, in reraise
    raise value
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 308, in _create_connection
    return _ConnectionRecord(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/impl.py", line 136, in _do_get
    return self._create_connection()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 437, in __init__
    self.__connect(first_connect_check=True)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 308, in _create_connection
    return _ConnectionRecord(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 639, in __connect
    connection = pool._invoke_creator(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 437, in __init__
    self.__connect(first_connect_check=True)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 639, in __connect
    connection = pool._invoke_creator(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File "/usr/lib/python3/dist-packages/psycopg2/__init__.py", line 126, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5432?

(Background on this error at: http://sqlalche.me/e/e3q8)
  File "/usr/lib/python3/dist-packages/psycopg2/__init__.py", line 126, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5432?

(Background on this error at: http://sqlalche.me/e/e3q8)
Process Process-34:
Traceback (most recent call last):
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2276, in _wrap_pool_connect
    return fn()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 363, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 497, in checkout
    rec._checkin_failed(err)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 153, in reraise
    raise value
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 494, in checkout
    dbapi_connection = rec.get_connection()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 593, in get_connection
    self.__connect()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 639, in __connect
    connection = pool._invoke_creator(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/usr/lib/python3/dist-packages/psycopg2/__init__.py", line 126, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5432?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/utils/dag_processing.py", line 624, in _run_processor_manager
    processor_manager.start()
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/utils/dag_processing.py", line 859, in start
    self._find_zombies()
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/utils/db.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/utils/dag_processing.py", line 1299, in _find_zombies
    LJ.latest_heartbeat < limit_dttm,
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3211, in all
    return list(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3367, in __iter__
    return self._execute_and_instances(context)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3389, in _execute_and_instances
    querycontext, self._connection_from_session, close_with_result=True
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3404, in _get_bind_args
    mapper=self._bind_mapper(), clause=querycontext.statement, **kw
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3382, in _connection_from_session
    conn = self.session.connection(**kw)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1124, in connection
    execution_options=execution_options,
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1130, in _connection_for_bind
    engine, execution_options
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 431, in _connection_for_bind
    conn = bind._contextual_connect()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2242, in _contextual_connect
    self._wrap_pool_connect(self.pool.connect, None),
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2280, in _wrap_pool_connect
    e, dialect, self
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1547, in _handle_dbapi_exception_noconnection
    util.raise_from_cause(sqlalchemy_exception, exc_info)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 398, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 152, in reraise
    raise value.with_traceback(tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2276, in _wrap_pool_connect
    return fn()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 363, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 497, in checkout
    rec._checkin_failed(err)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 153, in reraise
    raise value
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 494, in checkout
    dbapi_connection = rec.get_connection()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 593, in get_connection
    self.__connect()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 639, in __connect
    connection = pool._invoke_creator(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/usr/lib/python3/dist-packages/psycopg2/__init__.py", line 126, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5432?

(Background on this error at: http://sqlalche.me/e/e3q8)
[2020-03-18 23:00:44,629] {dag_processing.py:663} WARNING - DagFileProcessorManager (PID=279) exited with exit code 1 - re-launching
[2020-03-18 23:00:44,666] {dag_processing.py:556} INFO - Launched DagFileProcessorManager with pid: 7274
[2020-03-18 23:00:44,689] {scheduler_job.py:1439} ERROR - Error queuing tasks
[2020-03-18 23:00:44,692] {scheduler_job.py:1440} ERROR - (psycopg2.OperationalError) could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5432?

(Background on this error at: http://sqlalche.me/e/e3q8)
Traceback (most recent call last):
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2276, in _wrap_pool_connect
    return fn()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 363, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 497, in checkout
    rec._checkin_failed(err)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 153, in reraise
    raise value
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 494, in checkout
    dbapi_connection = rec.get_connection()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 593, in get_connection
    self.__connect()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 639, in __connect
    connection = pool._invoke_creator(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/usr/lib/python3/dist-packages/psycopg2/__init__.py", line 126, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5432?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/jobs/scheduler_job.py", line 1426, in _execute_helper
    State.FAILED)
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/utils/db.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/jobs/scheduler_job.py", line 832, in _change_state_for_tis_without_dagrun
    synchronize_session=False)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3887, in update
    update_op.exec_()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/persistence.py", line 1693, in exec_
    self._do_exec()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/persistence.py", line 1886, in _do_exec
    self._execute_stmt(update_stmt)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/persistence.py", line 1698, in _execute_stmt
    self.result = self.query._execute_crud(stmt, self.mapper)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3397, in _execute_crud
    mapper=mapper, clause=stmt, close_with_result=True
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3382, in _connection_from_session
    conn = self.session.connection(**kw)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1124, in connection
    execution_options=execution_options,
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1130, in _connection_for_bind
    engine, execution_options
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 431, in _connection_for_bind
    conn = bind._contextual_connect()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2242, in _contextual_connect
    self._wrap_pool_connect(self.pool.connect, None),
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2280, in _wrap_pool_connect
    e, dialect, self
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1547, in _handle_dbapi_exception_noconnection
    util.raise_from_cause(sqlalchemy_exception, exc_info)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 398, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 152, in reraise
    raise value.with_traceback(tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2276, in _wrap_pool_connect
    return fn()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 363, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 497, in checkout
    rec._checkin_failed(err)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 153, in reraise
    raise value
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 494, in checkout
    dbapi_connection = rec.get_connection()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 593, in get_connection
    self.__connect()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 639, in __connect
    connection = pool._invoke_creator(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/usr/lib/python3/dist-packages/psycopg2/__init__.py", line 126, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5432?

(Background on this error at: http://sqlalche.me/e/e3q8)[2020-03-18 23:00:44,700] {settings.py:54} INFO - Configured default timezone <Timezone [UTC]>

[2020-03-18 23:00:44,816] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=7274
Process Process-35:
Traceback (most recent call last):
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2276, in _wrap_pool_connect
    return fn()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 363, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 492, in checkout
    rec = pool._do_get()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/impl.py", line 139, in _do_get
    self._dec_overflow()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 153, in reraise
    raise value
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/impl.py", line 136, in _do_get
    return self._create_connection()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 308, in _create_connection
    return _ConnectionRecord(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 437, in __init__
    self.__connect(first_connect_check=True)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 639, in __connect
    connection = pool._invoke_creator(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/usr/lib/python3/dist-packages/psycopg2/__init__.py", line 126, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5432?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/utils/dag_processing.py", line 624, in _run_processor_manager
    processor_manager.start()
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/utils/dag_processing.py", line 859, in start
    self._find_zombies()
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/utils/db.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/utils/dag_processing.py", line 1299, in _find_zombies
    LJ.latest_heartbeat < limit_dttm,
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3211, in all
    return list(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3367, in __iter__
    return self._execute_and_instances(context)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3389, in _execute_and_instances
    querycontext, self._connection_from_session, close_with_result=True
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3404, in _get_bind_args
    mapper=self._bind_mapper(), clause=querycontext.statement, **kw
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3382, in _connection_from_session
    conn = self.session.connection(**kw)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1124, in connection
    execution_options=execution_options,
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1130, in _connection_for_bind
    engine, execution_options
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 431, in _connection_for_bind
    conn = bind._contextual_connect()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2242, in _contextual_connect
    self._wrap_pool_connect(self.pool.connect, None),
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2280, in _wrap_pool_connect
    e, dialect, self
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1547, in _handle_dbapi_exception_noconnection
    util.raise_from_cause(sqlalchemy_exception, exc_info)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 398, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 152, in reraise
    raise value.with_traceback(tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2276, in _wrap_pool_connect
    return fn()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 363, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 492, in checkout
    rec = pool._do_get()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/impl.py", line 139, in _do_get
    self._dec_overflow()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 153, in reraise
    raise value
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/impl.py", line 136, in _do_get
    return self._create_connection()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 308, in _create_connection
    return _ConnectionRecord(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 437, in __init__
    self.__connect(first_connect_check=True)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 639, in __connect
    connection = pool._invoke_creator(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/usr/lib/python3/dist-packages/psycopg2/__init__.py", line 126, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5432?

(Background on this error at: http://sqlalche.me/e/e3q8)
[2020-03-18 23:00:46,819] {dag_processing.py:663} WARNING - DagFileProcessorManager (PID=7274) exited with exit code 1 - re-launching
[2020-03-18 23:00:46,855] {dag_processing.py:556} INFO - Launched DagFileProcessorManager with pid: 7279
[2020-03-18 23:00:46,873] {settings.py:54} INFO - Configured default timezone <Timezone [UTC]>
[2020-03-18 23:00:46,893] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=7279
Process Process-36:
Traceback (most recent call last):
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2276, in _wrap_pool_connect
    return fn()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 363, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 492, in checkout
    rec = pool._do_get()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/impl.py", line 139, in _do_get
    self._dec_overflow()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 153, in reraise
    raise value
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/impl.py", line 136, in _do_get
    return self._create_connection()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 308, in _create_connection
    return _ConnectionRecord(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 437, in __init__
    self.__connect(first_connect_check=True)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 639, in __connect
    connection = pool._invoke_creator(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/usr/lib/python3/dist-packages/psycopg2/__init__.py", line 126, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5432?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/utils/dag_processing.py", line 624, in _run_processor_manager
    processor_manager.start()
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/utils/dag_processing.py", line 859, in start
    self._find_zombies()
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/utils/db.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/utils/dag_processing.py", line 1299, in _find_zombies
    LJ.latest_heartbeat < limit_dttm,
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3211, in all
    return list(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3367, in __iter__
    return self._execute_and_instances(context)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3389, in _execute_and_instances
    querycontext, self._connection_from_session, close_with_result=True
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3404, in _get_bind_args
    mapper=self._bind_mapper(), clause=querycontext.statement, **kw
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3382, in _connection_from_session
    conn = self.session.connection(**kw)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1124, in connection
    execution_options=execution_options,
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1130, in _connection_for_bind
    engine, execution_options
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 431, in _connection_for_bind
    conn = bind._contextual_connect()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2242, in _contextual_connect
    self._wrap_pool_connect(self.pool.connect, None),
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2280, in _wrap_pool_connect
    e, dialect, self
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1547, in _handle_dbapi_exception_noconnection
    util.raise_from_cause(sqlalchemy_exception, exc_info)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 398, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 152, in reraise
    raise value.with_traceback(tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2276, in _wrap_pool_connect
    return fn()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 363, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 492, in checkout
    rec = pool._do_get()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/impl.py", line 139, in _do_get
    self._dec_overflow()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 153, in reraise
    raise value
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/impl.py", line 136, in _do_get
    return self._create_connection()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 308, in _create_connection
    return _ConnectionRecord(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 437, in __init__
    self.__connect(first_connect_check=True)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 639, in __connect
    connection = pool._invoke_creator(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/usr/lib/python3/dist-packages/psycopg2/__init__.py", line 126, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5432?

(Background on this error at: http://sqlalche.me/e/e3q8)
[2020-03-18 23:00:48,823] {dag_processing.py:663} WARNING - DagFileProcessorManager (PID=7279) exited with exit code 1 - re-launching
[2020-03-18 23:00:48,846] {dag_processing.py:556} INFO - Launched DagFileProcessorManager with pid: 7284
[2020-03-18 23:00:48,856] {base_job.py:206} ERROR - SchedulerJob heartbeat got an exception
Traceback (most recent call last):
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2276, in _wrap_pool_connect
    return fn()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 363, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 497, in checkout
    rec._checkin_failed(err)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 153, in reraise
    raise value
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 494, in checkout
    dbapi_connection = rec.get_connection()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 593, in get_connection
    self.__connect()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 639, in __connect
    connection = pool._invoke_creator(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/usr/lib/python3/dist-packages/psycopg2/__init__.py", line 126, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5432?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/jobs/base_job.py", line 173, in heartbeat
    session.merge(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 2104, in merge
    _resolve_conflict_map=_resolve_conflict_map,
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 2177, in _merge
    merged = self.query(mapper.class_).get(key[1])
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 982, in get
    return self._get_impl(ident, loading.load_on_pk_identity)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 1094, in _get_impl
    return db_load_fn(self, primary_key_identity)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/loading.py", line 284, in load_on_pk_identity
    return q.one()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3325, in one
    ret = self.one_or_none()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3294, in one_or_none
    ret = list(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3367, in __iter__
    return self._execute_and_instances(context)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3389, in _execute_and_instances
    querycontext, self._connection_from_session, close_with_result=True
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3404, in _get_bind_args
    mapper=self._bind_mapper(), clause=querycontext.statement, **kw
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3382, in _connection_from_session
    conn = self.session.connection(**kw)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1124, in connection
    execution_options=execution_options,
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1130, in _connection_for_bind
    engine, execution_options
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 431, in _connection_for_bind
    conn = bind._contextual_connect()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2242, in _contextual_connect
    self._wrap_pool_connect(self.pool.connect, None),
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2280, in _wrap_pool_connect
    e, dialect, self
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1547, in _handle_dbapi_exception_noconnection
    util.raise_from_cause(sqlalchemy_exception, exc_info)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 398, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 152, in reraise
    raise value.with_traceback(tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2276, in _wrap_pool_connect
    return fn()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 363, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 497, in checkout
    rec._checkin_failed(err)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 153, in reraise
    raise value
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 494, in checkout
    dbapi_connection = rec.get_connection()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 593, in get_connection
    self.__connect()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 639, in __connect
    connection = pool._invoke_creator(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/usr/lib/python3/dist-packages/psycopg2/__init__.py", line 126, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5432?

(Background on this error at: http://sqlalche.me/e/e3q8)[2020-03-18 23:00:48,873] {settings.py:54} INFO - Configured default timezone <Timezone [UTC]>

[2020-03-18 23:00:48,952] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=7284
Process Process-37:
Traceback (most recent call last):
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2276, in _wrap_pool_connect
    return fn()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 363, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 492, in checkout
    rec = pool._do_get()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/impl.py", line 139, in _do_get
    self._dec_overflow()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 153, in reraise
    raise value
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/impl.py", line 136, in _do_get
    return self._create_connection()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 308, in _create_connection
    return _ConnectionRecord(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 437, in __init__
    self.__connect(first_connect_check=True)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 639, in __connect
    connection = pool._invoke_creator(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/usr/lib/python3/dist-packages/psycopg2/__init__.py", line 126, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5432?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/utils/dag_processing.py", line 624, in _run_processor_manager
    processor_manager.start()
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/utils/dag_processing.py", line 859, in start
    self._find_zombies()
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/utils/db.py", line 74, in wrapper
    return func(*args, **kwargs)
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/utils/dag_processing.py", line 1299, in _find_zombies
    LJ.latest_heartbeat < limit_dttm,
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3211, in all
    return list(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3367, in __iter__
    return self._execute_and_instances(context)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3389, in _execute_and_instances
    querycontext, self._connection_from_session, close_with_result=True
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3404, in _get_bind_args
    mapper=self._bind_mapper(), clause=querycontext.statement, **kw
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3382, in _connection_from_session
    conn = self.session.connection(**kw)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1124, in connection
    execution_options=execution_options,
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1130, in _connection_for_bind
    engine, execution_options
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 431, in _connection_for_bind
    conn = bind._contextual_connect()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2242, in _contextual_connect
    self._wrap_pool_connect(self.pool.connect, None),
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2280, in _wrap_pool_connect
    e, dialect, self
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1547, in _handle_dbapi_exception_noconnection
    util.raise_from_cause(sqlalchemy_exception, exc_info)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 398, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 152, in reraise
    raise value.with_traceback(tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2276, in _wrap_pool_connect
    return fn()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 363, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 492, in checkout
    rec = pool._do_get()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/impl.py", line 139, in _do_get
    self._dec_overflow()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 153, in reraise
    raise value
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/impl.py", line 136, in _do_get
    return self._create_connection()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 308, in _create_connection
    return _ConnectionRecord(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 437, in __init__
    self.__connect(first_connect_check=True)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 639, in __connect
    connection = pool._invoke_creator(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/usr/lib/python3/dist-packages/psycopg2/__init__.py", line 126, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5432?

(Background on this error at: http://sqlalche.me/e/e3q8)
^C[2020-03-18 23:00:50,476] {helpers.py:308} INFO - Sending Signals.SIGTERM to GPID 7284
[2020-03-18 23:00:50,477] {helpers.py:286} INFO - Process psutil.Process(pid=7284, status='terminated') (7284) terminated with exit code 1
[2020-03-18 23:00:50,482] {scheduler_job.py:1361} INFO - Exited execute loop
Traceback (most recent call last):
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2276, in _wrap_pool_connect
    return fn()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 363, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 497, in checkout
    rec._checkin_failed(err)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 153, in reraise
    raise value
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 494, in checkout
    dbapi_connection = rec.get_connection()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 593, in get_connection
    self.__connect()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 639, in __connect
    connection = pool._invoke_creator(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/usr/lib/python3/dist-packages/psycopg2/__init__.py", line 126, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5432?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/jdearce/.local/bin/airflow", line 37, in <module>
    args.func(args)
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/utils/cli.py", line 74, in wrapper
    return f(*args, **kwargs)
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/bin/cli.py", line 1042, in scheduler
    job.run()
  File "/home/jdearce/.local/lib/python3.6/site-packages/airflow/jobs/base_job.py", line 233, in run
    session.merge(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 2104, in merge
    _resolve_conflict_map=_resolve_conflict_map,
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 2177, in _merge
    merged = self.query(mapper.class_).get(key[1])
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 982, in get
    return self._get_impl(ident, loading.load_on_pk_identity)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 1094, in _get_impl
    return db_load_fn(self, primary_key_identity)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/loading.py", line 284, in load_on_pk_identity
    return q.one()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3325, in one
    ret = self.one_or_none()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3294, in one_or_none
    ret = list(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3367, in __iter__
    return self._execute_and_instances(context)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3389, in _execute_and_instances
    querycontext, self._connection_from_session, close_with_result=True
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3404, in _get_bind_args
    mapper=self._bind_mapper(), clause=querycontext.statement, **kw
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3382, in _connection_from_session
    conn = self.session.connection(**kw)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1124, in connection
    execution_options=execution_options,
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1130, in _connection_for_bind
    engine, execution_options
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 431, in _connection_for_bind
    conn = bind._contextual_connect()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2242, in _contextual_connect
    self._wrap_pool_connect(self.pool.connect, None),
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2280, in _wrap_pool_connect
    e, dialect, self
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1547, in _handle_dbapi_exception_noconnection
    util.raise_from_cause(sqlalchemy_exception, exc_info)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 398, in raise_from_cause
    reraise(type(exception), exception, tb=exc_tb, cause=cause)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 152, in reraise
    raise value.with_traceback(tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2276, in _wrap_pool_connect
    return fn()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 363, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 760, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 497, in checkout
    rec._checkin_failed(err)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 68, in __exit__
    compat.reraise(exc_type, exc_value, exc_tb)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 153, in reraise
    raise value
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 494, in checkout
    dbapi_connection = rec.get_connection()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 593, in get_connection
    self.__connect()
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 639, in __connect
    connection = pool._invoke_creator(self)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/jdearce/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 482, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/usr/lib/python3/dist-packages/psycopg2/__init__.py", line 126, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not connect to server: Connection refused
        Is the server running on host "localhost" (127.0.0.1) and accepting
        TCP/IP connections on port 5432?

(Background on this error at: http://sqlalche.me/e/e3q8)
jdearce@LAPTOP-I43LI54D:~$