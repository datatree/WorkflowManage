jdearce@LAPTOP-I43LI54D:~$ airflow scheduler

[2020-03-16 13:36:31,224] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=136
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2020-03-16 13:37:38,922] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:37:39,673] {scheduler_job.py:1320} INFO - Starting the scheduler
[2020-03-16 13:37:39,674] {scheduler_job.py:1328} INFO - Running execute loop for -1 seconds
[2020-03-16 13:37:39,675] {scheduler_job.py:1329} INFO - Processing each file at most -1 times
[2020-03-16 13:37:39,676] {scheduler_job.py:1332} INFO - Searching for files in /c/Users/jdearce/airflow/dags
[2020-03-16 13:37:40,163] {scheduler_job.py:1334} INFO - There are 20 files in /c/Users/jdearce/airflow/dags
[2020-03-16 13:37:42,736] {scheduler_job.py:1382} INFO - Resetting orphaned tasks for active dag runs
[2020-03-16 13:37:47,285] {base_job.py:317} INFO - Reset the following 21 TaskInstances:
        <TaskInstance: example_short_circuit_operator.condition_is_False 2020-03-09 00:00:00+00:00 [None]>
        <TaskInstance: example_short_circuit_operator.condition_is_True 2020-03-09 00:00:00+00:00 [None]>
        <TaskInstance: example_http_operator.http_sensor_check 2020-03-09 00:00:00+00:00 [None]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-09 00:00:00+00:00 [None]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-09 00:00:00+00:00 [None]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-09 04:00:00+00:00 [None]>
        <TaskInstance: latest_only.latest_only 2020-03-09 04:00:00+00:00 [None]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-09 12:00:00+00:00 [None]>
        <TaskInstance: latest_only.latest_only 2020-03-09 12:00:00+00:00 [None]>
        <TaskInstance: example_skip_dag.always_true_1 2020-03-10 00:00:00+00:00 [None]>
        <TaskInstance: example_skip_dag.always_true_2 2020-03-10 00:00:00+00:00 [None]>
        <TaskInstance: example_skip_dag.skip_operator_1 2020-03-10 00:00:00+00:00 [None]>
        <TaskInstance: example_skip_dag.skip_operator_2 2020-03-10 00:00:00+00:00 [None]>
        <TaskInstance: tutorial.print_date 2020-03-10 00:00:00+00:00 [None]>
        <TaskInstance: example_short_circuit_operator.condition_is_False 2020-03-10 00:00:00+00:00 [None]>
        <TaskInstance: example_short_circuit_operator.condition_is_True 2020-03-10 00:00:00+00:00 [None]>
        <TaskInstance: example_http_operator.http_sensor_check 2020-03-10 00:00:00+00:00 [None]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-09 16:00:00+00:00 [None]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-09 16:00:00+00:00 [None]>
        <TaskInstance: example_branch_operator.run_this_first 2020-03-10 00:00:00+00:00 [None]>
        <TaskInstance: latest_only.latest_only 2020-03-09 16:00:00+00:00 [None]>
[2020-03-16 13:37:47,363] {dag_processing.py:556} INFO - Launched DagFileProcessorManager with pid: 371
[2020-03-16 13:37:47,407] {settings.py:54} INFO - Configured default timezone <Timezone [UTC]>
[2020-03-16 13:37:47,474] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=371
[2020-03-16 13:37:57,478] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: example_http_operator.http_sensor_check 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_http_operator.http_sensor_check 2020-03-09 00:00:00+00:00 [scheduled]>
[2020-03-16 13:37:57,868] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 127 open slots and 2 task instances ready to be queued
[2020-03-16 13:37:57,869] {scheduler_job.py:986} INFO - DAG example_http_operator has 0/16 running and queued tasks
[2020-03-16 13:37:57,870] {scheduler_job.py:986} INFO - DAG example_http_operator has 1/16 running and queued tasks
[2020-03-16 13:37:57,884] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_http_operator.http_sensor_check 2020-03-09 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_http_operator.http_sensor_check 2020-03-10 00:00:00+00:00 [scheduled]>
[2020-03-16 13:37:58,778] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: example_http_operator.http_sensor_check 2020-03-09 00:00:00+00:00 [queued]>
        <TaskInstance: example_http_operator.http_sensor_check 2020-03-10 00:00:00+00:00 [queued]>
[2020-03-16 13:37:58,781] {scheduler_job.py:1148} INFO - Sending ('example_http_operator', 'http_sensor_check', datetime.datetime(2020, 3, 9, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 6 and queue default
[2020-03-16 13:37:58,783] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_http_operator', 'http_sensor_check', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 13:37:58,786] {scheduler_job.py:1148} INFO - Sending ('example_http_operator', 'http_sensor_check', datetime.datetime(2020, 3, 10, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 6 and queue default
[2020-03-16 13:37:58,787] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_http_operator', 'http_sensor_check', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 13:37:58,792] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_http_operator', 'http_sensor_check', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 13:37:58,796] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_http_operator', 'http_sensor_check', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 13:38:02,301] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=442
[2020-03-16 13:38:02,415] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=440
[2020-03-16 13:38:05,946] {scheduler_job.py:927} INFO - 6 tasks up for execution:
        <TaskInstance: example_branch_operator.run_this_first 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_branch_operator.branching 2020-03-09 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.condition_is_True 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.condition_is_True 2020-03-09 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.condition_is_False 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.condition_is_False 2020-03-09 00:00:00+00:00 [scheduled]>
[2020-03-16 13:38:05,961] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 125 open slots and 6 task instances ready to be queued
[2020-03-16 13:38:05,962] {scheduler_job.py:986} INFO - DAG example_branch_operator has 0/16 running and queued tasks
[2020-03-16 13:38:05,965] {scheduler_job.py:986} INFO - DAG example_branch_operator has 1/16 running and queued tasks
[2020-03-16 13:38:05,965] {scheduler_job.py:986} INFO - DAG example_short_circuit_operator has 0/16 running and queued tasks
[2020-03-16 13:38:05,967] {scheduler_job.py:986} INFO - DAG example_short_circuit_operator has 1/16 running and queued tasks
[2020-03-16 13:38:05,968] {scheduler_job.py:986} INFO - DAG example_short_circuit_operator has 2/16 running and queued tasks
[2020-03-16 13:38:05,969] {scheduler_job.py:986} INFO - DAG example_short_circuit_operator has 3/16 running and queued tasks
[2020-03-16 13:38:05,984] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_branch_operator.run_this_first 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_branch_operator.branching 2020-03-09 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.condition_is_True 2020-03-09 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.condition_is_False 2020-03-09 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.condition_is_True 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.condition_is_False 2020-03-10 00:00:00+00:00 [scheduled]>
[2020-03-16 13:38:06,152] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:06,154] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py
[2020-03-16 13:38:06,153] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:06,156] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py
[2020-03-16 13:38:06,557] {scheduler_job.py:1112} INFO - Setting the following 6 tasks to queued state:
        <TaskInstance: example_short_circuit_operator.condition_is_False 2020-03-09 00:00:00+00:00 [queued]>
        <TaskInstance: example_short_circuit_operator.condition_is_True 2020-03-09 00:00:00+00:00 [queued]>
        <TaskInstance: example_branch_operator.branching 2020-03-09 00:00:00+00:00 [queued]>
        <TaskInstance: example_short_circuit_operator.condition_is_False 2020-03-10 00:00:00+00:00 [queued]>
        <TaskInstance: example_short_circuit_operator.condition_is_True 2020-03-10 00:00:00+00:00 [queued]>
        <TaskInstance: example_branch_operator.run_this_first 2020-03-10 00:00:00+00:00 [queued]>
[2020-03-16 13:38:06,566] {scheduler_job.py:1148} INFO - Sending ('example_short_circuit_operator', 'condition_is_False', datetime.datetime(2020, 3, 9, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:38:06,569] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_short_circuit_operator', 'condition_is_False', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:38:06,574] {scheduler_job.py:1148} INFO - Sending ('example_short_circuit_operator', 'condition_is_True', datetime.datetime(2020, 3, 9, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:38:06,577] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_short_circuit_operator', 'condition_is_True', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:38:06,580] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'branching', datetime.datetime(2020, 3, 9, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 10 and queue default
[2020-03-16 13:38:06,582] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'branching', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:38:06,586] {scheduler_job.py:1148} INFO - Sending ('example_short_circuit_operator', 'condition_is_False', datetime.datetime(2020, 3, 10, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:38:06,590] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_short_circuit_operator', 'condition_is_False', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:38:06,596] {scheduler_job.py:1148} INFO - Sending ('example_short_circuit_operator', 'condition_is_True', datetime.datetime(2020, 3, 10, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:38:06,599] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_short_circuit_operator', 'condition_is_True', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:38:06,602] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'run_this_first', datetime.datetime(2020, 3, 10, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 11 and queue default
[2020-03-16 13:38:06,603] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'run_this_first', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:38:06,607] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'run_this_first', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:38:06,607] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'branching', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:38:06,609] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_short_circuit_operator', 'condition_is_False', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:38:06,612] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_short_circuit_operator', 'condition_is_False', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:38:06,611] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_short_circuit_operator', 'condition_is_True', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:38:06,613] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_short_circuit_operator', 'condition_is_True', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:38:08,732] {cli.py:545} INFO - Running <TaskInstance: example_http_operator.http_sensor_check 2020-03-09T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:38:08,732] {cli.py:545} INFO - Running <TaskInstance: example_http_operator.http_sensor_check 2020-03-10T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:38:12,006] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=486
[2020-03-16 13:38:12,241] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=484
[2020-03-16 13:38:12,431] {scheduler_job.py:927} INFO - 4 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only.latest_only 2020-03-09 16:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only.latest_only 2020-03-09 12:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only.latest_only 2020-03-09 04:00:00+00:00 [scheduled]>[2020-03-16 13:38:12,694] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=487

[2020-03-16 13:38:12,699] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=488
[2020-03-16 13:38:12,796] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 119 open slots and 4 task instances ready to be queued
[2020-03-16 13:38:12,817] {scheduler_job.py:986} INFO - DAG latest_only has 0/16 running and queued tasks
[2020-03-16 13:38:12,839] {scheduler_job.py:986} INFO - DAG latest_only has 1/16 running and queued tasks
[2020-03-16 13:38:12,865] {scheduler_job.py:986} INFO - DAG latest_only has 2/16 running and queued tasks
[2020-03-16 13:38:12,891] {scheduler_job.py:986} INFO - DAG latest_only has 3/16 running and queued tasks
[2020-03-16 13:38:12,930] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-09 04:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only.latest_only 2020-03-09 12:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only.latest_only 2020-03-09 16:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only.latest_only 2020-03-14 00:00:00+00:00 [scheduled]>
[2020-03-16 13:38:13,087] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=485
[2020-03-16 13:38:13,167] {scheduler_job.py:1112} INFO - Setting the following 4 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-14 00:00:00+00:00 [queued]>
        <TaskInstance: latest_only.latest_only 2020-03-09 16:00:00+00:00 [queued]>
        <TaskInstance: latest_only.latest_only 2020-03-09 12:00:00+00:00 [queued]>
        <TaskInstance: latest_only.latest_only 2020-03-09 04:00:00+00:00 [queued]>[2020-03-16 13:38:13,231] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=489

[2020-03-16 13:38:13,357] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 14, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:38:13,397] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:38:13,416] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 9, 16, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:38:13,429] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-09T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:38:13,435] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 9, 12, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:38:13,438] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-09T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:38:13,441] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 9, 4, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:38:13,443] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-09T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:38:13,451] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:38:13,456] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-09T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:38:13,464] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-09T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:38:13,489] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-09T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:38:17,282] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:17,400] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-16 13:38:18,075] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:18,076] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-16 13:38:18,420] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:18,420] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:18,424] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py
[2020-03-16 13:38:18,425] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py
[2020-03-16 13:38:18,426] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:18,429] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py
[2020-03-16 13:38:19,003] {scheduler_job.py:927} INFO - 5 tasks up for execution:
        <TaskInstance: example_skip_dag.skip_operator_2 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.skip_operator_1 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.one_success 2020-03-09 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.always_true_2 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.always_true_1 2020-03-10 00:00:00+00:00 [scheduled]>
[2020-03-16 13:38:19,018] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 115 open slots and 5 task instances ready to be queued
[2020-03-16 13:38:19,019] {scheduler_job.py:986} INFO - DAG example_skip_dag has 0/16 running and queued tasks
[2020-03-16 13:38:19,020] {scheduler_job.py:986} INFO - DAG example_skip_dag has 1/16 running and queued tasks
[2020-03-16 13:38:19,021] {scheduler_job.py:986} INFO - DAG example_skip_dag has 2/16 running and queued tasks
[2020-03-16 13:38:19,022] {scheduler_job.py:986} INFO - DAG example_skip_dag has 3/16 running and queued tasks
[2020-03-16 13:38:19,022] {scheduler_job.py:986} INFO - DAG example_skip_dag has 4/16 running and queued tasks
[2020-03-16 13:38:19,039] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_skip_dag.skip_operator_2 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.skip_operator_1 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.always_true_2 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.always_true_1 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.one_success 2020-03-09 00:00:00+00:00 [scheduled]>
[2020-03-16 13:38:19,098] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=538
[2020-03-16 13:38:19,251] {scheduler_job.py:1112} INFO - Setting the following 5 tasks to queued state:
        <TaskInstance: example_skip_dag.skip_operator_2 2020-03-10 00:00:00+00:00 [queued]>
        <TaskInstance: example_skip_dag.skip_operator_1 2020-03-10 00:00:00+00:00 [queued]>
        <TaskInstance: example_skip_dag.one_success 2020-03-09 00:00:00+00:00 [queued]>
        <TaskInstance: example_skip_dag.always_true_2 2020-03-10 00:00:00+00:00 [queued]>
        <TaskInstance: example_skip_dag.always_true_1 2020-03-10 00:00:00+00:00 [queued]>
[2020-03-16 13:38:19,268] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'skip_operator_2', datetime.datetime(2020, 3, 10, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:38:19,271] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'skip_operator_2', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:38:19,273] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'skip_operator_1', datetime.datetime(2020, 3, 10, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:38:19,276] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'skip_operator_1', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:38:19,279] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'one_success', datetime.datetime(2020, 3, 9, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:38:19,281] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'one_success', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:38:19,283] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'always_true_2', datetime.datetime(2020, 3, 10, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:38:19,290] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'always_true_2', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:38:19,294] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'always_true_1', datetime.datetime(2020, 3, 10, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:38:19,297] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'always_true_1', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:38:19,305] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'skip_operator_2', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:38:19,309] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'skip_operator_1', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:38:19,327] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'always_true_2', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:38:19,338] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'always_true_1', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:38:19,342] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'one_success', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:38:20,358] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:20,360] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py
[2020-03-16 13:38:20,836] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=543
[2020-03-16 13:38:21,084] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=539
[2020-03-16 13:38:21,102] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=542
[2020-03-16 13:38:21,565] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.run_this_first 2020-03-10T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:38:22,962] {cli.py:545} INFO - Running <TaskInstance: example_short_circuit_operator.condition_is_False 2020-03-10T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:38:23,784] {cli.py:545} INFO - Running <TaskInstance: example_short_circuit_operator.condition_is_True 2020-03-10T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:38:23,820] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.branching 2020-03-09T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:38:24,014] {cli.py:545} INFO - Running <TaskInstance: example_short_circuit_operator.condition_is_False 2020-03-09T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:38:25,451] {cli.py:545} INFO - Running <TaskInstance: example_short_circuit_operator.condition_is_True 2020-03-09T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:38:25,508] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=613
[2020-03-16 13:38:25,964] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:25,971] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-16 13:38:26,114] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=614
[2020-03-16 13:38:26,125] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=610
[2020-03-16 13:38:26,268] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=612
[2020-03-16 13:38:26,976] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=611
[2020-03-16 13:38:27,411] {scheduler_job.py:927} INFO - 3 tasks up for execution:
        <TaskInstance: tutorial.templated 2020-03-09 00:00:00+00:00 [scheduled]>
        <TaskInstance: tutorial.sleep 2020-03-09 00:00:00+00:00 [scheduled]>
        <TaskInstance: tutorial.print_date 2020-03-10 00:00:00+00:00 [scheduled]>[2020-03-16 13:38:27,478] {__init__.py:51} INFO - Using executor LocalExecutor

[2020-03-16 13:38:27,497] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-16 13:38:27,555] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 110 open slots and 3 task instances ready to be queued
[2020-03-16 13:38:27,575] {scheduler_job.py:986} INFO - DAG tutorial has 0/16 running and queued tasks
[2020-03-16 13:38:27,583] {scheduler_job.py:986} INFO - DAG tutorial has 1/16 running and queued tasks
[2020-03-16 13:38:27,595] {scheduler_job.py:986} INFO - DAG tutorial has 2/16 running and queued tasks
[2020-03-16 13:38:27,708] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: tutorial.print_date 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: tutorial.templated 2020-03-09 00:00:00+00:00 [scheduled]>
        <TaskInstance: tutorial.sleep 2020-03-09 00:00:00+00:00 [scheduled]>
[2020-03-16 13:38:27,831] {scheduler_job.py:1112} INFO - Setting the following 3 tasks to queued state:
        <TaskInstance: tutorial.templated 2020-03-09 00:00:00+00:00 [queued]>
        <TaskInstance: tutorial.sleep 2020-03-09 00:00:00+00:00 [queued]>
        <TaskInstance: tutorial.print_date 2020-03-10 00:00:00+00:00 [queued]>
[2020-03-16 13:38:27,904] {scheduler_job.py:1148} INFO - Sending ('tutorial', 'templated', datetime.datetime(2020, 3, 9, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 13:38:27,906] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'tutorial', 'templated', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 13:38:27,911] {scheduler_job.py:1148} INFO - Sending ('tutorial', 'sleep', datetime.datetime(2020, 3, 9, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 13:38:27,915] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'tutorial', 'sleep', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 13:38:27,918] {scheduler_job.py:1148} INFO - Sending ('tutorial', 'print_date', datetime.datetime(2020, 3, 10, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:38:27,921] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'tutorial', 'print_date', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 13:38:27,939] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'tutorial', 'print_date', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 13:38:27,946] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'tutorial', 'templated', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 13:38:27,956] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'tutorial', 'sleep', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 13:38:28,026] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:28,028] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-16 13:38:29,229] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:29,341] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-16 13:38:30,727] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:30,729] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-16 13:38:30,879] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:30,880] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:30,880] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:30,882] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-16 13:38:30,882] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-16 13:38:30,883] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-16 13:38:32,566] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:32,577] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-16 13:38:32,662] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=713
[2020-03-16 13:38:32,764] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-14T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:38:33,384] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-09T04:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:38:33,420] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=715
[2020-03-16 13:38:33,431] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=714
[2020-03-16 13:38:34,255] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-09T16:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:38:34,515] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-09T12:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:38:35,540] {scheduler_job.py:927} INFO - 8 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-09 16:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-09 00:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-09 16:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-09 12:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-09 04:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-09 00:00:00+00:00 [scheduled]>
[2020-03-16 13:38:35,766] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 108 open slots and 8 task instances ready to be queued
[2020-03-16 13:38:35,780] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-16 13:38:35,839] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-16 13:38:35,851] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 2/16 running and queued tasks
[2020-03-16 13:38:35,859] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 3/16 running and queued tasks
[2020-03-16 13:38:35,888] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 4/16 running and queued tasks
[2020-03-16 13:38:35,909] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 5/16 running and queued tasks
[2020-03-16 13:38:35,922] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 6/16 running and queued tasks
[2020-03-16 13:38:35,931] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 7/16 running and queued tasks
[2020-03-16 13:38:35,985] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-09 00:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-09 04:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-09 12:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-09 16:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-09 00:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-09 16:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-14 00:00:00+00:00 [scheduled]>
[2020-03-16 13:38:36,108] {scheduler_job.py:1112} INFO - Setting the following 8 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-14 00:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-09 16:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-09 00:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-14 00:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-09 16:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-09 12:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-09 04:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-09 00:00:00+00:00 [queued]>
[2020-03-16 13:38:36,129] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 14, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:38:36,130] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:38:36,132] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 9, 16, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:38:36,135] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-09T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:38:36,137] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 9, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:38:36,138] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:38:36,139] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 14, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-16 13:38:36,142] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:38:36,144] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 9, 16, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-16 13:38:36,147] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-09T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:38:36,153] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 9, 12, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-16 13:38:36,156] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-09T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:38:36,158] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 9, 4, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-16 13:38:36,160] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-09T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:38:36,161] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 9, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-16 13:38:36,163] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:38:36,167] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:38:36,172] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-09T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:38:36,177] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-09T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:38:36,181] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-09T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:38:36,200] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:38:36,216] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:38:36,219] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-09T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:38:36,236] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:38:39,588] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.skip_operator_2 2020-03-10T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:38:39,592] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.always_true_1 2020-03-10T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:38:39,941] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.always_true_2 2020-03-10T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:38:40,520] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.one_success 2020-03-09T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:38:40,714] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.skip_operator_1 2020-03-10T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:38:40,844] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:40,844] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:40,846] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:40,848] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py
[2020-03-16 13:38:40,849] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py
[2020-03-16 13:38:40,850] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py
[2020-03-16 13:38:43,825] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=817
[2020-03-16 13:38:43,833] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=818
[2020-03-16 13:38:43,853] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=820
[2020-03-16 13:38:44,992] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=819
[2020-03-16 13:38:45,024] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=822
[2020-03-16 13:38:45,090] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=823
[2020-03-16 13:38:45,116] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=825
[2020-03-16 13:38:45,982] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=824
[2020-03-16 13:38:53,931] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:53,973] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:38:54,537] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:54,538] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:38:56,358] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:56,361] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:38:56,923] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:56,925] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:38:57,278] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:57,280] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:38:57,456] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:57,458] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:38:57,927] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:57,928] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:38:58,298] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:38:58,300] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:38:58,762] {cli.py:545} INFO - Running <TaskInstance: tutorial.print_date 2020-03-10T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:38:59,210] {cli.py:545} INFO - Running <TaskInstance: tutorial.templated 2020-03-09T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:39:00,101] {cli.py:545} INFO - Running <TaskInstance: tutorial.sleep 2020-03-09T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:39:14,782] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-09T16:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:39:15,884] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-09T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:39:16,209] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-09T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:39:16,460] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-09T12:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:39:16,640] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-14T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:39:16,957] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-09T04:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:39:17,850] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-14T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:39:19,222] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-09T16:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:39:33,909] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-14 04:00:00+00:00 [scheduled]>
[2020-03-16 13:39:33,927] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 102 open slots and 1 task instances ready to be queued
[2020-03-16 13:39:33,929] {scheduler_job.py:986} INFO - DAG latest_only has 4/16 running and queued tasks
[2020-03-16 13:39:33,946] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-14 04:00:00+00:00 [scheduled]>
[2020-03-16 13:39:34,081] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-14 04:00:00+00:00 [queued]>
[2020-03-16 13:39:34,082] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 14, 4, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:39:34,083] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-14T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:39:34,089] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-14T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:39:37,010] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1343
[2020-03-16 13:39:46,706] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:39:46,708] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-16 13:40:04,152] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-14T04:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:40:17,997] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: example_http_operator.post_op 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_http_operator.post_op 2020-03-09 00:00:00+00:00 [scheduled]>
[2020-03-16 13:40:18,009] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 107 open slots and 2 task instances ready to be queued
[2020-03-16 13:40:18,010] {scheduler_job.py:986} INFO - DAG example_http_operator has 0/16 running and queued tasks
[2020-03-16 13:40:18,010] {scheduler_job.py:986} INFO - DAG example_http_operator has 1/16 running and queued tasks
[2020-03-16 13:40:18,026] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_http_operator.post_op 2020-03-09 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_http_operator.post_op 2020-03-10 00:00:00+00:00 [scheduled]>
[2020-03-16 13:40:18,815] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: example_http_operator.post_op 2020-03-09 00:00:00+00:00 [queued]>
        <TaskInstance: example_http_operator.post_op 2020-03-10 00:00:00+00:00 [queued]>
[2020-03-16 13:40:18,817] {scheduler_job.py:1148} INFO - Sending ('example_http_operator', 'post_op', datetime.datetime(2020, 3, 9, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 5 and queue default
[2020-03-16 13:40:18,819] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_http_operator', 'post_op', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 13:40:18,820] {scheduler_job.py:1148} INFO - Sending ('example_http_operator', 'post_op', datetime.datetime(2020, 3, 10, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 5 and queue default
[2020-03-16 13:40:18,821] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_http_operator', 'post_op', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 13:40:18,825] {scheduler_job.py:1288} INFO - Executor reports execution of example_http_operator.http_sensor_check execution_date=2020-03-09 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:40:18,826] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_http_operator', 'post_op', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 13:40:18,826] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_http_operator', 'post_op', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 13:40:18,832] {scheduler_job.py:1288} INFO - Executor reports execution of example_http_operator.http_sensor_check execution_date=2020-03-10 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:40:22,582] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1877
[2020-03-16 13:40:22,612] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=1878
[2020-03-16 13:40:31,544] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:40:31,546] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py
[2020-03-16 13:40:31,636] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:40:31,638] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py
[2020-03-16 13:40:38,357] {cli.py:545} INFO - Running <TaskInstance: example_http_operator.post_op 2020-03-10T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:40:38,494] {cli.py:545} INFO - Running <TaskInstance: example_http_operator.post_op 2020-03-09T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:40:40,041] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: example_branch_operator.branching 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_branch_operator.branch_a 2020-03-09 00:00:00+00:00 [scheduled]>
[2020-03-16 13:40:40,051] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 118 open slots and 2 task instances ready to be queued
[2020-03-16 13:40:40,052] {scheduler_job.py:986} INFO - DAG example_branch_operator has 0/16 running and queued tasks
[2020-03-16 13:40:40,052] {scheduler_job.py:986} INFO - DAG example_branch_operator has 1/16 running and queued tasks
[2020-03-16 13:40:40,068] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_branch_operator.branching 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_branch_operator.branch_a 2020-03-09 00:00:00+00:00 [scheduled]>
[2020-03-16 13:40:40,250] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: example_branch_operator.branching 2020-03-10 00:00:00+00:00 [queued]>
        <TaskInstance: example_branch_operator.branch_a 2020-03-09 00:00:00+00:00 [queued]>
[2020-03-16 13:40:40,253] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'branching', datetime.datetime(2020, 3, 10, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 10 and queue default
[2020-03-16 13:40:40,255] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'branching', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:40:40,256] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'branch_a', datetime.datetime(2020, 3, 9, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:40:40,257] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'branch_a', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:40:40,261] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'branch_a', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:40:40,261] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.run_this_first execution_date=2020-03-10 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:40:40,261] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'branching', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:40:40,269] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.branching execution_date=2020-03-09 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:40:42,062] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: example_short_circuit_operator.true_1 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.true_1 2020-03-09 00:00:00+00:00 [scheduled]>
[2020-03-16 13:40:42,080] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 119 open slots and 2 task instances ready to be queued
[2020-03-16 13:40:42,082] {scheduler_job.py:986} INFO - DAG example_short_circuit_operator has 0/16 running and queued tasks
[2020-03-16 13:40:42,086] {scheduler_job.py:986} INFO - DAG example_short_circuit_operator has 1/16 running and queued tasks
[2020-03-16 13:40:42,101] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_short_circuit_operator.true_1 2020-03-09 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.true_1 2020-03-10 00:00:00+00:00 [scheduled]>
[2020-03-16 13:40:42,581] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: example_short_circuit_operator.true_1 2020-03-09 00:00:00+00:00 [queued]>
        <TaskInstance: example_short_circuit_operator.true_1 2020-03-10 00:00:00+00:00 [queued]>
[2020-03-16 13:40:42,588] {scheduler_job.py:1148} INFO - Sending ('example_short_circuit_operator', 'true_1', datetime.datetime(2020, 3, 9, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:40:42,593] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_short_circuit_operator', 'true_1', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:40:42,598] {scheduler_job.py:1148} INFO - Sending ('example_short_circuit_operator', 'true_1', datetime.datetime(2020, 3, 10, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:40:42,602] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_short_circuit_operator', 'true_1', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:40:42,632] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_short_circuit_operator', 'true_1', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:40:42,640] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_short_circuit_operator', 'true_1', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:40:42,642] {scheduler_job.py:1288} INFO - Executor reports execution of example_short_circuit_operator.condition_is_True execution_date=2020-03-10 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:40:42,740] {scheduler_job.py:1288} INFO - Executor reports execution of example_short_circuit_operator.condition_is_True execution_date=2020-03-09 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:40:42,765] {scheduler_job.py:1288} INFO - Executor reports execution of example_short_circuit_operator.condition_is_False execution_date=2020-03-10 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:40:42,780] {scheduler_job.py:1288} INFO - Executor reports execution of example_short_circuit_operator.condition_is_False execution_date=2020-03-09 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:40:44,377] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2093
[2020-03-16 13:40:44,380] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2094
[2020-03-16 13:40:46,287] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-14 08:00:00+00:00 [scheduled]>
[2020-03-16 13:40:46,299] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 121 open slots and 1 task instances ready to be queued
[2020-03-16 13:40:46,300] {scheduler_job.py:986} INFO - DAG latest_only has 1/16 running and queued tasks
[2020-03-16 13:40:46,335] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-14 08:00:00+00:00 [scheduled]>
[2020-03-16 13:40:46,499] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-14 08:00:00+00:00 [queued]>
[2020-03-16 13:40:46,501] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 14, 8, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:40:46,502] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-14T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:40:46,506] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-14T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:40:46,507] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-14 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:40:46,516] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-09 04:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:40:46,528] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-09 16:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:40:46,534] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-09 12:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:40:46,968] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2120
[2020-03-16 13:40:47,718] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2121
[2020-03-16 13:40:48,123] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:40:48,127] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-16 13:40:49,996] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2161
[2020-03-16 13:40:50,185] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:40:50,188] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-16 13:40:50,242] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: example_skip_dag.one_success 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.final_2 2020-03-09 00:00:00+00:00 [scheduled]>
[2020-03-16 13:40:50,257] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 120 open slots and 2 task instances ready to be queued
[2020-03-16 13:40:50,258] {scheduler_job.py:986} INFO - DAG example_skip_dag has 0/16 running and queued tasks
[2020-03-16 13:40:50,260] {scheduler_job.py:986} INFO - DAG example_skip_dag has 1/16 running and queued tasks
[2020-03-16 13:40:50,277] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_skip_dag.one_success 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.final_2 2020-03-09 00:00:00+00:00 [scheduled]>
[2020-03-16 13:40:50,562] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: example_skip_dag.one_success 2020-03-10 00:00:00+00:00 [queued]>
        <TaskInstance: example_skip_dag.final_2 2020-03-09 00:00:00+00:00 [queued]>
[2020-03-16 13:40:50,568] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'one_success', datetime.datetime(2020, 3, 10, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:40:50,572] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'one_success', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:40:50,576] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'final_2', datetime.datetime(2020, 3, 9, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 13:40:50,579] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'final_2', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:40:50,588] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'one_success', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:40:50,597] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'final_2', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:40:50,626] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.skip_operator_2 execution_date=2020-03-10 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:40:50,639] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.always_true_1 execution_date=2020-03-10 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:40:50,649] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.always_true_2 execution_date=2020-03-10 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:40:50,654] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.one_success execution_date=2020-03-09 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:40:50,659] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.skip_operator_1 execution_date=2020-03-10 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:40:52,055] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:40:52,110] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py
[2020-03-16 13:40:52,366] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:40:52,371] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py
[2020-03-16 13:40:52,384] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.branching 2020-03-10T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:40:52,488] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.branch_a 2020-03-09T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:40:52,865] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:40:52,867] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-16 13:40:54,186] {cli.py:545} INFO - Running <TaskInstance: example_short_circuit_operator.true_1 2020-03-09T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:40:54,679] {cli.py:545} INFO - Running <TaskInstance: example_short_circuit_operator.true_1 2020-03-10T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:40:55,059] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2212
[2020-03-16 13:40:55,268] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2210
[2020-03-16 13:40:55,943] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-14T08:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:40:56,293] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-14 04:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-14 04:00:00+00:00 [scheduled]>
[2020-03-16 13:40:56,321] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 118 open slots and 2 task instances ready to be queued
[2020-03-16 13:40:56,328] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-16 13:40:56,331] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-16 13:40:56,349] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-14 04:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-14 04:00:00+00:00 [scheduled]>
[2020-03-16 13:40:56,453] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-14 04:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-14 04:00:00+00:00 [queued]>
[2020-03-16 13:40:56,471] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 14, 4, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:40:56,479] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-14T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:40:56,499] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 14, 4, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-16 13:40:56,552] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-14T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:40:56,586] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-14T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:40:56,623] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-14T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:40:56,640] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-09 16:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:40:56,718] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-09 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:40:56,733] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-09 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:40:56,760] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-09 12:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:40:56,791] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-14 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:40:56,854] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-09 04:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:40:56,873] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-14 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:40:56,886] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-09 16:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:40:58,156] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:40:58,173] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-16 13:40:58,238] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:40:58,287] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-16 13:41:00,569] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2290
[2020-03-16 13:41:00,578] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.final_2 2020-03-09T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:41:00,724] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2292
[2020-03-16 13:41:01,154] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.one_success 2020-03-10T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:41:04,305] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: example_http_operator.http_sensor_check 2020-03-14 00:00:00+00:00 [scheduled]>
[2020-03-16 13:41:04,314] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 117 open slots and 1 task instances ready to be queued
[2020-03-16 13:41:04,315] {scheduler_job.py:986} INFO - DAG example_http_operator has 2/16 running and queued tasks
[2020-03-16 13:41:04,328] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_http_operator.http_sensor_check 2020-03-14 00:00:00+00:00 [scheduled]>
[2020-03-16 13:41:04,395] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: example_http_operator.http_sensor_check 2020-03-14 00:00:00+00:00 [queued]>
[2020-03-16 13:41:04,396] {scheduler_job.py:1148} INFO - Sending ('example_http_operator', 'http_sensor_check', datetime.datetime(2020, 3, 14, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 6 and queue default
[2020-03-16 13:41:04,398] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_http_operator', 'http_sensor_check', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 13:41:04,401] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_http_operator', 'http_sensor_check', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 13:41:07,122] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:41:07,125] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:41:08,126] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:41:08,128] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:41:09,475] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2408
[2020-03-16 13:41:12,994] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-14T04:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:41:13,076] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-14T04:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:41:13,481] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:41:13,483] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py
[2020-03-16 13:41:16,386] {scheduler_job.py:927} INFO - 3 tasks up for execution:
        <TaskInstance: example_branch_operator.run_this_first 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.condition_is_True 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.condition_is_False 2020-03-14 00:00:00+00:00 [scheduled]>
[2020-03-16 13:41:16,401] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 116 open slots and 3 task instances ready to be queued
[2020-03-16 13:41:16,402] {scheduler_job.py:986} INFO - DAG example_branch_operator has 2/16 running and queued tasks
[2020-03-16 13:41:16,406] {scheduler_job.py:986} INFO - DAG example_short_circuit_operator has 2/16 running and queued tasks
[2020-03-16 13:41:16,408] {scheduler_job.py:986} INFO - DAG example_short_circuit_operator has 3/16 running and queued tasks
[2020-03-16 13:41:16,430] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_branch_operator.run_this_first 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.condition_is_True 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.condition_is_False 2020-03-14 00:00:00+00:00 [scheduled]>
[2020-03-16 13:41:16,705] {scheduler_job.py:1112} INFO - Setting the following 3 tasks to queued state:
        <TaskInstance: example_branch_operator.run_this_first 2020-03-14 00:00:00+00:00 [queued]>
        <TaskInstance: example_short_circuit_operator.condition_is_False 2020-03-14 00:00:00+00:00 [queued]>
        <TaskInstance: example_short_circuit_operator.condition_is_True 2020-03-14 00:00:00+00:00 [queued]>
[2020-03-16 13:41:16,708] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'run_this_first', datetime.datetime(2020, 3, 14, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 11 and queue default
[2020-03-16 13:41:16,710] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'run_this_first', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:41:16,711] {scheduler_job.py:1148} INFO - Sending ('example_short_circuit_operator', 'condition_is_False', datetime.datetime(2020, 3, 14, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:41:16,712] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_short_circuit_operator', 'condition_is_False', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:41:16,713] {scheduler_job.py:1148} INFO - Sending ('example_short_circuit_operator', 'condition_is_True', datetime.datetime(2020, 3, 14, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:41:16,714] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_short_circuit_operator', 'condition_is_True', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:41:16,717] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'run_this_first', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:41:16,718] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_short_circuit_operator', 'condition_is_False', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:41:16,719] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_short_circuit_operator', 'condition_is_True', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:41:19,251] {cli.py:545} INFO - Running <TaskInstance: example_http_operator.http_sensor_check 2020-03-14T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:41:21,724] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2555
[2020-03-16 13:41:22,212] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2557
[2020-03-16 13:41:22,456] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-14 12:00:00+00:00 [scheduled]>
[2020-03-16 13:41:22,482] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 115 open slots and 1 task instances ready to be queued
[2020-03-16 13:41:22,485] {scheduler_job.py:986} INFO - DAG latest_only has 1/16 running and queued tasks
[2020-03-16 13:41:22,507] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-14 12:00:00+00:00 [scheduled]>
[2020-03-16 13:41:22,577] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2556
[2020-03-16 13:41:22,842] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-14 12:00:00+00:00 [queued]>
[2020-03-16 13:41:22,844] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 14, 12, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:41:22,845] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-14T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:41:22,848] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-14T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:41:22,848] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-14 04:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:41:30,415] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2614
[2020-03-16 13:41:42,537] {scheduler_job.py:927} INFO - 4 tasks up for execution:
        <TaskInstance: example_skip_dag.skip_operator_2 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.skip_operator_1 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.always_true_2 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.always_true_1 2020-03-14 00:00:00+00:00 [scheduled]>
[2020-03-16 13:41:42,598] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 114 open slots and 4 task instances ready to be queued
[2020-03-16 13:41:42,599] {scheduler_job.py:986} INFO - DAG example_skip_dag has 2/16 running and queued tasks
[2020-03-16 13:41:42,602] {scheduler_job.py:986} INFO - DAG example_skip_dag has 3/16 running and queued tasks
[2020-03-16 13:41:42,602] {scheduler_job.py:986} INFO - DAG example_skip_dag has 4/16 running and queued tasks
[2020-03-16 13:41:42,603] {scheduler_job.py:986} INFO - DAG example_skip_dag has 5/16 running and queued tasks
[2020-03-16 13:41:42,614] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_skip_dag.skip_operator_2 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.skip_operator_1 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.always_true_2 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.always_true_1 2020-03-14 00:00:00+00:00 [scheduled]>
[2020-03-16 13:41:43,838] {scheduler_job.py:1112} INFO - Setting the following 4 tasks to queued state:
        <TaskInstance: example_skip_dag.skip_operator_2 2020-03-14 00:00:00+00:00 [queued]>
        <TaskInstance: example_skip_dag.skip_operator_1 2020-03-14 00:00:00+00:00 [queued]>
        <TaskInstance: example_skip_dag.always_true_2 2020-03-14 00:00:00+00:00 [queued]>
        <TaskInstance: example_skip_dag.always_true_1 2020-03-14 00:00:00+00:00 [queued]>
[2020-03-16 13:41:43,854] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'skip_operator_2', datetime.datetime(2020, 3, 14, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:41:43,855] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'skip_operator_2', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:41:43,856] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'skip_operator_1', datetime.datetime(2020, 3, 14, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:41:43,858] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'skip_operator_1', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:41:43,859] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'always_true_2', datetime.datetime(2020, 3, 14, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:41:43,860] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'always_true_2', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:41:43,861] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'always_true_1', datetime.datetime(2020, 3, 14, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:41:43,862] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'always_true_1', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:41:43,865] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'skip_operator_2', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:41:43,867] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'always_true_2', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:41:43,867] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'skip_operator_1', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:41:43,870] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'always_true_1', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:41:48,564] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2664
[2020-03-16 13:41:48,676] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:41:48,680] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:41:48,686] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:41:48,894] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py
[2020-03-16 13:41:48,931] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2666
[2020-03-16 13:41:49,037] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-16 13:41:49,131] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2665
[2020-03-16 13:41:49,203] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2667
[2020-03-16 13:41:49,208] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:41:49,270] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py
[2020-03-16 13:41:50,329] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-16 13:41:51,812] {cli.py:545} INFO - Running <TaskInstance: example_short_circuit_operator.condition_is_False 2020-03-14T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:41:52,979] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.run_this_first 2020-03-14T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:41:53,747] {cli.py:545} INFO - Running <TaskInstance: example_short_circuit_operator.condition_is_True 2020-03-14T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:41:53,753] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-14T12:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:41:54,475] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:50:34,607] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-16 13:50:34,794] {scheduler_job.py:224} WARNING - Killing PID 2648
[2020-03-16 13:50:35,008] {scheduler_job.py:224} WARNING - Killing PID 2811
[2020-03-16 13:50:35,604] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:50:35,604] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:50:35,605] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:50:35,611] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-16 13:50:35,613] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-16 13:50:35,613] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-16 13:50:35,954] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.skip_operator_2 2020-03-14T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:50:35,954] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.skip_operator_1 2020-03-14T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:50:35,955] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.always_true_1 2020-03-14T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:50:36,564] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.always_true_2 2020-03-14T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:50:56,564] {scheduler_job.py:927} INFO - 3 tasks up for execution:
        <TaskInstance: example_http_operator.post_op 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_http_operator.post_op 2020-03-09 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_http_operator.http_sensor_check 2020-03-15 00:00:00+00:00 [scheduled]>
[2020-03-16 13:50:57,035] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 3 task instances ready to be queued
[2020-03-16 13:50:57,180] {scheduler_job.py:986} INFO - DAG example_http_operator has 0/16 running and queued tasks
[2020-03-16 13:50:57,258] {scheduler_job.py:986} INFO - DAG example_http_operator has 1/16 running and queued tasks
[2020-03-16 13:50:57,343] {scheduler_job.py:986} INFO - DAG example_http_operator has 2/16 running and queued tasks
[2020-03-16 13:50:57,463] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_http_operator.http_sensor_check 2020-03-15 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_http_operator.post_op 2020-03-09 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_http_operator.post_op 2020-03-10 00:00:00+00:00 [scheduled]>
[2020-03-16 13:50:57,616] {scheduler_job.py:1112} INFO - Setting the following 3 tasks to queued state:
        <TaskInstance: example_http_operator.post_op 2020-03-10 00:00:00+00:00 [queued]>
        <TaskInstance: example_http_operator.post_op 2020-03-09 00:00:00+00:00 [queued]>
        <TaskInstance: example_http_operator.http_sensor_check 2020-03-15 00:00:00+00:00 [queued]>
[2020-03-16 13:50:57,689] {scheduler_job.py:1148} INFO - Sending ('example_http_operator', 'post_op', datetime.datetime(2020, 3, 10, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 2) to executor with priority 5 and queue default
[2020-03-16 13:50:57,711] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_http_operator', 'post_op', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 13:50:57,717] {scheduler_job.py:1148} INFO - Sending ('example_http_operator', 'post_op', datetime.datetime(2020, 3, 9, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 2) to executor with priority 5 and queue default
[2020-03-16 13:50:57,722] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_http_operator', 'post_op', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 13:50:57,745] {scheduler_job.py:1148} INFO - Sending ('example_http_operator', 'http_sensor_check', datetime.datetime(2020, 3, 15, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 6 and queue default
[2020-03-16 13:50:57,751] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_http_operator', 'http_sensor_check', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 13:50:57,758] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_http_operator', 'http_sensor_check', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 13:50:57,761] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_http_operator', 'post_op', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 13:50:57,764] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_http_operator', 'post_op', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 13:50:57,765] {scheduler_job.py:1288} INFO - Executor reports execution of example_http_operator.post_op execution_date=2020-03-09 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:50:58,000] {scheduler_job.py:1288} INFO - Executor reports execution of example_http_operator.post_op execution_date=2020-03-10 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:50:58,159] {scheduler_job.py:1288} INFO - Executor reports execution of example_http_operator.http_sensor_check execution_date=2020-03-14 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:51:00,832] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3153
[2020-03-16 13:51:00,914] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3155
[2020-03-16 13:51:01,056] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3154
[2020-03-16 13:51:02,640] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:51:02,645] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py
[2020-03-16 13:51:02,714] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:51:02,719] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py
[2020-03-16 13:51:02,750] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:51:02,756] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py
[2020-03-16 13:51:02,882] {cli.py:545} INFO - Running <TaskInstance: example_http_operator.post_op 2020-03-09T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:51:02,981] {cli.py:545} INFO - Running <TaskInstance: example_http_operator.http_sensor_check 2020-03-15T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:51:03,018] {cli.py:545} INFO - Running <TaskInstance: example_http_operator.post_op 2020-03-10T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:51:04,467] {scheduler_job.py:927} INFO - 5 tasks up for execution:
        <TaskInstance: example_short_circuit_operator.true_2 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.true_2 2020-03-09 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.true_1 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.condition_is_True 2020-03-15 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.condition_is_False 2020-03-15 00:00:00+00:00 [scheduled]>
[2020-03-16 13:51:04,879] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 125 open slots and 5 task instances ready to be queued
[2020-03-16 13:51:04,891] {scheduler_job.py:986} INFO - DAG example_short_circuit_operator has 0/16 running and queued tasks
[2020-03-16 13:51:04,934] {scheduler_job.py:986} INFO - DAG example_short_circuit_operator has 1/16 running and queued tasks
[2020-03-16 13:51:04,978] {scheduler_job.py:986} INFO - DAG example_short_circuit_operator has 2/16 running and queued tasks
[2020-03-16 13:51:05,020] {scheduler_job.py:986} INFO - DAG example_short_circuit_operator has 3/16 running and queued tasks
[2020-03-16 13:51:05,027] {scheduler_job.py:986} INFO - DAG example_short_circuit_operator has 4/16 running and queued tasks
[2020-03-16 13:51:05,083] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_short_circuit_operator.condition_is_True 2020-03-15 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.condition_is_False 2020-03-15 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.true_1 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.true_2 2020-03-09 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.true_2 2020-03-10 00:00:00+00:00 [scheduled]>
[2020-03-16 13:51:05,295] {scheduler_job.py:1112} INFO - Setting the following 5 tasks to queued state:
        <TaskInstance: example_short_circuit_operator.true_2 2020-03-10 00:00:00+00:00 [queued]>
        <TaskInstance: example_short_circuit_operator.true_2 2020-03-09 00:00:00+00:00 [queued]>
        <TaskInstance: example_short_circuit_operator.true_1 2020-03-14 00:00:00+00:00 [queued]>
        <TaskInstance: example_short_circuit_operator.condition_is_True 2020-03-15 00:00:00+00:00 [queued]>
        <TaskInstance: example_short_circuit_operator.condition_is_False 2020-03-15 00:00:00+00:00 [queued]>
[2020-03-16 13:51:05,512] {scheduler_job.py:1148} INFO - Sending ('example_short_circuit_operator', 'true_2', datetime.datetime(2020, 3, 10, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 13:51:05,532] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_short_circuit_operator', 'true_2', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:51:05,560] {scheduler_job.py:1148} INFO - Sending ('example_short_circuit_operator', 'true_2', datetime.datetime(2020, 3, 9, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 13:51:05,587] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_short_circuit_operator', 'true_2', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:51:05,644] {scheduler_job.py:1148} INFO - Sending ('example_short_circuit_operator', 'true_1', datetime.datetime(2020, 3, 14, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:51:05,658] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_short_circuit_operator', 'true_1', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:51:05,671] {scheduler_job.py:1148} INFO - Sending ('example_short_circuit_operator', 'condition_is_True', datetime.datetime(2020, 3, 15, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:51:05,679] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_short_circuit_operator', 'condition_is_True', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:51:05,698] {scheduler_job.py:1148} INFO - Sending ('example_short_circuit_operator', 'condition_is_False', datetime.datetime(2020, 3, 15, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:51:05,701] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_short_circuit_operator', 'condition_is_False', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:51:05,716] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_short_circuit_operator', 'condition_is_True', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:51:05,723] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_short_circuit_operator', 'condition_is_False', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:51:05,738] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_short_circuit_operator', 'true_1', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:51:05,745] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_short_circuit_operator', 'true_2', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:51:05,750] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_short_circuit_operator', 'true_2', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:51:05,796] {scheduler_job.py:1288} INFO - Executor reports execution of example_short_circuit_operator.true_1 execution_date=2020-03-10 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:51:06,865] {scheduler_job.py:1288} INFO - Executor reports execution of example_short_circuit_operator.true_1 execution_date=2020-03-09 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:51:07,248] {scheduler_job.py:1288} INFO - Executor reports execution of example_short_circuit_operator.condition_is_True execution_date=2020-03-14 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:51:07,301] {scheduler_job.py:1288} INFO - Executor reports execution of example_short_circuit_operator.condition_is_False execution_date=2020-03-14 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:51:09,533] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3248
[2020-03-16 13:51:09,681] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3247
[2020-03-16 13:51:10,076] {scheduler_job.py:927} INFO - 5 tasks up for execution:
        <TaskInstance: example_branch_operator.run_this_first 2020-03-15 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_branch_operator.follow_branch_a 2020-03-09 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_branch_operator.branching 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_branch_operator.branch_d 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only.latest_only 2020-03-14 16:00:00+00:00 [scheduled]>[2020-03-16 13:51:10,103] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3253
[2020-03-16 13:51:10,124] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3249
[2020-03-16 13:51:10,186] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3254

[2020-03-16 13:51:10,477] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 120 open slots and 5 task instances ready to be queued
[2020-03-16 13:51:10,501] {scheduler_job.py:986} INFO - DAG example_branch_operator has 0/16 running and queued tasks
[2020-03-16 13:51:10,506] {scheduler_job.py:986} INFO - DAG example_branch_operator has 1/16 running and queued tasks
[2020-03-16 13:51:10,624] {scheduler_job.py:986} INFO - DAG example_branch_operator has 2/16 running and queued tasks
[2020-03-16 13:51:10,743] {scheduler_job.py:986} INFO - DAG example_branch_operator has 3/16 running and queued tasks
[2020-03-16 13:51:10,814] {scheduler_job.py:986} INFO - DAG latest_only has 0/16 running and queued tasks
[2020-03-16 13:51:10,983] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_branch_operator.run_this_first 2020-03-15 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_branch_operator.branching 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_branch_operator.branch_d 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_branch_operator.follow_branch_a 2020-03-09 00:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only.latest_only 2020-03-14 16:00:00+00:00 [scheduled]>
[2020-03-16 13:51:12,055] {scheduler_job.py:1112} INFO - Setting the following 5 tasks to queued state:
        <TaskInstance: example_branch_operator.follow_branch_a 2020-03-09 00:00:00+00:00 [queued]>
        <TaskInstance: example_branch_operator.branch_d 2020-03-10 00:00:00+00:00 [queued]>
        <TaskInstance: example_branch_operator.branching 2020-03-14 00:00:00+00:00 [queued]>
        <TaskInstance: example_branch_operator.run_this_first 2020-03-15 00:00:00+00:00 [queued]>
        <TaskInstance: latest_only.latest_only 2020-03-14 16:00:00+00:00 [queued]>
[2020-03-16 13:51:12,330] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'follow_branch_a', datetime.datetime(2020, 3, 9, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:51:12,381] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'follow_branch_a', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:51:12,451] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'branch_d', datetime.datetime(2020, 3, 10, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:51:12,499] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'branch_d', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:51:12,536] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'branching', datetime.datetime(2020, 3, 14, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 10 and queue default
[2020-03-16 13:51:12,587] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'branching', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:51:12,598] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:51:12,611] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'run_this_first', datetime.datetime(2020, 3, 15, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 11 and queue default
[2020-03-16 13:51:12,653] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py
[2020-03-16 13:51:12,665] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'run_this_first', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:51:12,696] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 14, 16, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:51:12,706] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-14T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:51:12,722] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'run_this_first', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:51:12,729] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'branching', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:51:12,737] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'branch_d', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:51:12,753] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'follow_branch_a', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:51:12,770] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:51:12,773] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-14T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:51:12,782] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.branch_a execution_date=2020-03-09 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:51:12,867] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py
[2020-03-16 13:51:12,996] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:51:13,172] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:51:13,176] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:51:13,180] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.branching execution_date=2020-03-10 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:51:13,284] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py
[2020-03-16 13:51:13,483] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py
[2020-03-16 13:51:13,651] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py
[2020-03-16 13:51:13,893] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-14 08:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:51:14,227] {cli.py:545} INFO - Running <TaskInstance: example_short_circuit_operator.condition_is_False 2020-03-15T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:51:14,401] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-14 12:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:51:14,680] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.run_this_first execution_date=2020-03-14 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:51:15,615] {cli.py:545} INFO - Running <TaskInstance: example_short_circuit_operator.condition_is_True 2020-03-15T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:51:16,260] {cli.py:545} INFO - Running <TaskInstance: example_short_circuit_operator.true_2 2020-03-10T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:51:16,518] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3345
[2020-03-16 13:51:16,541] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3346
[2020-03-16 13:51:16,585] {cli.py:545} INFO - Running <TaskInstance: example_short_circuit_operator.true_2 2020-03-09T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:51:16,723] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3349
[2020-03-16 13:51:16,731] {cli.py:545} INFO - Running <TaskInstance: example_short_circuit_operator.true_1 2020-03-14T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:51:16,962] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3348
[2020-03-16 13:51:17,213] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3351
[2020-03-16 13:51:19,512] {scheduler_job.py:927} INFO - 6 tasks up for execution:
        <TaskInstance: example_skip_dag.skip_operator_2 2020-03-15 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.skip_operator_1 2020-03-15 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.one_success 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.final_2 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.always_true_2 2020-03-15 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.always_true_1 2020-03-15 00:00:00+00:00 [scheduled]>
[2020-03-16 13:51:20,138] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 118 open slots and 6 task instances ready to be queued
[2020-03-16 13:51:20,196] {scheduler_job.py:986} INFO - DAG example_skip_dag has 0/16 running and queued tasks
[2020-03-16 13:51:20,235] {scheduler_job.py:986} INFO - DAG example_skip_dag has 1/16 running and queued tasks
[2020-03-16 13:51:20,289] {scheduler_job.py:986} INFO - DAG example_skip_dag has 2/16 running and queued tasks
[2020-03-16 13:51:20,318] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:51:20,325] {scheduler_job.py:986} INFO - DAG example_skip_dag has 3/16 running and queued tasks
[2020-03-16 13:51:20,355] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-16 13:51:20,371] {scheduler_job.py:986} INFO - DAG example_skip_dag has 4/16 running and queued tasks
[2020-03-16 13:51:20,435] {scheduler_job.py:986} INFO - DAG example_skip_dag has 5/16 running and queued tasks
[2020-03-16 13:51:20,540] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:51:20,574] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-16 13:51:20,613] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_skip_dag.skip_operator_2 2020-03-15 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.skip_operator_1 2020-03-15 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.always_true_2 2020-03-15 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.always_true_1 2020-03-15 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.one_success 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.final_2 2020-03-10 00:00:00+00:00 [scheduled]>[2020-03-16 13:51:20,623] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:51:20,626] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:51:20,633] {__init__.py:51} INFO - Using executor LocalExecutor

[2020-03-16 13:51:21,540] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-16 13:51:21,655] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-16 13:51:21,840] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-16 13:51:21,954] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.run_this_first 2020-03-15T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:51:22,731] {scheduler_job.py:1112} INFO - Setting the following 6 tasks to queued state:
        <TaskInstance: example_skip_dag.skip_operator_2 2020-03-15 00:00:00+00:00 [queued]>
        <TaskInstance: example_skip_dag.skip_operator_1 2020-03-15 00:00:00+00:00 [queued]>
        <TaskInstance: example_skip_dag.one_success 2020-03-14 00:00:00+00:00 [queued]>
        <TaskInstance: example_skip_dag.final_2 2020-03-10 00:00:00+00:00 [queued]>
        <TaskInstance: example_skip_dag.always_true_2 2020-03-15 00:00:00+00:00 [queued]>
        <TaskInstance: example_skip_dag.always_true_1 2020-03-15 00:00:00+00:00 [queued]>[2020-03-16 13:51:22,827] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.follow_branch_a 2020-03-09T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain

[2020-03-16 13:51:23,722] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'skip_operator_2', datetime.datetime(2020, 3, 15, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:51:23,833] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'skip_operator_2', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:51:23,905] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'skip_operator_1', datetime.datetime(2020, 3, 15, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:51:24,009] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'skip_operator_1', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:51:24,112] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.branching 2020-03-14T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:51:24,164] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'one_success', datetime.datetime(2020, 3, 14, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:51:24,449] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.branch_d 2020-03-10T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:51:24,539] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'one_success', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:51:24,685] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-14T16:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:51:24,786] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'final_2', datetime.datetime(2020, 3, 10, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 13:51:25,211] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'final_2', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:51:25,270] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'always_true_2', datetime.datetime(2020, 3, 15, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:51:25,284] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'always_true_2', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:51:25,351] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'always_true_1', datetime.datetime(2020, 3, 15, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:51:25,370] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'always_true_1', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:51:25,570] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'skip_operator_2', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:51:25,583] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'skip_operator_1', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:51:25,589] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'always_true_2', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:51:25,651] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'always_true_1', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:51:25,710] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'one_success', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:51:25,767] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'final_2', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:51:26,008] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.final_2 execution_date=2020-03-09 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:51:26,276] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.one_success execution_date=2020-03-10 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:51:26,519] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.skip_operator_1 execution_date=2020-03-14 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:51:26,698] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.skip_operator_2 execution_date=2020-03-14 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:51:26,976] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.always_true_1 execution_date=2020-03-14 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:51:27,122] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.always_true_2 execution_date=2020-03-14 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:51:30,565] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-14 08:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-14 08:00:00+00:00 [scheduled]>[2020-03-16 13:51:30,885] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3556

[2020-03-16 13:51:31,418] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3551
[2020-03-16 13:51:31,669] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3562
[2020-03-16 13:51:31,767] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3565
[2020-03-16 13:51:31,786] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 2 task instances ready to be queued
[2020-03-16 13:51:31,894] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3557
[2020-03-16 13:51:31,912] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3558
[2020-03-16 13:51:32,617] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-16 13:51:33,295] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-16 13:51:34,000] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-14 08:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-14 08:00:00+00:00 [scheduled]>
[2020-03-16 13:51:35,047] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-14 08:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-14 08:00:00+00:00 [queued]>
[2020-03-16 13:51:35,664] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 14, 8, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:51:35,982] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-14T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:51:36,236] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 14, 8, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-16 13:51:36,255] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-14T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:51:36,368] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-14T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:51:36,392] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-14T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:51:36,394] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-14 04:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:51:36,672] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-14 04:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:51:37,674] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:51:37,787] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-16 13:51:38,640] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:51:38,798] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-16 13:51:39,277] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:51:39,350] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-16 13:51:39,767] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:51:39,838] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-16 13:51:40,018] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:51:40,088] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:51:40,278] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-16 13:51:40,501] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-16 13:51:40,782] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.skip_operator_1 2020-03-15T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:51:41,300] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.final_2 2020-03-10T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:51:41,643] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3721
[2020-03-16 13:51:41,678] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.skip_operator_2 2020-03-15T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:51:41,930] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3720
[2020-03-16 13:51:42,106] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.one_success 2020-03-14T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:51:42,540] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.always_true_2 2020-03-15T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:51:42,728] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.always_true_1 2020-03-15T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:51:43,113] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: example_http_operator.post_op 2020-03-15 00:00:00+00:00 [scheduled]>
[2020-03-16 13:51:43,307] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 115 open slots and 1 task instances ready to be queued
[2020-03-16 13:51:43,412] {scheduler_job.py:986} INFO - DAG example_http_operator has 0/16 running and queued tasks
[2020-03-16 13:51:43,702] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_http_operator.post_op 2020-03-15 00:00:00+00:00 [scheduled]>
[2020-03-16 13:51:44,486] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: example_http_operator.post_op 2020-03-15 00:00:00+00:00 [queued]>
[2020-03-16 13:51:44,512] {scheduler_job.py:1148} INFO - Sending ('example_http_operator', 'post_op', datetime.datetime(2020, 3, 15, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 5 and queue default
[2020-03-16 13:51:44,519] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_http_operator', 'post_op', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 13:51:44,530] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_http_operator', 'post_op', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 13:51:44,562] {scheduler_job.py:1288} INFO - Executor reports execution of example_http_operator.post_op execution_date=2020-03-09 00:00:00+00:00 exited with status success for try_number 2
[2020-03-16 13:51:44,586] {scheduler_job.py:1288} INFO - Executor reports execution of example_http_operator.http_sensor_check execution_date=2020-03-15 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:51:44,599] {scheduler_job.py:1288} INFO - Executor reports execution of example_http_operator.post_op execution_date=2020-03-10 00:00:00+00:00 exited with status success for try_number 2
[2020-03-16 13:51:44,745] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:51:44,748] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:51:44,897] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:51:45,065] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:51:47,884] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-14T08:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:51:47,997] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-14T08:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:51:48,462] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=3887
[2020-03-16 13:51:52,753] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:51:52,817] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py
[2020-03-16 13:51:54,998] {cli.py:545} INFO - Running <TaskInstance: example_http_operator.post_op 2020-03-15T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:51:59,645] {scheduler_job.py:927} INFO - 6 tasks up for execution:
        <TaskInstance: example_branch_operator.join 2020-03-09 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_branch_operator.follow_branch_d 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_branch_operator.branching 2020-03-15 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_branch_operator.branch_c 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.true_2 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.true_1 2020-03-15 00:00:00+00:00 [scheduled]>
[2020-03-16 13:51:59,759] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 6 task instances ready to be queued
[2020-03-16 13:51:59,769] {scheduler_job.py:986} INFO - DAG example_branch_operator has 0/16 running and queued tasks
[2020-03-16 13:51:59,778] {scheduler_job.py:986} INFO - DAG example_branch_operator has 1/16 running and queued tasks
[2020-03-16 13:51:59,788] {scheduler_job.py:986} INFO - DAG example_branch_operator has 2/16 running and queued tasks
[2020-03-16 13:51:59,794] {scheduler_job.py:986} INFO - DAG example_short_circuit_operator has 0/16 running and queued tasks
[2020-03-16 13:51:59,821] {scheduler_job.py:986} INFO - DAG example_branch_operator has 3/16 running and queued tasks
[2020-03-16 13:51:59,851] {scheduler_job.py:986} INFO - DAG example_short_circuit_operator has 1/16 running and queued tasks
[2020-03-16 13:51:59,988] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_branch_operator.branching 2020-03-15 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_branch_operator.branch_c 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_branch_operator.follow_branch_d 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.true_1 2020-03-15 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_branch_operator.join 2020-03-09 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.true_2 2020-03-14 00:00:00+00:00 [scheduled]>
[2020-03-16 13:52:00,414] {scheduler_job.py:1112} INFO - Setting the following 6 tasks to queued state:
        <TaskInstance: example_branch_operator.join 2020-03-09 00:00:00+00:00 [queued]>
        <TaskInstance: example_branch_operator.follow_branch_d 2020-03-10 00:00:00+00:00 [queued]>
        <TaskInstance: example_branch_operator.branch_c 2020-03-14 00:00:00+00:00 [queued]>
        <TaskInstance: example_short_circuit_operator.true_2 2020-03-14 00:00:00+00:00 [queued]>
        <TaskInstance: example_short_circuit_operator.true_1 2020-03-15 00:00:00+00:00 [queued]>
        <TaskInstance: example_branch_operator.branching 2020-03-15 00:00:00+00:00 [queued]>
[2020-03-16 13:52:00,557] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'join', datetime.datetime(2020, 3, 9, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 13:52:00,609] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'join', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:52:00,623] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'follow_branch_d', datetime.datetime(2020, 3, 10, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:52:00,629] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'follow_branch_d', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:52:00,634] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'branch_c', datetime.datetime(2020, 3, 14, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:52:00,636] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'branch_c', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:52:00,639] {scheduler_job.py:1148} INFO - Sending ('example_short_circuit_operator', 'true_2', datetime.datetime(2020, 3, 14, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 13:52:00,643] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_short_circuit_operator', 'true_2', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:52:00,649] {scheduler_job.py:1148} INFO - Sending ('example_short_circuit_operator', 'true_1', datetime.datetime(2020, 3, 15, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:52:00,653] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_short_circuit_operator', 'true_1', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:52:00,667] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'branching', datetime.datetime(2020, 3, 15, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 10 and queue default
[2020-03-16 13:52:00,682] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'branching', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:52:00,813] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'branching', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:52:00,880] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'branch_c', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:52:00,927] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'follow_branch_d', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:52:00,950] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_short_circuit_operator', 'true_1', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:52:01,023] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'join', '2020-03-09T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:52:01,062] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_short_circuit_operator', 'true_2', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:52:01,482] {scheduler_job.py:1288} INFO - Executor reports execution of example_short_circuit_operator.condition_is_False execution_date=2020-03-15 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:01,747] {scheduler_job.py:1288} INFO - Executor reports execution of example_short_circuit_operator.condition_is_True execution_date=2020-03-15 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:01,757] {scheduler_job.py:1288} INFO - Executor reports execution of example_short_circuit_operator.true_2 execution_date=2020-03-10 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:01,902] {scheduler_job.py:1288} INFO - Executor reports execution of example_short_circuit_operator.true_2 execution_date=2020-03-09 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:02,176] {scheduler_job.py:1288} INFO - Executor reports execution of example_short_circuit_operator.true_1 execution_date=2020-03-14 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:02,389] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.run_this_first execution_date=2020-03-15 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:02,522] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.branching execution_date=2020-03-14 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:02,633] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.follow_branch_a execution_date=2020-03-09 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:02,777] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.branch_d execution_date=2020-03-10 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:04,651] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=4189
[2020-03-16 13:52:04,996] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=4195
[2020-03-16 13:52:05,005] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=4196
[2020-03-16 13:52:05,016] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=4198
[2020-03-16 13:52:05,024] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=4191
[2020-03-16 13:52:05,030] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=4194
[2020-03-16 13:52:05,243] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-14 20:00:00+00:00 [scheduled]>
[2020-03-16 13:52:05,992] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 121 open slots and 1 task instances ready to be queued
[2020-03-16 13:52:06,087] {scheduler_job.py:986} INFO - DAG latest_only has 0/16 running and queued tasks
[2020-03-16 13:52:06,669] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-14 20:00:00+00:00 [scheduled]>
[2020-03-16 13:52:07,661] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-14 20:00:00+00:00 [queued]>
[2020-03-16 13:52:07,790] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 14, 20, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:52:07,800] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-14T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:52:07,864] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-14T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:52:08,032] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-14 16:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:08,758] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:52:08,822] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-16 13:52:09,092] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:52:09,097] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:52:09,103] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:52:09,110] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:52:09,115] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:52:09,140] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-16 13:52:09,171] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py
[2020-03-16 13:52:09,214] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-16 13:52:09,228] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py
[2020-03-16 13:52:09,249] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-16 13:52:09,262] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.branching 2020-03-15T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:52:10,190] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.follow_branch_d 2020-03-10T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:52:10,216] {cli.py:545} INFO - Running <TaskInstance: example_short_circuit_operator.true_1 2020-03-15T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:52:10,387] {cli.py:545} INFO - Running <TaskInstance: example_short_circuit_operator.true_2 2020-03-14T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:52:10,387] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.join 2020-03-09T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:52:10,425] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.branch_c 2020-03-14T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:52:10,577] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=4260
[2020-03-16 13:52:11,646] {scheduler_job.py:927} INFO - 3 tasks up for execution:
        <TaskInstance: tutorial.templated 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: tutorial.sleep 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: tutorial.print_date 2020-03-14 00:00:00+00:00 [scheduled]>
[2020-03-16 13:52:12,018] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 120 open slots and 3 task instances ready to be queued
[2020-03-16 13:52:12,107] {scheduler_job.py:986} INFO - DAG tutorial has 0/16 running and queued tasks
[2020-03-16 13:52:12,255] {scheduler_job.py:986} INFO - DAG tutorial has 1/16 running and queued tasks
[2020-03-16 13:52:12,372] {scheduler_job.py:986} INFO - DAG tutorial has 2/16 running and queued tasks
[2020-03-16 13:52:12,884] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: tutorial.print_date 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: tutorial.templated 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: tutorial.sleep 2020-03-10 00:00:00+00:00 [scheduled]>
[2020-03-16 13:52:14,462] {scheduler_job.py:1112} INFO - Setting the following 3 tasks to queued state:
        <TaskInstance: tutorial.templated 2020-03-10 00:00:00+00:00 [queued]>
        <TaskInstance: tutorial.sleep 2020-03-10 00:00:00+00:00 [queued]>
        <TaskInstance: tutorial.print_date 2020-03-14 00:00:00+00:00 [queued]>
[2020-03-16 13:52:15,020] {scheduler_job.py:1148} INFO - Sending ('tutorial', 'templated', datetime.datetime(2020, 3, 10, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 13:52:15,317] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'tutorial', 'templated', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 13:52:15,449] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:52:15,607] {scheduler_job.py:1148} INFO - Sending ('tutorial', 'sleep', datetime.datetime(2020, 3, 10, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 13:52:15,805] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-16 13:52:16,344] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'tutorial', 'sleep', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 13:52:16,887] {scheduler_job.py:1148} INFO - Sending ('tutorial', 'print_date', datetime.datetime(2020, 3, 14, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:52:17,028] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'tutorial', 'print_date', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 13:52:17,185] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'tutorial', 'print_date', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 13:52:17,257] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'tutorial', 'templated', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 13:52:17,343] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'tutorial', 'sleep', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 13:52:17,393] {scheduler_job.py:1288} INFO - Executor reports execution of tutorial.print_date execution_date=2020-03-10 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:18,893] {scheduler_job.py:1288} INFO - Executor reports execution of tutorial.templated execution_date=2020-03-09 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:19,259] {scheduler_job.py:1288} INFO - Executor reports execution of tutorial.sleep execution_date=2020-03-09 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:20,004] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-14T20:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:52:24,655] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=4386
[2020-03-16 13:52:25,171] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=4387
[2020-03-16 13:52:26,177] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=4388
[2020-03-16 13:52:29,870] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:52:29,879] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:52:29,902] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py
[2020-03-16 13:52:29,931] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py
[2020-03-16 13:52:30,083] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:52:30,133] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py
[2020-03-16 13:52:31,247] {cli.py:545} INFO - Running <TaskInstance: tutorial.print_date 2020-03-14T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:52:31,299] {cli.py:545} INFO - Running <TaskInstance: tutorial.templated 2020-03-10T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:52:31,427] {cli.py:545} INFO - Running <TaskInstance: tutorial.sleep 2020-03-10T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:52:32,071] {scheduler_job.py:1288} INFO - Executor reports execution of example_http_operator.post_op execution_date=2020-03-15 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:44,399] {scheduler_job.py:927} INFO - 4 tasks up for execution:
        <TaskInstance: example_branch_operator.join 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_branch_operator.follow_branch_c 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_branch_operator.branch_b 2020-03-15 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.true_2 2020-03-15 00:00:00+00:00 [scheduled]>
[2020-03-16 13:52:44,414] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 127 open slots and 4 task instances ready to be queued
[2020-03-16 13:52:44,415] {scheduler_job.py:986} INFO - DAG example_branch_operator has 0/16 running and queued tasks
[2020-03-16 13:52:44,416] {scheduler_job.py:986} INFO - DAG example_branch_operator has 1/16 running and queued tasks
[2020-03-16 13:52:44,418] {scheduler_job.py:986} INFO - DAG example_branch_operator has 2/16 running and queued tasks
[2020-03-16 13:52:44,418] {scheduler_job.py:986} INFO - DAG example_short_circuit_operator has 0/16 running and queued tasks
[2020-03-16 13:52:44,440] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_branch_operator.branch_b 2020-03-15 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_branch_operator.follow_branch_c 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_branch_operator.join 2020-03-10 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.true_2 2020-03-15 00:00:00+00:00 [scheduled]>
[2020-03-16 13:52:44,996] {scheduler_job.py:1112} INFO - Setting the following 4 tasks to queued state:
        <TaskInstance: example_branch_operator.join 2020-03-10 00:00:00+00:00 [queued]>
        <TaskInstance: example_branch_operator.follow_branch_c 2020-03-14 00:00:00+00:00 [queued]>
        <TaskInstance: example_short_circuit_operator.true_2 2020-03-15 00:00:00+00:00 [queued]>
        <TaskInstance: example_branch_operator.branch_b 2020-03-15 00:00:00+00:00 [queued]>
[2020-03-16 13:52:45,196] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'join', datetime.datetime(2020, 3, 10, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 13:52:45,197] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'join', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:52:45,198] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'follow_branch_c', datetime.datetime(2020, 3, 14, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:52:45,199] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'follow_branch_c', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:52:45,201] {scheduler_job.py:1148} INFO - Sending ('example_short_circuit_operator', 'true_2', datetime.datetime(2020, 3, 15, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 13:52:45,202] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_short_circuit_operator', 'true_2', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:52:45,203] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'branch_b', datetime.datetime(2020, 3, 15, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:52:45,204] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'branch_b', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:52:45,206] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'branch_b', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:52:45,206] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'follow_branch_c', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:52:45,207] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.follow_branch_d execution_date=2020-03-10 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:45,207] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'join', '2020-03-10T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:52:45,208] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_short_circuit_operator', 'true_2', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 13:52:45,216] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.branching execution_date=2020-03-15 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:45,237] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.join execution_date=2020-03-09 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:45,262] {scheduler_job.py:1288} INFO - Executor reports execution of example_short_circuit_operator.true_2 execution_date=2020-03-14 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:45,363] {scheduler_job.py:1288} INFO - Executor reports execution of example_short_circuit_operator.true_1 execution_date=2020-03-15 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:45,508] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.branch_c execution_date=2020-03-14 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:46,972] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-15 00:00:00+00:00 [scheduled]>
[2020-03-16 13:52:47,430] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 123 open slots and 1 task instances ready to be queued
[2020-03-16 13:52:47,559] {scheduler_job.py:986} INFO - DAG latest_only has 0/16 running and queued tasks
[2020-03-16 13:52:47,721] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-15 00:00:00+00:00 [scheduled]>
[2020-03-16 13:52:47,904] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=4731
[2020-03-16 13:52:47,918] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=4732
[2020-03-16 13:52:47,938] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=4733
[2020-03-16 13:52:48,108] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=4730
[2020-03-16 13:52:48,174] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-15 00:00:00+00:00 [queued]>
[2020-03-16 13:52:48,655] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 15, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:52:48,847] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:52:49,074] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:52:49,425] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-14 20:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:50,706] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:52:50,711] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-16 13:52:50,784] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:52:50,791] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:52:50,798] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py
[2020-03-16 13:52:50,813] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-16 13:52:51,029] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:52:51,042] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-16 13:52:51,231] {cli.py:545} INFO - Running <TaskInstance: example_short_circuit_operator.true_2 2020-03-15T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:52:51,309] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.join 2020-03-10T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:52:51,358] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.follow_branch_c 2020-03-14T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:52:51,426] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: example_skip_dag.one_success 2020-03-15 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.final_2 2020-03-14 00:00:00+00:00 [scheduled]>
[2020-03-16 13:52:51,474] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 123 open slots and 2 task instances ready to be queued
[2020-03-16 13:52:51,480] {scheduler_job.py:986} INFO - DAG example_skip_dag has 0/16 running and queued tasks
[2020-03-16 13:52:51,484] {scheduler_job.py:986} INFO - DAG example_skip_dag has 1/16 running and queued tasks
[2020-03-16 13:52:51,533] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_skip_dag.one_success 2020-03-15 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.final_2 2020-03-14 00:00:00+00:00 [scheduled]>
[2020-03-16 13:52:51,570] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.branch_b 2020-03-15T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:52:51,778] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: example_skip_dag.one_success 2020-03-15 00:00:00+00:00 [queued]>
        <TaskInstance: example_skip_dag.final_2 2020-03-14 00:00:00+00:00 [queued]>[2020-03-16 13:52:51,910] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=4774

[2020-03-16 13:52:52,312] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'one_success', datetime.datetime(2020, 3, 15, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:52:52,323] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'one_success', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:52:52,328] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'final_2', datetime.datetime(2020, 3, 14, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 13:52:52,333] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'final_2', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:52:52,345] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'one_success', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:52:52,348] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'final_2', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:52:52,351] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.skip_operator_1 execution_date=2020-03-15 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:52,564] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.final_2 execution_date=2020-03-10 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:52,703] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.skip_operator_2 execution_date=2020-03-15 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:52,847] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.always_true_1 execution_date=2020-03-15 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:53,183] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.always_true_2 execution_date=2020-03-15 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:53,435] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.one_success execution_date=2020-03-14 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:55,696] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-14 12:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-14 12:00:00+00:00 [scheduled]>
[2020-03-16 13:52:56,124] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 121 open slots and 2 task instances ready to be queued
[2020-03-16 13:52:56,182] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-16 13:52:56,295] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-16 13:52:56,719] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-14 12:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-14 12:00:00+00:00 [scheduled]>[2020-03-16 13:52:56,748] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=4837
[2020-03-16 13:52:56,810] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=4838
[2020-03-16 13:52:56,893] {__init__.py:51} INFO - Using executor LocalExecutor

[2020-03-16 13:52:57,529] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-16 13:52:58,418] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-14 12:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-14 12:00:00+00:00 [queued]>
[2020-03-16 13:52:58,957] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 14, 12, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:52:59,178] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-14T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:52:59,232] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 14, 12, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-16 13:52:59,256] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-14T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:52:59,398] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-14T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:52:59,401] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-14T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:52:59,513] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-14 08:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:52:59,519] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-15T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:52:59,962] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-14 08:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:53:01,079] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:53:01,090] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-16 13:53:01,171] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:53:01,211] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-16 13:53:01,671] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.one_success 2020-03-15T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:53:02,128] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=4916
[2020-03-16 13:53:02,164] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=4914
[2020-03-16 13:53:02,174] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.final_2 2020-03-14T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:53:04,550] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:53:04,599] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:53:04,776] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:53:04,800] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:53:05,664] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-14T12:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:53:05,746] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-14T12:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:53:08,215] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: example_branch_operator.join 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_branch_operator.follow_branch_b 2020-03-15 00:00:00+00:00 [scheduled]>
[2020-03-16 13:53:08,491] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 123 open slots and 2 task instances ready to be queued
[2020-03-16 13:53:08,576] {scheduler_job.py:986} INFO - DAG example_branch_operator has 0/16 running and queued tasks
[2020-03-16 13:53:08,625] {scheduler_job.py:986} INFO - DAG example_branch_operator has 1/16 running and queued tasks
[2020-03-16 13:53:09,270] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_branch_operator.follow_branch_b 2020-03-15 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_branch_operator.join 2020-03-14 00:00:00+00:00 [scheduled]>
[2020-03-16 13:53:10,086] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: example_branch_operator.join 2020-03-14 00:00:00+00:00 [queued]>
        <TaskInstance: example_branch_operator.follow_branch_b 2020-03-15 00:00:00+00:00 [queued]>
[2020-03-16 13:53:10,349] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'join', datetime.datetime(2020, 3, 14, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 13:53:10,359] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'join', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:53:10,392] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'follow_branch_b', datetime.datetime(2020, 3, 15, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:53:10,405] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'follow_branch_b', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:53:10,420] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'follow_branch_b', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:53:10,431] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'join', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:53:10,615] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.follow_branch_c execution_date=2020-03-14 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:53:10,754] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.join execution_date=2020-03-10 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:53:10,926] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.branch_b execution_date=2020-03-15 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:53:12,546] {scheduler_job.py:1288} INFO - Executor reports execution of example_short_circuit_operator.true_2 execution_date=2020-03-15 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:53:12,782] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=5137
[2020-03-16 13:53:12,793] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=5138
[2020-03-16 13:53:14,316] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-15 04:00:00+00:00 [scheduled]>
[2020-03-16 13:53:14,454] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued
[2020-03-16 13:53:14,492] {scheduler_job.py:986} INFO - DAG latest_only has 0/16 running and queued tasks
[2020-03-16 13:53:14,588] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-15 04:00:00+00:00 [scheduled]>
[2020-03-16 13:53:14,711] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-15 04:00:00+00:00 [queued]>
[2020-03-16 13:53:14,714] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 15, 4, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:53:14,715] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-15T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:53:14,718] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-15 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:53:14,718] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-15T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:53:14,782] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:53:14,784] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:53:14,786] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-16 13:53:14,788] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-16 13:53:14,933] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.join 2020-03-14T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:53:14,939] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.follow_branch_b 2020-03-15T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:53:16,360] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: example_skip_dag.final_2 2020-03-15 00:00:00+00:00 [scheduled]>
[2020-03-16 13:53:16,399] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 125 open slots and 1 task instances ready to be queued
[2020-03-16 13:53:16,403] {scheduler_job.py:986} INFO - DAG example_skip_dag has 0/16 running and queued tasks
[2020-03-16 13:53:16,433] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_skip_dag.final_2 2020-03-15 00:00:00+00:00 [scheduled]>[2020-03-16 13:53:16,434] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=5216

[2020-03-16 13:53:16,683] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: example_skip_dag.final_2 2020-03-15 00:00:00+00:00 [queued]>
[2020-03-16 13:53:16,906] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'final_2', datetime.datetime(2020, 3, 15, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 13:53:17,066] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'final_2', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:53:17,159] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'final_2', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 13:53:17,238] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.final_2 execution_date=2020-03-14 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:53:18,741] {scheduler_job.py:927} INFO - 3 tasks up for execution:
        <TaskInstance: tutorial.templated 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: tutorial.sleep 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: tutorial.print_date 2020-03-15 00:00:00+00:00 [scheduled]>
[2020-03-16 13:53:18,784] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 124 open slots and 3 task instances ready to be queued
[2020-03-16 13:53:18,788] {scheduler_job.py:986} INFO - DAG tutorial has 0/16 running and queued tasks
[2020-03-16 13:53:18,792] {scheduler_job.py:986} INFO - DAG tutorial has 1/16 running and queued tasks
[2020-03-16 13:53:18,794] {scheduler_job.py:986} INFO - DAG tutorial has 2/16 running and queued tasks
[2020-03-16 13:53:18,822] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: tutorial.print_date 2020-03-15 00:00:00+00:00 [scheduled]>
        <TaskInstance: tutorial.templated 2020-03-14 00:00:00+00:00 [scheduled]>
        <TaskInstance: tutorial.sleep 2020-03-14 00:00:00+00:00 [scheduled]>
[2020-03-16 13:53:18,907] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:53:18,911] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-16 13:53:19,085] {scheduler_job.py:1112} INFO - Setting the following 3 tasks to queued state:
        <TaskInstance: tutorial.templated 2020-03-14 00:00:00+00:00 [queued]>
        <TaskInstance: tutorial.sleep 2020-03-14 00:00:00+00:00 [queued]>
        <TaskInstance: tutorial.print_date 2020-03-15 00:00:00+00:00 [queued]>
[2020-03-16 13:53:19,147] {scheduler_job.py:1148} INFO - Sending ('tutorial', 'templated', datetime.datetime(2020, 3, 14, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 13:53:19,157] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'tutorial', 'templated', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 13:53:19,163] {scheduler_job.py:1148} INFO - Sending ('tutorial', 'sleep', datetime.datetime(2020, 3, 14, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 13:53:19,178] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'tutorial', 'sleep', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 13:53:19,184] {scheduler_job.py:1148} INFO - Sending ('tutorial', 'print_date', datetime.datetime(2020, 3, 15, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:53:19,223] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'tutorial', 'print_date', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 13:53:19,399] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'tutorial', 'print_date', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 13:53:19,438] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'tutorial', 'templated', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 13:53:19,467] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'tutorial', 'sleep', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 13:53:19,515] {scheduler_job.py:1288} INFO - Executor reports execution of tutorial.print_date execution_date=2020-03-14 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:53:19,931] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=5251
[2020-03-16 13:53:19,976] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-15T04:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:53:19,992] {scheduler_job.py:1288} INFO - Executor reports execution of tutorial.templated execution_date=2020-03-10 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:53:20,882] {scheduler_job.py:1288} INFO - Executor reports execution of tutorial.sleep execution_date=2020-03-10 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:53:24,177] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=5276
[2020-03-16 13:53:24,197] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=5279
[2020-03-16 13:53:24,438] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=5281
[2020-03-16 13:53:24,592] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:53:24,723] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-16 13:53:26,729] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.final_2 2020-03-15T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:53:28,073] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:53:28,075] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py
[2020-03-16 13:53:28,144] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:53:28,148] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:53:28,154] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py
[2020-03-16 13:53:28,159] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py
[2020-03-16 13:53:28,607] {cli.py:545} INFO - Running <TaskInstance: tutorial.print_date 2020-03-15T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:53:28,707] {cli.py:545} INFO - Running <TaskInstance: tutorial.sleep 2020-03-14T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:53:28,743] {cli.py:545} INFO - Running <TaskInstance: tutorial.templated 2020-03-14T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:53:38,036] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: example_branch_operator.join 2020-03-15 00:00:00+00:00 [scheduled]>
[2020-03-16 13:53:38,097] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued
[2020-03-16 13:53:38,101] {scheduler_job.py:986} INFO - DAG example_branch_operator has 0/16 running and queued tasks
[2020-03-16 13:53:38,156] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_branch_operator.join 2020-03-15 00:00:00+00:00 [scheduled]>
[2020-03-16 13:53:38,332] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: example_branch_operator.join 2020-03-15 00:00:00+00:00 [queued]>
[2020-03-16 13:53:38,339] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'join', datetime.datetime(2020, 3, 15, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 13:53:38,344] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'join', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:53:38,376] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'join', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 13:53:38,401] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.follow_branch_b execution_date=2020-03-15 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:53:38,454] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.join execution_date=2020-03-14 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:53:40,140] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-15 08:00:00+00:00 [scheduled]>
[2020-03-16 13:53:40,190] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued
[2020-03-16 13:53:40,193] {scheduler_job.py:986} INFO - DAG latest_only has 0/16 running and queued tasks
[2020-03-16 13:53:40,234] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-15 08:00:00+00:00 [scheduled]>
[2020-03-16 13:53:40,326] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-15 08:00:00+00:00 [queued]>
[2020-03-16 13:53:40,328] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 15, 8, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:53:40,331] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-15T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:53:40,335] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-15T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:53:40,337] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.one_success execution_date=2020-03-15 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:53:40,365] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-15 04:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:53:41,206] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=5560
[2020-03-16 13:53:43,017] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=5585
[2020-03-16 13:53:43,584] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:53:43,588] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-16 13:53:44,082] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-14 16:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-14 16:00:00+00:00 [scheduled]>
[2020-03-16 13:53:44,186] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 2 task instances ready to be queued
[2020-03-16 13:53:44,194] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-16 13:53:44,211] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-16 13:53:44,264] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-14 16:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-14 16:00:00+00:00 [scheduled]>
[2020-03-16 13:53:44,389] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-14 16:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-14 16:00:00+00:00 [queued]>[2020-03-16 13:53:44,400] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.join 2020-03-15T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain

[2020-03-16 13:53:44,424] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 14, 16, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:53:44,438] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-14T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:53:44,445] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 14, 16, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-16 13:53:44,451] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-14T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:53:44,465] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-14T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:53:44,480] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-14T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:53:44,530] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-14 12:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:53:44,659] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-14 12:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:53:45,954] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:53:45,963] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-16 13:53:46,502] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-15T08:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:53:47,766] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=5640
[2020-03-16 13:53:47,778] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=5641
[2020-03-16 13:53:50,673] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:53:50,679] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:53:50,777] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:53:50,807] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:53:51,284] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-14T16:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:53:51,389] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-14T16:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:53:56,300] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-15 12:00:00+00:00 [scheduled]>
[2020-03-16 13:53:56,348] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued
[2020-03-16 13:53:56,379] {scheduler_job.py:986} INFO - DAG latest_only has 0/16 running and queued tasks
[2020-03-16 13:53:56,442] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-15 12:00:00+00:00 [scheduled]>
[2020-03-16 13:53:56,666] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-15 12:00:00+00:00 [queued]>
[2020-03-16 13:53:56,668] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 15, 12, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:53:56,673] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-15T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:53:56,682] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-15T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:53:56,711] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.final_2 execution_date=2020-03-15 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:53:58,254] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: tutorial.templated 2020-03-15 00:00:00+00:00 [scheduled]>
        <TaskInstance: tutorial.sleep 2020-03-15 00:00:00+00:00 [scheduled]>
[2020-03-16 13:53:58,313] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 125 open slots and 2 task instances ready to be queued
[2020-03-16 13:53:58,326] {scheduler_job.py:986} INFO - DAG tutorial has 0/16 running and queued tasks
[2020-03-16 13:53:58,329] {scheduler_job.py:986} INFO - DAG tutorial has 1/16 running and queued tasks
[2020-03-16 13:53:58,383] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: tutorial.templated 2020-03-15 00:00:00+00:00 [scheduled]>
        <TaskInstance: tutorial.sleep 2020-03-15 00:00:00+00:00 [scheduled]>
[2020-03-16 13:53:58,574] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: tutorial.templated 2020-03-15 00:00:00+00:00 [queued]>
        <TaskInstance: tutorial.sleep 2020-03-15 00:00:00+00:00 [queued]>
[2020-03-16 13:53:58,598] {scheduler_job.py:1148} INFO - Sending ('tutorial', 'templated', datetime.datetime(2020, 3, 15, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 13:53:58,604] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'tutorial', 'templated', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 13:53:58,608] {scheduler_job.py:1148} INFO - Sending ('tutorial', 'sleep', datetime.datetime(2020, 3, 15, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 13:53:58,613] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'tutorial', 'sleep', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 13:53:58,619] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'tutorial', 'templated', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 13:53:58,621] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'tutorial', 'sleep', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 13:53:58,624] {scheduler_job.py:1288} INFO - Executor reports execution of tutorial.templated execution_date=2020-03-14 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:53:58,676] {scheduler_job.py:1288} INFO - Executor reports execution of tutorial.print_date execution_date=2020-03-15 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:53:58,745] {scheduler_job.py:1288} INFO - Executor reports execution of tutorial.sleep execution_date=2020-03-14 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:53:59,736] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=5822
[2020-03-16 13:54:01,754] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=5855
[2020-03-16 13:54:01,790] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=5854
[2020-03-16 13:54:02,439] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:54:02,477] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-16 13:54:03,141] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-15T12:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:54:03,975] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:54:03,979] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py
[2020-03-16 13:54:04,133] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:54:04,143] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py
[2020-03-16 13:54:04,190] {cli.py:545} INFO - Running <TaskInstance: tutorial.sleep 2020-03-15T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:54:04,489] {cli.py:545} INFO - Running <TaskInstance: tutorial.templated 2020-03-15T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:54:09,096] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.join execution_date=2020-03-15 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:54:12,681] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-15 16:00:00+00:00 [scheduled]>
[2020-03-16 13:54:12,723] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 127 open slots and 1 task instances ready to be queued
[2020-03-16 13:54:12,723] {scheduler_job.py:986} INFO - DAG latest_only has 0/16 running and queued tasks
[2020-03-16 13:54:12,751] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-15 16:00:00+00:00 [scheduled]>
[2020-03-16 13:54:12,840] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-15 16:00:00+00:00 [queued]>
[2020-03-16 13:54:12,841] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 15, 16, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:54:12,842] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-15T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:54:12,845] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-15T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:54:12,845] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-15 08:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:54:14,688] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-14 20:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-14 20:00:00+00:00 [scheduled]>
[2020-03-16 13:54:14,753] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 2 task instances ready to be queued
[2020-03-16 13:54:14,774] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-16 13:54:14,802] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-16 13:54:14,893] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-14 20:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-14 20:00:00+00:00 [scheduled]>
[2020-03-16 13:54:14,971] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-14 20:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-14 20:00:00+00:00 [queued]>
[2020-03-16 13:54:14,979] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 14, 20, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:54:14,985] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-14T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:54:15,007] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 14, 20, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-16 13:54:15,016] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-14T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:54:15,044] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-14T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:54:15,098] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-14T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:54:15,172] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-14 16:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:54:15,292] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-14 16:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:54:15,627] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=6069
[2020-03-16 13:54:18,439] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=6099
[2020-03-16 13:54:18,447] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=6098
[2020-03-16 13:54:18,557] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:54:18,561] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-16 13:54:18,897] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-15T16:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:54:20,844] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:54:20,861] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:54:20,865] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:54:20,881] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:54:21,227] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-14T20:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:54:21,249] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-14T20:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:54:26,910] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-15 20:00:00+00:00 [scheduled]>
[2020-03-16 13:54:26,989] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 125 open slots and 1 task instances ready to be queued
[2020-03-16 13:54:26,997] {scheduler_job.py:986} INFO - DAG latest_only has 1/16 running and queued tasks
[2020-03-16 13:54:27,034] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-15 20:00:00+00:00 [scheduled]>
[2020-03-16 13:54:27,208] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-15 20:00:00+00:00 [queued]>
[2020-03-16 13:54:27,302] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 15, 20, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:54:27,328] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-15T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:54:27,383] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-15T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:54:27,398] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-15 12:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:54:29,027] {scheduler_job.py:1288} INFO - Executor reports execution of tutorial.templated execution_date=2020-03-15 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:54:29,087] {scheduler_job.py:1288} INFO - Executor reports execution of tutorial.sleep execution_date=2020-03-15 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:54:30,119] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=6274
[2020-03-16 13:54:32,695] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:54:32,698] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-16 13:54:32,859] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-15T20:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:54:45,070] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 13:54:45,097] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued
[2020-03-16 13:54:45,099] {scheduler_job.py:986} INFO - DAG latest_only has 0/16 running and queued tasks
[2020-03-16 13:54:45,115] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 13:54:45,326] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-16 00:00:00+00:00 [queued]>
[2020-03-16 13:54:45,328] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 16, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:54:45,329] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:54:45,333] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-15 16:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:54:45,333] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:54:47,091] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-15 00:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-15 00:00:00+00:00 [scheduled]>
[2020-03-16 13:54:47,121] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 127 open slots and 2 task instances ready to be queued
[2020-03-16 13:54:47,122] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-16 13:54:47,125] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-16 13:54:47,157] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-15 00:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-15 00:00:00+00:00 [scheduled]>
[2020-03-16 13:54:47,235] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=6451
[2020-03-16 13:54:47,551] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-15 00:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-15 00:00:00+00:00 [queued]>
[2020-03-16 13:54:47,559] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 15, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:54:47,563] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:54:47,566] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 15, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-16 13:54:47,569] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:54:47,581] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:54:47,591] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:54:47,599] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-14 20:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:54:47,624] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-14 20:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:54:50,400] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:54:50,429] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-16 13:54:51,119] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=6472
[2020-03-16 13:54:51,220] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=6473
[2020-03-16 13:54:51,277] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-16T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:54:53,790] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:54:53,796] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:54:54,469] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-15T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:54:54,507] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:54:54,852] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:54:55,066] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-15T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:55:01,593] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-16 04:00:00+00:00 [scheduled]>
[2020-03-16 13:55:01,655] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued
[2020-03-16 13:55:01,686] {scheduler_job.py:986} INFO - DAG latest_only has 0/16 running and queued tasks
[2020-03-16 13:55:01,752] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-16 04:00:00+00:00 [scheduled]>
[2020-03-16 13:55:02,009] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-16 04:00:00+00:00 [queued]>
[2020-03-16 13:55:02,015] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 16, 4, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:55:02,020] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-16T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:55:02,025] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-15 20:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:55:02,025] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-16T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:55:04,203] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=6651
[2020-03-16 13:55:06,147] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:55:06,149] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-16 13:55:06,343] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-16T04:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:55:13,440] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-16 08:00:00+00:00 [scheduled]>
[2020-03-16 13:55:13,461] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued
[2020-03-16 13:55:13,462] {scheduler_job.py:986} INFO - DAG latest_only has 0/16 running and queued tasks
[2020-03-16 13:55:13,491] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-16 08:00:00+00:00 [scheduled]>
[2020-03-16 13:55:13,804] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-16 08:00:00+00:00 [queued]>
[2020-03-16 13:55:13,806] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 16, 8, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:55:13,807] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-16T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:55:13,810] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-16T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:55:13,810] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-16 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:55:15,445] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-15 04:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-15 04:00:00+00:00 [scheduled]>
[2020-03-16 13:55:15,474] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 127 open slots and 2 task instances ready to be queued
[2020-03-16 13:55:15,475] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-16 13:55:15,478] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-16 13:55:15,498] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-15 04:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-15 04:00:00+00:00 [scheduled]>
[2020-03-16 13:55:15,809] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-15 04:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-15 04:00:00+00:00 [queued]>
[2020-03-16 13:55:15,813] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 15, 4, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:55:15,814] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-15T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:55:15,815] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 15, 4, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-16 13:55:15,817] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-15T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:55:15,821] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-15T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:55:15,822] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-15T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:55:15,823] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-15 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:55:15,888] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-15 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:55:16,010] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=6805
[2020-03-16 13:55:18,425] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:55:18,437] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-16 13:55:18,679] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=6828
[2020-03-16 13:55:18,717] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-16T08:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:55:18,861] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=6827
[2020-03-16 13:55:21,029] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:55:21,033] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:55:21,222] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:55:21,225] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:55:21,257] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-15T04:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:55:21,377] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-15T04:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:55:27,599] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-16 12:00:00+00:00 [scheduled]>
[2020-03-16 13:55:27,627] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued
[2020-03-16 13:55:27,628] {scheduler_job.py:986} INFO - DAG latest_only has 0/16 running and queued tasks
[2020-03-16 13:55:27,652] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-16 12:00:00+00:00 [scheduled]>
[2020-03-16 13:55:27,879] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-16 12:00:00+00:00 [queued]>
[2020-03-16 13:55:27,935] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 16, 12, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 13:55:28,005] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-16T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:55:28,121] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-16T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:55:28,150] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-16 04:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:55:31,012] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=7000
[2020-03-16 13:55:33,371] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:55:33,374] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-16 13:55:33,593] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-16T12:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:55:41,761] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-16 08:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:55:45,759] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-15 08:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-15 08:00:00+00:00 [scheduled]>
[2020-03-16 13:55:45,811] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 2 task instances ready to be queued
[2020-03-16 13:55:45,813] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-16 13:55:45,817] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-16 13:55:45,839] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-15 08:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-15 08:00:00+00:00 [scheduled]>
[2020-03-16 13:55:45,995] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-15 08:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-15 08:00:00+00:00 [queued]>
[2020-03-16 13:55:45,998] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 15, 8, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:55:45,999] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-15T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:55:46,000] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 15, 8, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-16 13:55:46,001] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-15T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:55:46,006] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-15T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:55:46,006] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-15T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:55:46,010] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-15 04:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:55:46,114] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-15 04:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:55:48,728] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=7205
[2020-03-16 13:55:48,854] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=7206
[2020-03-16 13:55:51,302] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:55:51,312] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:55:51,454] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:55:51,472] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:55:51,576] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-15T08:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:55:51,744] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-15T08:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:55:58,094] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.task1 2020-03-16 12:00:00+00:00 [scheduled]>
[2020-03-16 13:55:58,171] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued
[2020-03-16 13:55:58,190] {scheduler_job.py:986} INFO - DAG latest_only has 0/16 running and queued tasks
[2020-03-16 13:55:58,304] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.task1 2020-03-16 12:00:00+00:00 [scheduled]>
[2020-03-16 13:55:58,461] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.task1 2020-03-16 12:00:00+00:00 [queued]>
[2020-03-16 13:55:58,475] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'task1', datetime.datetime(2020, 3, 16, 12, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 13:55:58,484] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'task1', '2020-03-16T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:55:58,494] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'task1', '2020-03-16T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 13:55:58,496] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-16 12:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:56:01,544] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=7337
[2020-03-16 13:56:04,128] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:56:04,132] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-16 13:56:04,405] {cli.py:545} INFO - Running <TaskInstance: latest_only.task1 2020-03-16T12:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:56:08,018] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: example_http_operator.http_sensor_check 2020-03-14 00:00:00+00:00 [scheduled]>
[2020-03-16 13:56:08,054] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 127 open slots and 1 task instances ready to be queued
[2020-03-16 13:56:08,056] {scheduler_job.py:986} INFO - DAG example_http_operator has 0/16 running and queued tasks
[2020-03-16 13:56:08,093] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_http_operator.http_sensor_check 2020-03-14 00:00:00+00:00 [scheduled]>
[2020-03-16 13:56:08,175] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: example_http_operator.http_sensor_check 2020-03-14 00:00:00+00:00 [queued]>
[2020-03-16 13:56:08,178] {scheduler_job.py:1148} INFO - Sending ('example_http_operator', 'http_sensor_check', datetime.datetime(2020, 3, 14, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 2) to executor with priority 6 and queue default
[2020-03-16 13:56:08,179] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_http_operator', 'http_sensor_check', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 13:56:08,183] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_http_operator', 'http_sensor_check', '2020-03-14T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 13:56:10,812] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=7446
[2020-03-16 13:56:12,715] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:56:12,720] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py
[2020-03-16 13:56:12,961] {cli.py:545} INFO - Running <TaskInstance: example_http_operator.http_sensor_check 2020-03-14T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:56:14,082] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-15 12:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-15 12:00:00+00:00 [scheduled]>
[2020-03-16 13:56:14,112] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 127 open slots and 2 task instances ready to be queued
[2020-03-16 13:56:14,113] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-16 13:56:14,118] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-16 13:56:14,207] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-15 12:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-15 12:00:00+00:00 [scheduled]>
[2020-03-16 13:56:14,421] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-15 12:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-15 12:00:00+00:00 [queued]>
[2020-03-16 13:56:14,457] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 15, 12, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:56:14,463] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-15T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:56:14,467] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 15, 12, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-16 13:56:14,476] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-15T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:56:14,494] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-15T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:56:14,498] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-15T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:56:14,501] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-15 08:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:56:14,597] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-15 08:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:56:17,669] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=7549
[2020-03-16 13:56:17,705] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=7550
[2020-03-16 13:56:20,126] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:56:20,128] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:56:20,179] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:56:20,182] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:56:20,320] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-15T12:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:56:20,374] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-15T12:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:56:24,315] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.task1 execution_date=2020-03-16 12:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:56:38,153] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-15 16:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-15 16:00:00+00:00 [scheduled]>
[2020-03-16 13:56:38,181] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 127 open slots and 2 task instances ready to be queued
[2020-03-16 13:56:38,183] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-16 13:56:38,186] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-16 13:56:38,211] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-15 16:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-15 16:00:00+00:00 [scheduled]>
[2020-03-16 13:56:38,304] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-15 16:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-15 16:00:00+00:00 [queued]>
[2020-03-16 13:56:38,306] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 15, 16, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:56:38,307] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-15T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:56:38,308] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 15, 16, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-16 13:56:38,309] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-15T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:56:38,312] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-15 12:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:56:38,312] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-15T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:56:38,312] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-15T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:56:38,336] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-15 12:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:56:41,150] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=7866
[2020-03-16 13:56:41,313] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=7867
[2020-03-16 13:56:43,412] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:56:43,419] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:56:43,459] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:56:43,465] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:56:43,587] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-15T16:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:56:43,698] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-15T16:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:56:52,320] {scheduler_job.py:1288} INFO - Executor reports execution of example_http_operator.http_sensor_check execution_date=2020-03-14 00:00:00+00:00 exited with status success for try_number 2
[2020-03-16 13:57:02,337] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-15 20:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-15 20:00:00+00:00 [scheduled]>
[2020-03-16 13:57:02,374] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 2 task instances ready to be queued
[2020-03-16 13:57:02,376] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-16 13:57:02,379] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-16 13:57:02,400] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-15 20:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-15 20:00:00+00:00 [scheduled]>
[2020-03-16 13:57:02,494] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-15 20:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-15 20:00:00+00:00 [queued]>
[2020-03-16 13:57:02,503] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 15, 20, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:57:02,509] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-15T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:57:02,516] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 15, 20, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-16 13:57:02,521] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-15T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:57:02,529] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-15T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:57:02,537] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-15T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:57:02,538] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-15 16:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:57:02,571] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-15 16:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:57:05,108] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=8179
[2020-03-16 13:57:05,119] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=8178
[2020-03-16 13:57:07,096] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:57:07,099] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:57:07,181] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:57:07,183] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:57:07,265] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-15T20:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:57:07,363] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-15T20:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:57:22,393] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-16 00:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 13:57:22,421] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 2 task instances ready to be queued
[2020-03-16 13:57:22,423] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-16 13:57:22,425] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-16 13:57:22,443] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-16 00:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 13:57:22,539] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-16 00:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-16 00:00:00+00:00 [queued]>
[2020-03-16 13:57:22,541] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 16, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:57:22,542] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:57:22,543] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 16, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-16 13:57:22,545] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:57:22,548] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-15 20:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:57:22,548] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:57:22,549] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:57:22,555] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-15 20:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:57:24,504] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: example_http_operator.post_op 2020-03-15 00:00:00+00:00 [scheduled]>
[2020-03-16 13:57:24,543] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued
[2020-03-16 13:57:24,548] {scheduler_job.py:986} INFO - DAG example_http_operator has 0/16 running and queued tasks
[2020-03-16 13:57:24,628] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_http_operator.post_op 2020-03-15 00:00:00+00:00 [scheduled]>
[2020-03-16 13:57:24,773] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: example_http_operator.post_op 2020-03-15 00:00:00+00:00 [queued]>
[2020-03-16 13:57:24,782] {scheduler_job.py:1148} INFO - Sending ('example_http_operator', 'post_op', datetime.datetime(2020, 3, 15, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 2) to executor with priority 5 and queue default
[2020-03-16 13:57:24,788] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_http_operator', 'post_op', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 13:57:24,795] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_http_operator', 'post_op', '2020-03-15T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 13:57:25,271] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=8453
[2020-03-16 13:57:25,303] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=8454
[2020-03-16 13:57:27,725] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:57:27,746] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:57:27,808] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=8481
[2020-03-16 13:57:28,107] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-16T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:57:28,178] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:57:28,209] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:57:28,725] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-16T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:57:29,984] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:57:29,993] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py
[2020-03-16 13:57:30,822] {cli.py:545} INFO - Running <TaskInstance: example_http_operator.post_op 2020-03-15T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:57:46,536] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-16 04:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-16 04:00:00+00:00 [scheduled]>
[2020-03-16 13:57:46,566] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 127 open slots and 2 task instances ready to be queued
[2020-03-16 13:57:46,566] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-16 13:57:46,567] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-16 13:57:46,621] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-16 04:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-16 04:00:00+00:00 [scheduled]>
[2020-03-16 13:57:46,723] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-16 04:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-16 04:00:00+00:00 [queued]>
[2020-03-16 13:57:46,727] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 16, 4, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:57:46,729] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-16T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:57:46,730] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 16, 4, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-16 13:57:46,732] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-16T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:57:46,737] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-16T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:57:46,741] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-16T04:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:57:46,743] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-16 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:57:46,793] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-16 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:57:49,160] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=8796
[2020-03-16 13:57:49,237] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=8797
[2020-03-16 13:57:51,326] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:57:51,329] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:57:51,357] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:57:51,362] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:57:51,551] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-16T04:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:57:51,623] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-16T04:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:58:08,631] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-16 08:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-16 08:00:00+00:00 [scheduled]>
[2020-03-16 13:58:08,658] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 2 task instances ready to be queued
[2020-03-16 13:58:08,660] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-16 13:58:08,663] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-16 13:58:08,693] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-16 08:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-16 08:00:00+00:00 [scheduled]>
[2020-03-16 13:58:08,743] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-16 08:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-16 08:00:00+00:00 [queued]>
[2020-03-16 13:58:08,746] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 16, 8, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:58:08,747] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-16T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:58:08,749] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 16, 8, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-16 13:58:08,750] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-16T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:58:08,756] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-16 04:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:58:08,754] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-16T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:58:08,757] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-16T08:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:58:08,766] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-16 04:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:58:10,866] {scheduler_job.py:1288} INFO - Executor reports execution of example_http_operator.post_op execution_date=2020-03-15 00:00:00+00:00 exited with status success for try_number 2
[2020-03-16 13:58:11,680] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=9109
[2020-03-16 13:58:11,821] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=9108
[2020-03-16 13:58:13,747] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:58:13,750] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:58:13,966] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-16T08:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:58:13,991] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:58:13,994] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:58:14,114] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-16T08:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:58:30,739] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-16 12:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-16 12:00:00+00:00 [scheduled]>
[2020-03-16 13:58:30,767] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 2 task instances ready to be queued
[2020-03-16 13:58:30,769] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-16 13:58:30,772] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-16 13:58:30,789] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-16 12:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-16 12:00:00+00:00 [scheduled]>
[2020-03-16 13:58:30,842] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-16 12:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-16 12:00:00+00:00 [queued]>
[2020-03-16 13:58:30,844] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 16, 12, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:58:30,846] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-16T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:58:30,847] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 16, 12, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-16 13:58:30,849] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-16T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:58:30,853] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-16 08:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:58:30,853] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-16T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:58:30,854] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-16T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:58:30,863] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-16 08:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:58:33,341] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=9404
[2020-03-16 13:58:33,447] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=9405
[2020-03-16 13:58:35,545] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:58:35,548] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:58:35,593] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:58:35,598] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:58:35,784] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-16T12:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:58:35,821] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-16T12:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:58:53,088] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task1 2020-03-16 12:00:00+00:00 [scheduled]>
[2020-03-16 13:58:53,126] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued
[2020-03-16 13:58:53,128] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-16 13:58:53,142] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task1 2020-03-16 12:00:00+00:00 [scheduled]>
[2020-03-16 13:58:54,734] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task1 2020-03-16 12:00:00+00:00 [queued]>
[2020-03-16 13:58:54,750] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task1', datetime.datetime(2020, 3, 16, 12, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 13:58:54,753] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task1', '2020-03-16T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:58:54,756] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-16 12:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:58:54,756] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task1', '2020-03-16T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:58:54,762] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-16 12:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:58:56,694] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=9697
[2020-03-16 13:58:58,475] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:58:58,479] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:58:58,657] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task1 2020-03-16T12:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:59:29,173] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task4 2020-03-16 12:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task3 2020-03-16 12:00:00+00:00 [scheduled]>
[2020-03-16 13:59:29,215] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 2 task instances ready to be queued
[2020-03-16 13:59:29,217] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-16 13:59:29,220] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-16 13:59:29,237] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task4 2020-03-16 12:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task3 2020-03-16 12:00:00+00:00 [scheduled]>
[2020-03-16 13:59:29,307] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task4 2020-03-16 12:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.task3 2020-03-16 12:00:00+00:00 [queued]>
[2020-03-16 13:59:29,309] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task4', datetime.datetime(2020, 3, 16, 12, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 13:59:29,311] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task4', '2020-03-16T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:59:29,312] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task3', datetime.datetime(2020, 3, 16, 12, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 13:59:29,313] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task3', '2020-03-16T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:59:29,316] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task1 execution_date=2020-03-16 12:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:59:29,317] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task3', '2020-03-16T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:59:29,318] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task4', '2020-03-16T12:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 13:59:31,722] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=9958
[2020-03-16 13:59:31,753] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=9957
[2020-03-16 13:59:33,897] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:59:33,904] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:59:33,933] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 13:59:33,936] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 13:59:34,136] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task4 2020-03-16T12:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:59:34,141] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task3 2020-03-16T12:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 13:59:49,404] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task4 execution_date=2020-03-16 12:00:00+00:00 exited with status success for try_number 1
[2020-03-16 13:59:49,630] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task3 execution_date=2020-03-16 12:00:00+00:00 exited with status success for try_number 1
[2020-03-16 17:05:31,113] {scheduler_job.py:224} WARNING - Killing PID 13969
[2020-03-16 17:05:40,858] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-16 16:00:00+00:00 [scheduled]>
[2020-03-16 17:05:40,891] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued
[2020-03-16 17:05:40,893] {scheduler_job.py:986} INFO - DAG latest_only has 0/16 running and queued tasks
[2020-03-16 17:05:40,908] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-16 16:00:00+00:00 [scheduled]>
[2020-03-16 17:05:41,059] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-16 16:00:00+00:00 [queued]>
[2020-03-16 17:05:41,066] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 16, 16, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 17:05:41,068] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-16T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 17:05:41,072] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-16T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 17:05:43,169] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=14030
[2020-03-16 17:05:45,686] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 17:05:45,689] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-16 17:05:45,837] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-16T16:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 17:53:22,346] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-16 16:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-16 16:00:00+00:00 [scheduled]>
[2020-03-16 17:53:22,405] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 2 task instances ready to be queued
[2020-03-16 17:53:22,413] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-16 17:53:22,418] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-16 17:53:22,449] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-16 16:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-16 16:00:00+00:00 [scheduled]>
[2020-03-16 17:53:23,665] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-16 16:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-16 16:00:00+00:00 [queued]>
[2020-03-16 17:53:23,668] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 16, 16, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 17:53:23,668] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-16T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 17:53:23,671] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 16, 16, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-16 17:53:23,672] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-16T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 17:53:23,675] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-16T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 17:53:23,676] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-16T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 17:53:38,701] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.task1 2020-03-16 16:00:00+00:00 [scheduled]>
[2020-03-16 17:53:38,712] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued
[2020-03-16 17:53:38,713] {scheduler_job.py:986} INFO - DAG latest_only has 0/16 running and queued tasks
[2020-03-16 17:53:38,731] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.task1 2020-03-16 16:00:00+00:00 [scheduled]>
[2020-03-16 17:53:39,188] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.task1 2020-03-16 16:00:00+00:00 [queued]>
[2020-03-16 17:53:39,190] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'task1', datetime.datetime(2020, 3, 16, 16, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 17:53:39,191] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'task1', '2020-03-16T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 17:53:39,195] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-16 16:00:00+00:00 exited with status success for try_number 1
[2020-03-16 17:53:39,195] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'task1', '2020-03-16T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 17:54:44,415] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=14280
[2020-03-16 17:54:44,415] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=14204
[2020-03-16 17:54:44,415] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=14205
[2020-03-16 17:56:06,234] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 17:56:06,242] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-16 17:56:13,073] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 17:56:13,077] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 17:56:15,564] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 17:56:15,567] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 17:56:16,502] {cli.py:545} INFO - Running <TaskInstance: latest_only.task1 2020-03-16T16:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 17:56:18,526] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-16T16:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 17:56:19,360] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-16T16:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 17:56:39,253] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.task1 execution_date=2020-03-16 16:00:00+00:00 exited with status success for try_number 1
[2020-03-16 17:56:41,259] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task1 2020-03-16 16:00:00+00:00 [scheduled]>
[2020-03-16 17:56:41,276] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued
[2020-03-16 17:56:41,277] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-16 17:56:41,295] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task1 2020-03-16 16:00:00+00:00 [scheduled]>
[2020-03-16 17:56:41,683] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task1 2020-03-16 16:00:00+00:00 [queued]>
[2020-03-16 17:56:41,688] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task1', datetime.datetime(2020, 3, 16, 16, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 17:56:41,692] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task1', '2020-03-16T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 17:56:41,706] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task1', '2020-03-16T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 17:56:41,715] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-16 16:00:00+00:00 exited with status success for try_number 1
[2020-03-16 17:56:41,729] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-16 16:00:00+00:00 exited with status success for try_number 1
[2020-03-16 17:56:43,681] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=15886
[2020-03-16 17:56:45,797] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 17:56:45,799] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 17:56:46,749] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task1 2020-03-16T16:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 17:57:03,272] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task4 2020-03-16 16:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task3 2020-03-16 16:00:00+00:00 [scheduled]>
[2020-03-16 17:57:03,283] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 2 task instances ready to be queued
[2020-03-16 17:57:03,284] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-16 17:57:03,287] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-16 17:57:03,298] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task4 2020-03-16 16:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task3 2020-03-16 16:00:00+00:00 [scheduled]>
[2020-03-16 17:57:03,380] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task4 2020-03-16 16:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.task3 2020-03-16 16:00:00+00:00 [queued]>
[2020-03-16 17:57:03,383] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task4', datetime.datetime(2020, 3, 16, 16, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 17:57:03,384] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task4', '2020-03-16T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 17:57:03,386] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task3', datetime.datetime(2020, 3, 16, 16, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 17:57:03,387] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task3', '2020-03-16T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 17:57:03,390] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task1 execution_date=2020-03-16 16:00:00+00:00 exited with status success for try_number 1
[2020-03-16 17:57:03,390] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task4', '2020-03-16T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 17:57:03,392] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task3', '2020-03-16T16:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 17:57:05,019] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=16144
[2020-03-16 17:57:05,084] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=16145
[2020-03-16 17:57:06,500] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 17:57:06,501] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 17:57:06,578] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 17:57:06,580] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 17:57:06,630] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task3 2020-03-16T16:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 17:57:06,767] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task4 2020-03-16T16:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 17:57:29,348] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task3 execution_date=2020-03-16 16:00:00+00:00 exited with status success for try_number 1
[2020-03-16 17:57:29,354] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task4 execution_date=2020-03-16 16:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:29:37,257] {scheduler_job.py:224} WARNING - Killing PID 26579
[2020-03-16 22:29:38,032] {scheduler_job.py:224} WARNING - Killing PID 26582
[2020-03-16 22:29:38,414] {scheduler_job.py:224} WARNING - Killing PID 26579
[2020-03-16 22:29:38,573] {scheduler_job.py:224} WARNING - Killing PID 26582
[2020-03-16 22:29:39,471] {scheduler_job.py:224} WARNING - Killing PID 26579
[2020-03-16 22:29:39,557] {scheduler_job.py:224} WARNING - Killing PID 26582
[2020-03-16 22:29:39,713] {scheduler_job.py:224} WARNING - Killing PID 26579
[2020-03-16 22:29:39,758] {scheduler_job.py:224} WARNING - Killing PID 26582
[2020-03-16 22:29:53,469] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.latest_only 2020-03-16 20:00:00+00:00 [scheduled]>
[2020-03-16 22:29:53,511] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued
[2020-03-16 22:29:53,523] {scheduler_job.py:986} INFO - DAG latest_only has 0/16 running and queued tasks
[2020-03-16 22:29:53,624] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-16 20:00:00+00:00 [scheduled]>
[2020-03-16 22:29:53,959] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.latest_only 2020-03-16 20:00:00+00:00 [queued]>
[2020-03-16 22:29:53,977] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'latest_only', datetime.datetime(2020, 3, 16, 20, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 22:29:53,980] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-16T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 22:29:53,987] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'latest_only', '2020-03-16T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 22:29:55,924] {scheduler_job.py:927} INFO - 4 tasks up for execution:
        <TaskInstance: example_skip_dag.skip_operator_2 2020-03-16 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.skip_operator_1 2020-03-16 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.always_true_2 2020-03-16 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.always_true_1 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:29:55,960] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 127 open slots and 4 task instances ready to be queued
[2020-03-16 22:29:55,962] {scheduler_job.py:986} INFO - DAG example_skip_dag has 0/16 running and queued tasks
[2020-03-16 22:29:55,972] {scheduler_job.py:986} INFO - DAG example_skip_dag has 1/16 running and queued tasks
[2020-03-16 22:29:55,975] {scheduler_job.py:986} INFO - DAG example_skip_dag has 2/16 running and queued tasks
[2020-03-16 22:29:55,977] {scheduler_job.py:986} INFO - DAG example_skip_dag has 3/16 running and queued tasks
[2020-03-16 22:29:56,037] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_skip_dag.skip_operator_2 2020-03-16 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.skip_operator_1 2020-03-16 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.always_true_2 2020-03-16 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_skip_dag.always_true_1 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:29:56,891] {scheduler_job.py:1112} INFO - Setting the following 4 tasks to queued state:
        <TaskInstance: example_skip_dag.skip_operator_2 2020-03-16 00:00:00+00:00 [queued]>
        <TaskInstance: example_skip_dag.skip_operator_1 2020-03-16 00:00:00+00:00 [queued]>
        <TaskInstance: example_skip_dag.always_true_2 2020-03-16 00:00:00+00:00 [queued]>
        <TaskInstance: example_skip_dag.always_true_1 2020-03-16 00:00:00+00:00 [queued]>
[2020-03-16 22:29:57,756] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'skip_operator_2', datetime.datetime(2020, 3, 16, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 22:29:57,822] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'skip_operator_2', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 22:29:58,027] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'skip_operator_1', datetime.datetime(2020, 3, 16, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 22:29:58,140] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'skip_operator_1', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 22:29:58,377] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'always_true_2', datetime.datetime(2020, 3, 16, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 22:29:58,593] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'always_true_2', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 22:29:58,837] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'always_true_1', datetime.datetime(2020, 3, 16, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 22:29:58,991] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'always_true_1', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 22:29:59,627] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'skip_operator_2', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 22:29:59,770] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'skip_operator_1', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 22:29:59,842] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'always_true_2', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 22:29:59,936] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'always_true_1', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 22:30:00,137] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=26612
[2020-03-16 22:30:09,341] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=26631
[2020-03-16 22:30:09,938] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=26632
[2020-03-16 22:30:10,337] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=26633
[2020-03-16 22:30:10,809] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=26634
[2020-03-16 22:30:10,971] {scheduler_job.py:927} INFO - 3 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task2 2020-03-16 20:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-16 20:00:00+00:00 [scheduled]>
        <TaskInstance: tutorial.print_date 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:30:11,903] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:30:11,916] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 123 open slots and 3 task instances ready to be queued
[2020-03-16 22:30:12,028] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-16 22:30:12,220] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-16 22:30:12,560] {scheduler_job.py:986} INFO - DAG tutorial has 0/16 running and queued tasks
[2020-03-16 22:30:12,584] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-16 22:30:13,167] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-16 20:00:00+00:00 [scheduled]>
        <TaskInstance: tutorial.print_date 2020-03-16 00:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-16 20:00:00+00:00 [scheduled]>
[2020-03-16 22:30:14,517] {cli.py:545} INFO - Running <TaskInstance: latest_only.latest_only 2020-03-16T20:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:30:14,589] {scheduler_job.py:1112} INFO - Setting the following 3 tasks to queued state:
        <TaskInstance: tutorial.print_date 2020-03-16 00:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.latest_only 2020-03-16 20:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.task2 2020-03-16 20:00:00+00:00 [queued]>
[2020-03-16 22:30:14,969] {scheduler_job.py:1148} INFO - Sending ('tutorial', 'print_date', datetime.datetime(2020, 3, 16, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 22:30:15,089] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'tutorial', 'print_date', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 22:30:15,169] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'latest_only', datetime.datetime(2020, 3, 16, 20, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 4 and queue default
[2020-03-16 22:30:15,237] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-16T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 22:30:15,315] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task2', datetime.datetime(2020, 3, 16, 20, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 22:30:15,373] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-16T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 22:30:15,438] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'latest_only', '2020-03-16T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 22:30:15,485] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'tutorial', 'print_date', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 22:30:15,492] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:30:15,510] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task2', '2020-03-16T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 22:30:15,783] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-16 22:30:15,852] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:30:16,544] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-16 22:30:16,858] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:30:17,101] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-16 22:30:17,970] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:30:18,155] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-16 22:30:20,107] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.skip_operator_2 2020-03-16T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:30:20,581] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.always_true_2 2020-03-16T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:30:20,639] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.skip_operator_1 2020-03-16T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:30:21,804] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.always_true_1 2020-03-16T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:30:23,225] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=26698
[2020-03-16 22:30:23,508] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=26699
[2020-03-16 22:30:23,771] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=26701
[2020-03-16 22:30:30,997] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:30:31,275] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 22:30:31,621] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:30:32,065] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 22:30:32,066] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:30:32,646] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py
[2020-03-16 22:30:36,224] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.latest_only 2020-03-16T20:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:30:36,276] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: example_http_operator.http_sensor_check 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:30:36,744] {cli.py:545} INFO - Running <TaskInstance: tutorial.print_date 2020-03-16T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:30:36,961] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 120 open slots and 1 task instances ready to be queued
[2020-03-16 22:30:36,981] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task2 2020-03-16T20:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:30:37,152] {scheduler_job.py:986} INFO - DAG example_http_operator has 0/16 running and queued tasks
[2020-03-16 22:30:38,720] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_http_operator.http_sensor_check 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:30:40,269] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: example_http_operator.http_sensor_check 2020-03-16 00:00:00+00:00 [queued]>
[2020-03-16 22:30:40,701] {scheduler_job.py:1148} INFO - Sending ('example_http_operator', 'http_sensor_check', datetime.datetime(2020, 3, 16, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 6 and queue default
[2020-03-16 22:30:40,883] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_http_operator', 'http_sensor_check', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 22:30:41,293] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_http_operator', 'http_sensor_check', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 22:30:46,016] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=26881
[2020-03-16 22:30:52,582] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:30:52,811] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py
[2020-03-16 22:30:55,141] {cli.py:545} INFO - Running <TaskInstance: example_http_operator.http_sensor_check 2020-03-16T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:30:57,567] {scheduler_job.py:927} INFO - 3 tasks up for execution:
        <TaskInstance: example_branch_operator.run_this_first 2020-03-16 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.condition_is_True 2020-03-16 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.condition_is_False 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:30:57,731] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 127 open slots and 3 task instances ready to be queued
[2020-03-16 22:30:57,752] {scheduler_job.py:986} INFO - DAG example_branch_operator has 0/16 running and queued tasks
[2020-03-16 22:30:57,765] {scheduler_job.py:986} INFO - DAG example_short_circuit_operator has 0/16 running and queued tasks
[2020-03-16 22:30:57,791] {scheduler_job.py:986} INFO - DAG example_short_circuit_operator has 1/16 running and queued tasks
[2020-03-16 22:30:57,886] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_branch_operator.run_this_first 2020-03-16 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.condition_is_True 2020-03-16 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.condition_is_False 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:30:58,084] {scheduler_job.py:1112} INFO - Setting the following 3 tasks to queued state:
        <TaskInstance: example_branch_operator.run_this_first 2020-03-16 00:00:00+00:00 [queued]>
        <TaskInstance: example_short_circuit_operator.condition_is_False 2020-03-16 00:00:00+00:00 [queued]>
        <TaskInstance: example_short_circuit_operator.condition_is_True 2020-03-16 00:00:00+00:00 [queued]>
[2020-03-16 22:30:58,153] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'run_this_first', datetime.datetime(2020, 3, 16, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 11 and queue default
[2020-03-16 22:30:58,167] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'run_this_first', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 22:30:58,177] {scheduler_job.py:1148} INFO - Sending ('example_short_circuit_operator', 'condition_is_False', datetime.datetime(2020, 3, 16, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 22:30:58,185] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_short_circuit_operator', 'condition_is_False', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 22:30:58,193] {scheduler_job.py:1148} INFO - Sending ('example_short_circuit_operator', 'condition_is_True', datetime.datetime(2020, 3, 16, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 22:30:58,208] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_short_circuit_operator', 'condition_is_True', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 22:30:58,234] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'run_this_first', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 22:30:58,243] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_short_circuit_operator', 'condition_is_False', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 22:30:58,251] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_short_circuit_operator', 'condition_is_True', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 22:31:00,491] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only.task1 2020-03-16 20:00:00+00:00 [scheduled]>
[2020-03-16 22:31:01,015] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 124 open slots and 1 task instances ready to be queued
[2020-03-16 22:31:01,233] {scheduler_job.py:986} INFO - DAG latest_only has 0/16 running and queued tasks
[2020-03-16 22:31:01,899] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only.task1 2020-03-16 20:00:00+00:00 [scheduled]>
[2020-03-16 22:31:02,393] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only.task1 2020-03-16 20:00:00+00:00 [queued]>
[2020-03-16 22:31:02,512] {scheduler_job.py:1148} INFO - Sending ('latest_only', 'task1', datetime.datetime(2020, 3, 16, 20, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 22:31:02,600] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only', 'task1', '2020-03-16T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 22:31:02,735] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only', 'task1', '2020-03-16T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py']
[2020-03-16 22:31:02,765] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=27073
[2020-03-16 22:31:02,959] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=27074
[2020-03-16 22:31:03,101] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=27076
[2020-03-16 22:31:03,236] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.latest_only execution_date=2020-03-16 20:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:31:06,678] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:31:06,701] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-16 22:31:06,774] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:31:06,836] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py
[2020-03-16 22:31:06,883] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:31:06,960] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=27105
[2020-03-16 22:31:06,975] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py
[2020-03-16 22:31:07,457] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.run_this_first 2020-03-16T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:31:07,585] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: example_skip_dag.one_success 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:31:07,640] {cli.py:545} INFO - Running <TaskInstance: example_short_circuit_operator.condition_is_False 2020-03-16T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:31:07,663] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 123 open slots and 1 task instances ready to be queued
[2020-03-16 22:31:07,674] {cli.py:545} INFO - Running <TaskInstance: example_short_circuit_operator.condition_is_True 2020-03-16T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:31:07,678] {scheduler_job.py:986} INFO - DAG example_skip_dag has 0/16 running and queued tasks
[2020-03-16 22:31:07,710] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_skip_dag.one_success 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:31:07,940] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: example_skip_dag.one_success 2020-03-16 00:00:00+00:00 [queued]>
[2020-03-16 22:31:07,973] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'one_success', datetime.datetime(2020, 3, 16, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 22:31:08,009] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'one_success', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 22:31:08,074] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'one_success', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 22:31:08,084] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.always_true_1 execution_date=2020-03-16 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:31:08,258] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.skip_operator_2 execution_date=2020-03-16 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:31:08,359] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.always_true_2 execution_date=2020-03-16 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:31:08,498] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.skip_operator_1 execution_date=2020-03-16 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:31:09,694] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:31:09,708] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only.py
[2020-03-16 22:31:11,766] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=27176
[2020-03-16 22:31:13,508] {cli.py:545} INFO - Running <TaskInstance: latest_only.task1 2020-03-16T20:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:31:17,192] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:31:17,362] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-16 22:31:18,583] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.one_success 2020-03-16T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:31:20,236] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: example_http_operator.post_op 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:31:20,354] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 123 open slots and 1 task instances ready to be queued
[2020-03-16 22:31:20,380] {scheduler_job.py:986} INFO - DAG example_http_operator has 0/16 running and queued tasks
[2020-03-16 22:31:20,543] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_http_operator.post_op 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:31:21,876] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: example_http_operator.post_op 2020-03-16 00:00:00+00:00 [queued]>
[2020-03-16 22:31:22,105] {scheduler_job.py:1148} INFO - Sending ('example_http_operator', 'post_op', datetime.datetime(2020, 3, 16, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 5 and queue default
[2020-03-16 22:31:22,241] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_http_operator', 'post_op', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 22:31:22,390] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_http_operator', 'post_op', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 22:31:22,414] {scheduler_job.py:1288} INFO - Executor reports execution of example_http_operator.http_sensor_check execution_date=2020-03-16 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:31:25,691] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=27335
[2020-03-16 22:31:28,808] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:31:28,812] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py
[2020-03-16 22:31:29,440] {cli.py:545} INFO - Running <TaskInstance: example_http_operator.post_op 2020-03-16T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:31:31,159] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: example_branch_operator.branching 2020-03-16 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.true_1 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:31:31,174] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 127 open slots and 2 task instances ready to be queued
[2020-03-16 22:31:31,175] {scheduler_job.py:986} INFO - DAG example_branch_operator has 0/16 running and queued tasks
[2020-03-16 22:31:31,178] {scheduler_job.py:986} INFO - DAG example_short_circuit_operator has 0/16 running and queued tasks
[2020-03-16 22:31:31,205] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_branch_operator.branching 2020-03-16 00:00:00+00:00 [scheduled]>
        <TaskInstance: example_short_circuit_operator.true_1 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:31:32,526] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: example_branch_operator.branching 2020-03-16 00:00:00+00:00 [queued]>
        <TaskInstance: example_short_circuit_operator.true_1 2020-03-16 00:00:00+00:00 [queued]>
[2020-03-16 22:31:32,584] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'branching', datetime.datetime(2020, 3, 16, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 10 and queue default
[2020-03-16 22:31:32,596] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'branching', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 22:31:32,622] {scheduler_job.py:1148} INFO - Sending ('example_short_circuit_operator', 'true_1', datetime.datetime(2020, 3, 16, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 22:31:32,659] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_short_circuit_operator', 'true_1', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 22:31:32,736] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'branching', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 22:31:32,760] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_short_circuit_operator', 'true_1', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 22:31:32,777] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.run_this_first execution_date=2020-03-16 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:31:36,026] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only.task1 execution_date=2020-03-16 20:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:31:36,804] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=27432
[2020-03-16 22:31:37,228] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=27435
[2020-03-16 22:31:38,102] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: example_skip_dag.final_2 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:31:38,434] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 125 open slots and 1 task instances ready to be queued
[2020-03-16 22:31:38,494] {scheduler_job.py:986} INFO - DAG example_skip_dag has 0/16 running and queued tasks
[2020-03-16 22:31:38,856] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_skip_dag.final_2 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:31:39,460] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: example_skip_dag.final_2 2020-03-16 00:00:00+00:00 [queued]>
[2020-03-16 22:31:39,670] {scheduler_job.py:1148} INFO - Sending ('example_skip_dag', 'final_2', datetime.datetime(2020, 3, 16, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 22:31:39,735] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_skip_dag', 'final_2', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 22:31:39,827] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_skip_dag', 'final_2', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py']
[2020-03-16 22:31:39,903] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.one_success execution_date=2020-03-16 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:31:40,347] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:31:40,402] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-16 22:31:40,529] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:31:40,594] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py
[2020-03-16 22:31:41,382] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.branching 2020-03-16T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:31:41,551] {cli.py:545} INFO - Running <TaskInstance: example_short_circuit_operator.true_1 2020-03-16T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:31:41,618] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: tutorial.templated 2020-03-16 00:00:00+00:00 [scheduled]>
        <TaskInstance: tutorial.sleep 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:31:41,794] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 124 open slots and 2 task instances ready to be queued
[2020-03-16 22:31:41,837] {scheduler_job.py:986} INFO - DAG tutorial has 0/16 running and queued tasks
[2020-03-16 22:31:41,868] {scheduler_job.py:986} INFO - DAG tutorial has 1/16 running and queued tasks
[2020-03-16 22:31:42,088] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: tutorial.templated 2020-03-16 00:00:00+00:00 [scheduled]>
        <TaskInstance: tutorial.sleep 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:31:42,662] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: tutorial.templated 2020-03-16 00:00:00+00:00 [queued]>
        <TaskInstance: tutorial.sleep 2020-03-16 00:00:00+00:00 [queued]>[2020-03-16 22:31:42,779] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=27483

[2020-03-16 22:31:42,896] {scheduler_job.py:1148} INFO - Sending ('tutorial', 'templated', datetime.datetime(2020, 3, 16, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 22:31:42,960] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'tutorial', 'templated', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 22:31:43,011] {scheduler_job.py:1148} INFO - Sending ('tutorial', 'sleep', datetime.datetime(2020, 3, 16, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 22:31:43,101] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'tutorial', 'sleep', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 22:31:43,207] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'tutorial', 'templated', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 22:31:43,260] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'tutorial', 'sleep', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py']
[2020-03-16 22:31:43,286] {scheduler_job.py:1288} INFO - Executor reports execution of tutorial.print_date execution_date=2020-03-16 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:31:45,534] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task1 2020-03-16 20:00:00+00:00 [scheduled]>
[2020-03-16 22:31:46,008] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 123 open slots and 1 task instances ready to be queued
[2020-03-16 22:31:46,034] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-16 22:31:46,494] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task1 2020-03-16 20:00:00+00:00 [scheduled]>
[2020-03-16 22:31:47,066] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task1 2020-03-16 20:00:00+00:00 [queued]>
[2020-03-16 22:31:47,300] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task1', datetime.datetime(2020, 3, 16, 20, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 22:31:47,351] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task1', '2020-03-16T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 22:31:47,443] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task1', '2020-03-16T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 22:31:47,550] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.latest_only execution_date=2020-03-16 20:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:31:47,583] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:31:47,690] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_skip_dag.py
[2020-03-16 22:31:47,788] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task2 execution_date=2020-03-16 20:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:31:48,171] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=27524
[2020-03-16 22:31:48,479] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=27525
[2020-03-16 22:31:49,919] {cli.py:545} INFO - Running <TaskInstance: example_skip_dag.final_2 2020-03-16T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:31:52,548] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=27563
[2020-03-16 22:31:52,798] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:31:52,814] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py
[2020-03-16 22:31:52,983] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:31:53,071] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/tutorial.py
[2020-03-16 22:31:53,958] {cli.py:545} INFO - Running <TaskInstance: tutorial.sleep 2020-03-16T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:31:54,227] {scheduler_job.py:1288} INFO - Executor reports execution of example_http_operator.post_op execution_date=2020-03-16 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:31:54,591] {cli.py:545} INFO - Running <TaskInstance: tutorial.templated 2020-03-16T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:31:55,847] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:31:55,854] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 22:31:56,078] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task1 2020-03-16T20:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:32:04,535] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: example_branch_operator.branch_c 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:32:04,623] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 125 open slots and 1 task instances ready to be queued
[2020-03-16 22:32:04,637] {scheduler_job.py:986} INFO - DAG example_branch_operator has 0/16 running and queued tasks
[2020-03-16 22:32:04,738] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_branch_operator.branch_c 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:32:05,001] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: example_branch_operator.branch_c 2020-03-16 00:00:00+00:00 [queued]>
[2020-03-16 22:32:05,051] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'branch_c', datetime.datetime(2020, 3, 16, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-03-16 22:32:05,076] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'branch_c', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 22:32:05,118] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'branch_c', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 22:32:05,123] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.branching execution_date=2020-03-16 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:32:06,678] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: example_short_circuit_operator.true_2 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:32:06,940] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 125 open slots and 1 task instances ready to be queued
[2020-03-16 22:32:07,022] {scheduler_job.py:986} INFO - DAG example_short_circuit_operator has 0/16 running and queued tasks
[2020-03-16 22:32:07,238] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_short_circuit_operator.true_2 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:32:07,730] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: example_short_circuit_operator.true_2 2020-03-16 00:00:00+00:00 [queued]>
[2020-03-16 22:32:07,850] {scheduler_job.py:1148} INFO - Sending ('example_short_circuit_operator', 'true_2', datetime.datetime(2020, 3, 16, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 22:32:07,930] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_short_circuit_operator', 'true_2', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 22:32:08,022] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_short_circuit_operator', 'true_2', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py']
[2020-03-16 22:32:08,085] {scheduler_job.py:1288} INFO - Executor reports execution of example_short_circuit_operator.condition_is_True execution_date=2020-03-16 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:32:08,305] {scheduler_job.py:1288} INFO - Executor reports execution of example_short_circuit_operator.condition_is_False execution_date=2020-03-16 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:32:08,379] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=27783
[2020-03-16 22:32:08,478] {scheduler_job.py:1288} INFO - Executor reports execution of example_short_circuit_operator.true_1 execution_date=2020-03-16 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:32:10,555] {scheduler_job.py:1288} INFO - Executor reports execution of example_skip_dag.final_2 execution_date=2020-03-16 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:32:11,084] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=27822
[2020-03-16 22:32:11,148] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:32:11,163] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-16 22:32:11,453] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.branch_c 2020-03-16T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:32:13,520] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:32:13,542] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_short_circuit_operator.py
[2020-03-16 22:32:13,972] {cli.py:545} INFO - Running <TaskInstance: example_short_circuit_operator.true_2 2020-03-16T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:32:26,924] {scheduler_job.py:1288} INFO - Executor reports execution of tutorial.templated execution_date=2020-03-16 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:32:26,935] {scheduler_job.py:1288} INFO - Executor reports execution of tutorial.sleep execution_date=2020-03-16 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:32:28,982] {scheduler_job.py:927} INFO - 2 tasks up for execution:
        <TaskInstance: latest_only_with_trigger.task4 2020-03-16 20:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task3 2020-03-16 20:00:00+00:00 [scheduled]>
[2020-03-16 22:32:29,065] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 2 task instances ready to be queued
[2020-03-16 22:32:29,078] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 0/16 running and queued tasks
[2020-03-16 22:32:29,081] {scheduler_job.py:986} INFO - DAG latest_only_with_trigger has 1/16 running and queued tasks
[2020-03-16 22:32:29,103] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task4 2020-03-16 20:00:00+00:00 [scheduled]>
        <TaskInstance: latest_only_with_trigger.task3 2020-03-16 20:00:00+00:00 [scheduled]>
[2020-03-16 22:32:29,546] {scheduler_job.py:1112} INFO - Setting the following 2 tasks to queued state:
        <TaskInstance: latest_only_with_trigger.task4 2020-03-16 20:00:00+00:00 [queued]>
        <TaskInstance: latest_only_with_trigger.task3 2020-03-16 20:00:00+00:00 [queued]>
[2020-03-16 22:32:29,550] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task4', datetime.datetime(2020, 3, 16, 20, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 22:32:29,551] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task4', '2020-03-16T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 22:32:29,554] {scheduler_job.py:1148} INFO - Sending ('latest_only_with_trigger', 'task3', datetime.datetime(2020, 3, 16, 20, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 22:32:29,555] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'latest_only_with_trigger', 'task3', '2020-03-16T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 22:32:29,567] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task4', '2020-03-16T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 22:32:29,571] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task1 execution_date=2020-03-16 20:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:32:29,570] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'latest_only_with_trigger', 'task3', '2020-03-16T20:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py']
[2020-03-16 22:32:33,537] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=28076
[2020-03-16 22:32:33,896] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=28077
[2020-03-16 22:32:37,743] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:32:37,751] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 22:32:37,798] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:32:37,909] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_latest_only_with_trigger.py
[2020-03-16 22:32:38,478] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task4 2020-03-16T20:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:32:38,503] {cli.py:545} INFO - Running <TaskInstance: latest_only_with_trigger.task3 2020-03-16T20:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:32:41,590] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: example_branch_operator.follow_branch_c 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:32:41,750] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued
[2020-03-16 22:32:41,782] {scheduler_job.py:986} INFO - DAG example_branch_operator has 0/16 running and queued tasks
[2020-03-16 22:32:41,858] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_branch_operator.follow_branch_c 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:32:42,376] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: example_branch_operator.follow_branch_c 2020-03-16 00:00:00+00:00 [queued]>
[2020-03-16 22:32:42,444] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'follow_branch_c', datetime.datetime(2020, 3, 16, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 2 and queue default
[2020-03-16 22:32:42,495] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'follow_branch_c', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 22:32:42,619] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'follow_branch_c', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 22:32:42,645] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.branch_c execution_date=2020-03-16 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:32:42,929] {scheduler_job.py:1288} INFO - Executor reports execution of example_short_circuit_operator.true_2 execution_date=2020-03-16 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:32:46,626] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=28184
[2020-03-16 22:32:49,229] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:32:49,277] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-16 22:32:50,494] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.follow_branch_c 2020-03-16T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:33:02,546] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task3 execution_date=2020-03-16 20:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:33:02,560] {scheduler_job.py:1288} INFO - Executor reports execution of latest_only_with_trigger.task4 execution_date=2020-03-16 20:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:33:06,665] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: example_branch_operator.join 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:33:06,809] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued
[2020-03-16 22:33:06,849] {scheduler_job.py:986} INFO - DAG example_branch_operator has 0/16 running and queued tasks
[2020-03-16 22:33:06,977] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_branch_operator.join 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:33:07,138] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: example_branch_operator.join 2020-03-16 00:00:00+00:00 [queued]>
[2020-03-16 22:33:07,143] {scheduler_job.py:1148} INFO - Sending ('example_branch_operator', 'join', datetime.datetime(2020, 3, 16, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 1 and queue default
[2020-03-16 22:33:07,148] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_branch_operator', 'join', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 22:33:07,165] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_branch_operator', 'join', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py']
[2020-03-16 22:33:07,166] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.follow_branch_c execution_date=2020-03-16 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:33:10,314] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=28454
[2020-03-16 22:33:13,181] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:33:13,196] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_branch_operator.py
[2020-03-16 22:33:13,398] {cli.py:545} INFO - Running <TaskInstance: example_branch_operator.join 2020-03-16T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
[2020-03-16 22:33:26,689] {scheduler_job.py:1288} INFO - Executor reports execution of example_branch_operator.join execution_date=2020-03-16 00:00:00+00:00 exited with status success for try_number 1
[2020-03-16 22:36:48,562] {scheduler_job.py:927} INFO - 1 tasks up for execution:
        <TaskInstance: example_http_operator.post_op 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:36:48,599] {scheduler_job.py:958} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued
[2020-03-16 22:36:48,602] {scheduler_job.py:986} INFO - DAG example_http_operator has 0/16 running and queued tasks
[2020-03-16 22:36:48,636] {scheduler_job.py:1036} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_http_operator.post_op 2020-03-16 00:00:00+00:00 [scheduled]>
[2020-03-16 22:36:49,129] {scheduler_job.py:1112} INFO - Setting the following 1 tasks to queued state:
        <TaskInstance: example_http_operator.post_op 2020-03-16 00:00:00+00:00 [queued]>
[2020-03-16 22:36:49,139] {scheduler_job.py:1148} INFO - Sending ('example_http_operator', 'post_op', datetime.datetime(2020, 3, 16, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 2) to executor with priority 5 and queue default
[2020-03-16 22:36:49,142] {base_executor.py:59} INFO - Adding to queue: ['airflow', 'run', 'example_http_operator', 'post_op', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 22:36:49,220] {local_executor.py:84} INFO - QueuedLocalWorker running ['airflow', 'run', 'example_http_operator', 'post_op', '2020-03-16T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py']
[2020-03-16 22:36:52,535] {settings.py:252} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=30486
[2020-03-16 22:36:55,061] {__init__.py:51} INFO - Using executor LocalExecutor
[2020-03-16 22:36:55,070] {dagbag.py:92} INFO - Filling up the DagBag from /home/jdearce/.local/lib/python3.6/site-packages/airflow/example_dags/example_http_operator.py
[2020-03-16 22:36:55,283] {cli.py:545} INFO - Running <TaskInstance: example_http_operator.post_op 2020-03-16T00:00:00+00:00 [queued]> on host LAPTOP-I43LI54D.localdomain
^C[2020-03-16 22:37:15,465] {helpers.py:308} INFO - Sending Signals.SIGTERM to GPID 371
[2020-03-16 22:37:15,571] {helpers.py:286} INFO - Process psutil.Process(pid=30722, status='terminated') (30722) terminated with exit code None
[2020-03-16 22:37:15,631] {helpers.py:286} INFO - Process psutil.Process(pid=371, status='terminated') (371) terminated with exit code 0
[2020-03-16 22:37:15,640] {helpers.py:286} INFO - Process psutil.Process(pid=30724, status='terminated') (30724) terminated with exit code None
[2020-03-16 22:37:15,648] {scheduler_job.py:1361} INFO - Exited execute loop
jdearce@LAPTOP-I43LI54D:~$